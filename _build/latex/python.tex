%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Course Overview}}

\usepackage{sphinxmessages}




\title{DartBrains}
\date{Sep 14, 2020}
\release{}
\author{Luke Chang}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{content/intro::doc}}
\sphinxhref{https://zenodo.org/badge/latestdoi/171529794}{\sphinxincludegraphics{{/Users/lukechang/Github/dartbrains/_build/.doctrees/images/d38ba91bffa4b9d2174fcd896bae5ce57ad382ad/171529794}.svg#left}}



How can we understand how the brain works? This course provides an introduction to in vivo neuroimaging in humans using functional magnetic resonance imaging (fMRI). The goal of the class is to introduce: (1) how the scanner generates data, (2) how psychological states can be probed in the scanner, and (3) how this data can be processed and analyzed. Students will be expected to analyze brain imaging data using the opensource Python programming language. We will be using several packages such as numpy, matplotlib, nibabel, nilearn, fmriprep, and nltools. This course will be useful for students working in neuroimaging labs, completing a neuroimaging thesis, or interested in pursuing graduate training in fields related to cognitive neuroscience.


\chapter{Goals}
\label{\detokenize{content/intro:goals}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Learn the basics of fMRI signals

\item {} 
Introduce standard data preprocessing techniques

\item {} 
Introduce the general linear model

\item {} 
Introduce advanced analysis techniques

\end{enumerate}


\chapter{Overview}
\label{\detokenize{content/intro:overview}}
This course is designed primarily around learning the basics of fMRI data analysis using the Python programming language. We will cover a lot of ground from introducing the Python programming language, to signal processing, to working with opensource packages from the Python Scientific Computing community. The format will be slightly different than the typical in\sphinxhyphen{}person version of the course.
\begin{itemize}
\item {} 
All course materials will be made available online in the format of a jupyter book, https://dartbrains.org/. Lectures will primarily be delivered via publicly available pre\sphinxhyphen{}recorded lectures available on youtube.

\item {} 
Each course module will have a jupyter book tutorial and an accompanying assignment that will require completing some type of programming task or analysis. Students will work within small groups throughout the term on their assignments.

\item {} 
Scheduled classes will primarily take the form of a Q\&A where solutions to the assignments will be discussed and specific topics can be discussed in more detail.

\item {} 
There will be a single exam, in which students will have to replicate an analysis of a published study using real data.

\item {} 
For the final project, students will analyze an existing publicly available dataset to answer a new research question. Unfortunately, we will not be able to collect a new fMRI dataset as the Dartmouth Brain Imaging Center is currently closed for this term.

\end{itemize}


\chapter{Questions}
\label{\detokenize{content/intro:questions}}
Please post any questions that arise from the material in this course on our \sphinxhref{https://www.askpbs.org/c/dartbrains}{Discourse Page}


\chapter{License for this book}
\label{\detokenize{content/intro:license-for-this-book}}
All content is licensed under the \sphinxhref{https://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons Attribution\sphinxhyphen{}ShareAlike 4.0 International}
(CC BY\sphinxhyphen{}SA 4.0) license.


\chapter{Acknowledgements}
\label{\detokenize{content/intro:acknowledgements}}
Dartbrains was created by \sphinxhref{http://www.lukejchang.com/}{Luke Chang} and supported by an NSF CAREER Award 1848370 and the \sphinxhref{https://dcal.dartmouth.edu/about/impact/experiential-learning}{Dartmouth Center for the Advancement of Learning}. Our jupyterhub server was built and maintained by the Research Computing staff at Dartmouth. Special thanks to Arnold Song, William Hamblen, Christian Darabos, and John Hudson.


\section{Instructors}
\label{\detokenize{content/Instructors:instructors}}\label{\detokenize{content/Instructors::doc}}

\subsection{Professors}
\label{\detokenize{content/Instructors:professors}}
\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{chang}.jpg}

\sphinxhref{http://lukejchang.com}{Luke Chang}, PhD (Fall 2020) is an Assistant Professor of Psychological and Brain Sciences at Dartmouth College and directs the \sphinxhref{http://cosanlab.com/}{Computational Social Affective Neuroscience Laboratory}. He completed a BA in psychology at Reed College, an MA in psychology at the New School for Social Research, and a PhD in clinical psychology and cognitive neuroscience at the University of Arizona with Alan Sanfey, PhD. He completed his predoctoral clinical internship training in behavioral medicine at the University of California Los Angeles and a postdoctoral fellowship at the University of Colorado Boulder under the mentorship of Tor Wager, PhD. His research program is focused on understanding the neurobiological and computational mechanisms underlying emotions and social interactions. Professor Chang is highly committed to innovating training in methods. He is the lead developer of the \sphinxhref{https://dartbrains.org/}{dartbrains} course, the \sphinxhref{https://neurolearn.readthedocs.io/en/latest/}{nltools} python data analysis project, the \sphinxhref{http://compsan.org/}{Computational Social and Affective Neuroscience} community page, and Co\sphinxhyphen{}Director of the \sphinxhref{http://mindsummerschool.org/}{Methods in Neuroscience at Dartmouth Computational Summer School}.

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{finn}.jpg}

\sphinxhref{https://esfinn.github.io/}{Emily Finn}, PhD (Winter 2021) is an Assistant Professor of Psychological and Brain Sciences at Dartmouth College and directs the \sphinxhref{http://thefinnlab.github.io/}{Functional Imaging \& Naturalistic Neuroscience (FINN) Lab}. She completed a BA in linguistics at Yale University and a PhD in neuroscience, also at Yale. She then did her postdoctoral training in the Section on Functional Imaging Methods in the Laboratory of Brain and Cognition and the National Institute of Mental Health. Her research is focused on individual variability in brain activity and behavior, especially as it relates to appraisal of ambiguous information under naturalistic conditions. Professor Finn is committed to the ideals of open science, including data and code sharing (see examples \sphinxhref{https://openneuro.org/datasets/ds001338}{here}, \sphinxhref{https://github.com/esfinn/cpm\_tutorial}{here}, and \sphinxhref{https://github.com/esfinn/intersubj\_rsa}{here}, and to helping train other scientists in innovative new methods for neuroimaging data acquisition and analysis.

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{wager}.jpg}

\sphinxhref{https://sites.dartmouth.edu/canlab/}{Tor Wager}, PhD (Spring 2021) is the Diana L. Taylor Distinguished Professor in Neuroscience at Dartmouth College. He received his Ph.D. from the University of Michigan in Cognitive Psychology in 2003, and served as an Assistant (2004\sphinxhyphen{}2008) and Associate Professor (2009) at Columbia University, and as Associate (2010\sphinxhyphen{}2014) and Full Professor (2014\sphinxhyphen{}2019) at the University of Colorado, Boulder. Since 2004, he has directed the \sphinxhref{https://sites.dartmouth.edu/canlab/}{Cognitive and Affective Neuroscience laboratory}, a research lab devoted to work on the neurophysiology of affective processes—pain, emotion, stress, and empathy—and how they are shaped by cognitive and social influences. Dr. Wager and his lab are also dedicated to developing analysis methods for functional neuroimaging and sharing ideas, tools, and scientific data with the scientific community and public. See https://canlab.github.io for papers, data, tools, and code.

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{huckins}.jpg}

\sphinxhref{https://mtnhuck.github.io/}{Jeremy Huckins}, PhD (Fall 2019) is a Lecturer and Post\sphinxhyphen{}Doctoral researcher in the department of Psychological and Brain Sciences at Dartmouth College. He completed a BA in Neuroscience at Bowdoin College, worked with as a researcher with the \sphinxhref{https://king.med.harvard.edu/}{King Lab} at Harvard Medical School then completed a PhD in Experimental and Molecular Medicine at Dartmouth College. His current research program is focused on gaining insights into mental health using fMRI and mobile smartphone sensing.


\subsection{Teaching Assistants}
\label{\detokenize{content/Instructors:teaching-assistants}}
\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{saleki}.jpg}

\sphinxhref{https://sites.dartmouth.edu/peter/people/}{Sharif Saleki} (Fall 2020) is a graduate student at Dartmouth college working with Peter Tse.

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{gonzalez}.jpg}

\sphinxhref{https://github.com/BryanGonzalez262}{Bryan Gonzalez} (Spring 2020) is a graduate student at Dartmouth college working with Luke Chang. He received his BA and MA from NYU. After a stint as a producer in creative media industries, his interest in research began in the Social Relations Lab at Columbia University studying speech mimicry. He later spent time learning polysomnography at Weill Cornell before coming to NYU Langone as a senior research coordinator. There, his efforts focused on finding biological markers of PTSD. At Dartmouth, Bryan is primarily interested in the computational mechanisms underpinning theory of mind across perceptual, behavioral and cognitive domains. His research is strongly influenced by reinforcement learning models, and probes mental state attribution in action understanding, preference learning, and anthropomorphism in human\sphinxhyphen{}robot interaction. Bryan is also passionate about promoting diversity in STEM education. In his free time, he loves running, live music, and curling up with his cat, “Puppy”.

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{ziman}.png}

\sphinxhref{https://kirstensgithub.github.io/}{Kirsten Ziman} (Fall 2019) is a graduate student at Dartmouth college. She completed her bachelor’s in Neuroscience at the University of Southern California, and worked in research settings at USC and UCLA before joining the Dartmouth community. Currently, she is studying attention and memory with professor Jeremy Manning.

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{brietzke}.jpg}

\sphinxhref{http://www.dartmouth-socialneurolab.com/people}{Sasha Brietzke} (Spring 2019) is graduate student at Dartmouth College. She completed a BA at Johns Hopkins University and an IRTA postbac fellowship at the NIH. She currently works in the \sphinxhref{http://www.dartmouth-socialneurolab.com}{Dartmouth Social Neuroscience Lab} investigating the self through a social cognitive lens.


\section{Syllabus}
\label{\detokenize{content/Syllabus:syllabus}}\label{\detokenize{content/Syllabus::doc}}

\subsection{INSTRUCTORS}
\label{\detokenize{content/Syllabus:instructors}}
\sphinxstylestrong{Professor}: Luke Chang, PhD

Email: \sphinxurl{mailto:luke.j.chang@dartmouth.edu}

Office Hours: Tuesday 1\sphinxhyphen{}2pm

\sphinxstylestrong{Teaching Assistant}: Sharif Saleki

Email: \sphinxurl{mailto:sharif.saleki.gr@dartmouth.edu}

Office Hours: Monday \& Wednesday, 11:30 \sphinxhyphen{} 12:30 pm


\subsection{SPACE AND TIME}
\label{\detokenize{content/Syllabus:space-and-time}}
Remote Zoom Meetings
2A Tue/Thurs: 2:25\sphinxhyphen{}4:15
X\sphinxhyphen{}Hour Wed:  4:35\sphinxhyphen{}5:25


\subsection{CLASS FORMAT}
\label{\detokenize{content/Syllabus:class-format}}
The format of Psych60 will be slightly different than the typical in\sphinxhyphen{}person version of the course. All course materials will be made available online in the format of a jupyter book, https://dartbrains.org/. Lectures will primarily be delivered via publicly available pre\sphinxhyphen{}recorded lectures available on youtube. Each course module will have a jupyter book tutorial and an accompanying assignment that will require completing some type of programming task or analysis. Students will work within small groups throughout the term on their assignments. Scheduled classes will primarily take the form of a Q\&A where solutions to the assignments will be discussed and specific topics can be discussed in more detail. There will be a single exam, in which students will have to replicate an analysis of a published study using real data. For the final project, students will analyze an existing publicly available dataset to answer a new research question. Unfortunately, we will not be able to collect a new fMRI dataset as the Dartmouth Brain Imaging Center is currently closed for this term.


\subsection{ZOOM MEETINGS}
\label{\detokenize{content/Syllabus:zoom-meetings}}
All class meetings will be held on zoom. By participating in the zoom meetings, all students will consent to the following:

** (1) Consent to recording of course and group office hours**
\begin{itemize}
\item {} 
I affirm my understanding that this course and any associated group meetings involving students and the instructor, including but not limited to scheduled and ad hoc office hours and other consultations, may be recorded within any digital platform used to offer remote instruction for this course;

\item {} 
I further affirm that the instructor owns the copyright to their instructional materials, of which these recordings constitute a part, and distribution of any of these recordings in whole or in part without prior written consent of the instructor may be subject to discipline by Dartmouth up to and including expulsion;

\item {} 
I authorize Dartmouth and anyone acting on behalf of Dartmouth to record my participation and appearance in any medium, and to use my name, likeness, and voice in connection with such recording; and

\item {} 
I authorize Dartmouth and anyone acting on behalf of Dartmouth to use, reproduce, or distribute such recording without restrictions or limitation for any educational purpose deemed appropriate by Dartmouth and anyone acting on behalf of Dartmouth.

\end{itemize}

** (2) Requirement of consent to one\sphinxhyphen{}on\sphinxhyphen{}one recordings**
\begin{itemize}
\item {} 
By enrolling in this course, I hereby affirm that I will not under any circumstance make a recording in any medium of any one\sphinxhyphen{}on\sphinxhyphen{}one meeting with the instructor without obtaining the prior written consent of all those participating, and I understand that if I violate this prohibition, I will be subject to discipline by Dartmouth up to and including expulsion, as well as any other civil or criminal penalties under applicable law.

\end{itemize}


\subsection{COMMUNICATION}
\label{\detokenize{content/Syllabus:communication}}
For this class, we will have most of our discussion through the dart\sphinxhyphen{}psych60.slack.com channel.


\subsection{TEXTBOOK}
\label{\detokenize{content/Syllabus:textbook}}
{[}OPTIONAL{]} Lindquist, M. \& Wager, T (2015). Principles of fMRI. Available from https://leanpub.com/principlesoffmri.


\subsection{ONLINE VIDEOS}
\label{\detokenize{content/Syllabus:online-videos}}
Students are encouraged to watch assigned videos freely available online to supplement the classroom experience.  Most videos will be available on youtube from:
\begin{itemize}
\item {} 
\sphinxhref{https://www.youtube.com/channel/UC\_BIby85hZmcItMrkAlc8eA}{Principles of fMRI Course} by Tor Wager \& Martin Lindquist

\item {} 
\sphinxhref{http://mikexcohen.com/lectures.html}{Analyzing Neural Time Series Data: Principles \& Theory} By Mike X Cohen

\item {} 
\sphinxhref{https://www.youtube.com/channel/UCZ7gF0zm35FwrFpDND6DWeA}{Mumford Brain Stats} by Jeanette Mumford

\end{itemize}


\subsection{LECTURES}
\label{\detokenize{content/Syllabus:lectures}}
Lectures will primarily be delivered via freely available youtube videos. Students are expected to watch these on their own time and bring question to class or post on slack.


\subsection{READINGS}
\label{\detokenize{content/Syllabus:readings}}
Readings will supplement the online lectures and will be made available via Canvas.


\subsection{CLASS PARTICIPATION (15\% of grade)}
\label{\detokenize{content/Syllabus:class-participation-15-of-grade}}
You will be expected to participate in class discussions each day of class. This might include asking clarifying questions or helping another student.


\subsection{HOMEWORK (30\% of grade)}
\label{\detokenize{content/Syllabus:homework-30-of-grade}}
We will have occasional homework assignments based on lab assignments. You will be required to upload your jupyter notebook to the canvas site by midnight the day the homework is due.


\subsection{ANATOMY FLASH (5\% of grade)}
\label{\detokenize{content/Syllabus:anatomy-flash-5-of-grade}}
In each class, a student will present a very brief presentation on one region of the brain. Flash presentations will include how to identify the region anatomically, a quick overview of its function, and a brief example of an interesting imaging study that identified a functional property of the region. Presentations should be developed on google slides and should be between 2\sphinxhyphen{}5 minutes.


\subsection{EXAM (20\% of grade)}
\label{\detokenize{content/Syllabus:exam-20-of-grade}}
To ensure that students learn key concepts about the principles of fMRI data analysis, we will have one exam that will involve analyzing a new dataset to demonstrate competence of analysis skills.


\subsection{ANALYSIS PROJECT (30\% of grade)}
\label{\detokenize{content/Syllabus:analysis-project-30-of-grade}}
A key component of this course is learning how to process and analyze imaging data. Students will be introduced to key concepts during the laboratory assignments. Students will be expected to apply what they learn to analyzing an fMRI dataset. Most students will likely analyze the data collected in class, but are free to analyze any publicly available dataset (e.g., https://openfmri.org/, https://www.datalad.org/datasets.html). Students may work in small groups (\textasciitilde{}2\sphinxhyphen{}3 people), but each will independently write a final report of the research.  The final written paper should be in journal format using APA style with an abstract, intro, methods, results, discussion, and references. Format 12\sphinxhyphen{}20 typed double\sphinxhyphen{}space pages, 11pt Ariel or Times font. (bibliography not included in the page limit).  Each person is expected to write their own intro and conclusion, but the group can collaborate on the methods and results sections if they want.  At the end of the class each group will give a \textasciitilde{}10 minute presentation on their project (background, hypothesis, experimental design, results, analyses, conclusions).  Paper Due at Midnight on the last day of class.


\subsection{CLASSROOM POLICIES}
\label{\detokenize{content/Syllabus:classroom-policies}}

\subsubsection{HONOR CODE}
\label{\detokenize{content/Syllabus:honor-code}}
Students are expected to strictly adhere to the Dartmouth Academic Honor Principle. As described in the Student Handbook, fundamental to the principle of independent learning is the requirement of honesty and integrity in the performance of academic assignments, both in the classroom and outside. Dartmouth operates on the principle of academic honor. Students who submit work that is not their own or who commit other acts of academic dishonesty will forfeit the opportunity to continue at Dartmouth. If you have questions or concerns regarding this policy during the course, please contact Professor Chang.


\subsubsection{PLAGIARISM}
\label{\detokenize{content/Syllabus:plagiarism}}
Writing about scientific publications without just rephrasing is difficult, particularly when not everything is fully understood. Doing this properly takes time and practice, and one goal of the course is to move us in that direction. I don’t expect to see a perfect scientific treatment at this stage. But I do want to see evidence of independent thought when considering the material and implications (rather than just regurgitating it), and some degree of creativity. When quoting, be sure appropriate citations are made.


\subsubsection{MISSED ASSIGNMENTS}
\label{\detokenize{content/Syllabus:missed-assignments}}
A student will only be excused from an assignment by permission of the Instructor and on the basis of a written note from a dean, doctor, or supervisor of official college\sphinxhyphen{}sponsored events being held off\sphinxhyphen{}campus and requiring a students’ absence. If excused, a make\sphinxhyphen{}up must be taken as soon as possible (usually within 1 day of the originally\sphinxhyphen{}scheduled exam/assignment date).


\subsubsection{LATE ASSIGNMENTS}
\label{\detokenize{content/Syllabus:late-assignments}}
All papers and presentations are due at the date and time specified.  Scores for late papers will be reduced by 10\% for every 24\sphinxhyphen{}hour period a paper is late. No extensions will be granted due to computer failure, roommate difficulties, printing problems, etc.  According to College policy, there are no excused absences from class for participation in College\sphinxhyphen{}sponsored extracurricular activities.


\subsubsection{DISABILITIES}
\label{\detokenize{content/Syllabus:disabilities}}
Any student with a documented disability needing academic adjustments or accommodations is requested to speak with me by the end of the second week of the term. All discussions will remain confidential, although the Academic Skills Center may be consulted to verify the documentation of the disability.


\subsubsection{MENTAL HEALTH}
\label{\detokenize{content/Syllabus:mental-health}}
The academic environment at Dartmouth is challenging, our terms are intensive, and classes are not the only demanding part of your life. There are a number of resources available to you on campus to support your wellness, including your \sphinxhref{https://students.dartmouth.edu/undergraduate-deans/}{undergraduate dean}, \sphinxhref{https://students.dartmouth.edu/health-service/counseling/about}{Counseling and Human Development}, and the \sphinxhref{https://students.dartmouth.edu/wellness-center/}{Student Wellness Center}. I encourage you to use these resources to take care of yourself throughout the term, and email me if you experience any difficulties.


\subsubsection{RELIGIOUS OBSERVANCES}
\label{\detokenize{content/Syllabus:religious-observances}}
Some students may wish to take part in religious observances that occur during this academic term. If you have a religious observance which conflicts with your participation in the course, please meet with me by the end of the second week of the term to discuss appropriate accommodations.


\subsubsection{TITLE IX}
\label{\detokenize{content/Syllabus:title-ix}}
At Dartmouth, we value integrity, responsibility, and respect for the rights and interests of others, all central to our Principles of Community. We are dedicated to establishing and maintaining a safe and inclusive campus where all have equal access to the educational and employment opportunities Dartmouth offers. We strive to promote an environment of sexual respect, safety, and well\sphinxhyphen{}being. In its policies and standards, Dartmouth demonstrates unequivocally that sexual assault, gender\sphinxhyphen{}based harassment, domestic violence, dating violence, and stalking are not tolerated in our community.

\sphinxhref{https://sexual-respect.dartmouth.edu}{The Sexual Respect Website} at Dartmouth provides a wealth of information on your rights with regard to sexual respect and resources that are available to all in our community.

Please note that, as a faculty member, I am obligated to share disclosures regarding conduct under Title IX with Dartmouth’s Title IX Coordinator. Confidential resources are also available, and include licensed medical or counseling professionals (e.g., a licensed psychologist), staff members of organizations recognized as rape crisis centers under state law (such as WISE), and ordained clergy (see \sphinxurl{https://dartgo.org/titleix\_resources}).

Should you have any questions, please feel free to contact Dartmouth’s Title IX Coordinator or the Deputy Title IX Coordinator for the Guarini School. Their contact information can be found on the sexual respect website at: \sphinxurl{https://sexual-respect.dartmouth.edu}.


\section{Schedule}
\label{\detokenize{content/Schedule:schedule}}\label{\detokenize{content/Schedule::doc}}
\sphinxstylestrong{Measurement and Signal}
\begin{itemize}
\item {} 
Readings: Huettel, ch 7

\item {} 
Videos: Principles of fMRI Modules 5\sphinxhyphen{}8

\item {} 
Lab: Introduction to Python

\item {} 
Lab: Introduction to Data Frames \& Plotting

\end{itemize}

\sphinxstylestrong{Image Processing}
\begin{itemize}
\item {} 
Readings: Poldrack ch 1,2

\item {} 
Videos: Principles of fMRI Modules 9

\item {} 
Lab: Introduction to Neuroimaging Data

\item {} 
Lab: Signal vs Noise with ICA

\end{itemize}

\sphinxstylestrong{Signal Processing}
\begin{itemize}
\item {} 
Readings: Cohen ch 10, 11

\item {} 
Videos: Cohen lecturelets

\item {} 
Lab: Signal Processing Basics

\end{itemize}

\sphinxstylestrong{Data Preprocessing}
\begin{itemize}
\item {} 
Readings: Poldrack ch 3,4

\item {} 
Videos: Principles of fMRI Modules 13\sphinxhyphen{}14

\item {} 
Lab: Preprocessing with Nipype Quickstart

\item {} 
Lab: Building Preprocessing Worklows with Nipype

\item {} 
Lab: Automated Preprocessing with fMRIprep

\end{itemize}

\sphinxstylestrong{General Linear Model}
\begin{itemize}
\item {} 
Readings: Poldrack ch 5

\item {} 
Videos: Principles of fMRI Modules 15\sphinxhyphen{}22

\item {} 
Lab: Introduction to the General Linear Model

\item {} 
Lab: Modeling Single Subject Data

\end{itemize}

\sphinxstylestrong{Group Analysis}
\begin{itemize}
\item {} 
Readings: Poldrack ch 6

\item {} 
Videos: Principles of fMRI Modules 23\sphinxhyphen{}25

\item {} 
Lab:

\end{itemize}

\sphinxstylestrong{Multiple Comparisons}
\begin{itemize}
\item {} 
Readings: Poldrack ch 7

\item {} 
Videos: Principles of fMRI Modules 26\sphinxhyphen{}29

\item {} 
Lab:

\end{itemize}

\sphinxstylestrong{Connectivity}
\begin{itemize}
\item {} 
Readings: Poldrack ch 8

\item {} 
Videos:

\item {} 
Lab:

\end{itemize}

\sphinxstylestrong{Prediction/Classification}
\begin{itemize}
\item {} 
Readings: Poldrack ch 9

\item {} 
Videos:

\item {} 
Lab:

\end{itemize}

\sphinxstylestrong{Representational Similarity Analysis}
\begin{itemize}
\item {} 
Readings: Haxby et al., 2014; Kriegeskorte et al., 2008

\item {} 
Videos:

\item {} 
Lab:

\end{itemize}

\sphinxstylestrong{Intersubject Synchrony}
\begin{itemize}
\item {} 
Readings: Hasson et al., 2011; Nummenmaa et al, 2018

\item {} 
Videos:

\item {} 
Lab:

\end{itemize}


\section{Introduction to JupyterHub}
\label{\detokenize{content/Introduction_to_JupyterHub:introduction-to-jupyterhub}}\label{\detokenize{content/Introduction_to_JupyterHub::doc}}
\sphinxstyleemphasis{Written by Luke Chang \& Jeremy Huckins}

In this course we will primarily be using python to learn about fMRI data analysis. All of the laboratories can be run on your own individual laptops once you have installed Python (preferably via an \sphinxhref{https://www.anaconda.com/distribution/}{anaconda distribution}. However, the datasets are large and there can be annoying issues with different versions of packages and installing software across different operating systems. We will also occasionally be using additional software that will be called by Python (e.g., preprocessing). We have a docker container available that will contain all of the software and have created tutorials to \sphinxhref{https://dartbrains.org/features/notebooks/Download\_Localizer\_Data.html}{download the data}. In addition, some of the analyses we will run can be very computationally expensive and may exceed the capabilities of your laptop.

To meet these needs, Dartmouth’s Research Computing has generously provided a dedicated server hosted on Amazon Web Services that will allow us to store data, access specialized software, and run analyses. This means that everyone should be able to run all of the tutorials on their laptops, tablets, etc by accessing notebooks on the jupyterhub server and will not need to install anything beyond a working browser.


\subsection{Login}
\label{\detokenize{content/Introduction_to_JupyterHub:login}}
The main portal to access this resource will be through the Jupyterhub interface. This allows you to remotely login in to the server through your browser at https://jhub.dartmouth.edu using your netid. Please let us know if you are having difficulty logging in.

Once you’ve logged in you should see a screen like this.

\sphinxincludegraphics{{jhub}.png}

The \sphinxcode{\sphinxupquote{Psych60}} folder contains all of the relevant notebooks and data for the course.

Every time you login jupyterhub will spin up a new server just for you and will update all of the files.


\subsection{Server}
\label{\detokenize{content/Introduction_to_JupyterHub:server}}
Every student will be able to have their own personal server to work on. This server is running on AWS cloud computing and should have all of the software you need to run the tutorials. If your server is idle for 10 minutes, it will automatically shut down. There are also a limited amount of resources available (e.g., storage, RAM). Each user has access to 512mb of RAM, keep an eye on how much your jobs are using. The server may crash if it exceeds 512mb.


\subsection{Jupyter Notebooks}
\label{\detokenize{content/Introduction_to_JupyterHub:jupyter-notebooks}}
Jupyter notebooks are a great way to have your code, comments and results show up inline in a web browser. Work for this class will be done in Jupyter notebooks so you can reference what you have done, see the results and someone else could redo it in the future, similar to a typical lab notebook.

Rather than writing and re\sphinxhyphen{}writing an entire program, you can write lines of code and run them one at a time. Then, if you need to make a change, you can go back and make your edit and rerun the program again, all in the same window. In our specific case, we are going to use JupyterHub which lets several people access the same computer and data at the same time through a web browser.

Finally, you can view examples and share your work with the world very easily through \sphinxhref{https://nbviewer.jupyter.org}{nbviewer}.  One easy trick if you use a cloud storage service like dropbox is to paste a link to the dropbox file in nbviewer.  These links will persist as long as the file remains being shared via dropbox.

\sphinxstyleemphasis{\sphinxstylestrong{Do not work directly on the notebooks in the \sphinxcode{\sphinxupquote{Psych60/notebook}} folder}}. These will always be updating as I edit them. Instead, make sure you copy the notebooks you are working on to \sphinxstylestrong{your own personal folder}. These can only be changed by you and won’t be deleted or updated when your server starts.


\subsubsection{Opening a notebook on the server}
\label{\detokenize{content/Introduction_to_JupyterHub:opening-a-notebook-on-the-server}}
Click on Files, then Psych60, then notebooks. Click on any notebook you would like to load. Make sure you copy the notebook to another location outside of Psych60 to make sure your work won’t be deleted.

For example, our first laboratory will be \sphinxstylestrong{1\_Introduction\_to\_Programming.ipynb}.


\subsubsection{Copying Notebook}
\label{\detokenize{content/Introduction_to_JupyterHub:copying-notebook}}
Make sure you copy your notebook to a different directory to make sure it will not be erased when you restart your server.

First, you will need to create a folder called \sphinxcode{\sphinxupquote{Homework}}. Click on \sphinxcode{\sphinxupquote{New}} then \sphinxcode{\sphinxupquote{Folder}}.
\sphinxincludegraphics{{create_homework_folder}.png}

Second, you will need to rename folder. \sphinxcode{\sphinxupquote{Check}} the box next to the new untitled folder, then click \sphinxcode{\sphinxupquote{Rename}}, then type \sphinxcode{\sphinxupquote{Homework}}
\sphinxincludegraphics{{rename_folder}.png}

Third, for all notebooks you will need to save a copy into your homework directory. Go to \sphinxcode{\sphinxupquote{File}} then \sphinxcode{\sphinxupquote{Save as}} then type in \sphinxcode{\sphinxupquote{psych60/notebooks/Homework}}. You will need to do this for each new notebook assignment.
\sphinxincludegraphics{{save_notebook}.png}


\subsection{Alternative to Jupyterhub}
\label{\detokenize{content/Introduction_to_JupyterHub:alternative-to-jupyterhub}}
If you use jupyter notebooks on your own computer then you own computer will be doing the processing. If you put your computer to sleep then processing will stop. It will also likely slow down other programs you are using on your computer. I would recommend installing it on your own computer so you can learn more about how to use it, or if you are interested in tinkering with the software or you happen to have a particularly fast/newer computer. We don’t recommend going this route unless you don’t have reliable access to the internet.

Please contact Professor Chang if you want any assistance doing this.


\subsubsection{Installing Jupyter Notebooks on your own computer}
\label{\detokenize{content/Introduction_to_JupyterHub:installing-jupyter-notebooks-on-your-own-computer}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Install python. We recommend using the \sphinxhref{https://www.anaconda.com/distribution/}{Acaconda Distribution} as it comes with most of the relevant scientific computing packages we will be using.  Be sure to download Python 3.

\end{enumerate}

Alternative 1: Install jupyter notebook (it comes with Anaconda)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{jupyter}
\end{sphinxVerbatim}

Alternative 2: If you already have python installed:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{upgrade} \PYG{n}{pip}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{jupyter}
\end{sphinxVerbatim}


\subsubsection{Starting Jupter Notebooks on your computer}
\label{\detokenize{content/Introduction_to_JupyterHub:starting-jupter-notebooks-on-your-computer}}
Open a terminal, navigate to the directory you want to work from then type \sphinxcode{\sphinxupquote{jupyter notebook}} or \sphinxcode{\sphinxupquote{jupyter lab}}


\subsection{Plotting and Atlases}
\label{\detokenize{content/Introduction_to_JupyterHub:plotting-and-atlases}}
For most of our labs we will be using Python to plot our data and results.  However, it is often useful to have a more interactive experience.  We recommend additionally downloading \sphinxhref{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSLeyes}{FSLeyes}, which is a standalone image viewer developed by FSL.  It can be installed by either downloading directly from the website, or using \sphinxcode{\sphinxupquote{pip}}.

\sphinxcode{\sphinxupquote{pip install fsleyes}}

If you are using a mac, you will likely also need to add an X11 window system such as \sphinxhref{https://www.xquartz.org/}{xQuartz} for the viewer to work properly.


\subsection{References}
\label{\detokenize{content/Introduction_to_JupyterHub:references}}
\sphinxhref{https://365datascience.com/the-jupyter-dashboard-a-walkthrough/}{Jupyter Dashboard Walkthrough}

\sphinxhref{https://jupyter.readthedocs.io/en/latest/running.html\#running}{Jupyter Notebook Manual}

\sphinxhref{https://medium.com/codingthesmartway-com-blog/getting-started-with-jupyter-notebook-for-python-4e7082bd5d46}{Getting Started With Jupyter Notebook}

\sphinxhref{https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet}{Markdown Cheatsheet}

\sphinxhref{https://github.com/datitran/jupyter2slides}{Convert jupyter notebook to slides}


\section{Download Data}
\label{\detokenize{content/Download_Data:download-data}}\label{\detokenize{content/Download_Data::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

Many of the imaging tutorials throughout this course will use open data from the Pinel Localizer task.

The Pinel Localizer task was designed to probe several different types of basic cognitive processes, such as visual perception, finger tapping, language, and math. Several of the tasks are cued by reading text on the screen (i.e., visual modality) and also by hearing auditory instructions (i.e., auditory modality). The trials are randomized across conditions and have been optimized to maximize efficiency for a rapid event related design. There are 100 trials in total over a 5\sphinxhyphen{}minute scanning session. Read the original \sphinxhref{https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-8-91}{paper} for more specific details about the task and the \sphinxhref{https://doi.org/10.1016/j.neuroimage.2015.09.052}{dataset paper}.

This dataset is well suited for these tutorials as it is (a) publicly available to anyone in the world, (b) relatively small (only about 5min), and (c) provides many options to create different types of contrasts.

There are a total of 94 subjects available, but we will primarily only be working with a smaller subset of about 15.

Though the data is being shared on the \sphinxhref{https://osf.io/vhtf6/files/}{OSF website}, we recommend downloading it from our \sphinxhref{https://gin.g-node.org/ljchang/Localizer}{g\sphinxhyphen{}node repository} as we have fixed a few issues with BIDS formatting and have also performed preprocessing using fmriprep.

In this notebook, we will walk through how to access the datset using DataLad. Note, that the entire dataset is fairly large (\textasciitilde{}42gb), but the tutorials will mostly only be working with a small portion of the data, so no need to download the entire thing. If you are taking the Psych60 course at Dartmouth, we have already made the data available on the jupyterhub server.


\subsection{DataLad}
\label{\detokenize{content/Download_Data:datalad}}
The easist way to access the data is using \sphinxhref{https://www.datalad.org/}{DataLad}, which is an open source version control system for data built on top of \sphinxhref{https://git-annex.branchable.com/}{git\sphinxhyphen{}annex}. Think of it like git for data. It provides a handy command line interface for downloading data, tracking changes, and sharing it with others.

While DataLad offers a number of useful features for working with datasets, there are three in particular that we think make it worth the effort to install for this course.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Cloning a DataLad Repository can be completed with a single line of code \sphinxcode{\sphinxupquote{datalad clone \textless{}repository\textgreater{}}} and provides the full directory structure in the form of symbolic links. This allows you to explore all of the files in the dataset, without having to download the entire dataset at once.

\item {} 
Specific files can be easily downloaded using \sphinxcode{\sphinxupquote{datalad get \textless{}filename\textgreater{}}}, and files can be removed from your computer at any time using \sphinxcode{\sphinxupquote{datalad drop \textless{}filename\textgreater{}}}. As these datasets are large, this will allow you to only work with the data that you need for a specific tutorial and you can drop the rest when you are done with it.

\item {} 
All of the DataLad commands can be run within Python using the datalad \sphinxhref{http://docs.datalad.org/en/latest/modref.html}{python api}.

\end{enumerate}

We will only be covering a few basic DataLad functions to get and drop data. We encourage the interested reader to read the very comprehensive DataLad \sphinxhref{http://handbook.datalad.org/en/latest/}{User Handbook} for more details and troubleshooting.


\subsubsection{Installing Datalad}
\label{\detokenize{content/Download_Data:installing-datalad}}
DataLad can be easily installed using \sphinxhref{https://pip.pypa.io/en/stable/}{pip}.

\sphinxcode{\sphinxupquote{pip install datalad}}

Unfortunately, it currently requires manually installing the \sphinxhref{https://git-annex.branchable.com/}{git\sphinxhyphen{}annex} dependency, which is not automatically installed using pip.

If you are using OSX, we recommend installing git\sphinxhyphen{}annex using \sphinxhref{https://brew.sh/}{homebrew} package manager.

\sphinxcode{\sphinxupquote{brew install git\sphinxhyphen{}annex}}

If you are on Debian/Ubuntu we recommend enabling the \sphinxhref{http://neuro.debian.net/}{NeuroDebian} repository and installing with apt\sphinxhyphen{}get.

\sphinxcode{\sphinxupquote{sudo apt\sphinxhyphen{}get install datalad}}

For more installation options, we recommend reading the DataLad \sphinxhref{https://git-annex.branchable.com/}{installation instructions}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{!}pip install datalad
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Requirement already satisfied: datalad in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (0.12.6)
Requirement already satisfied: msgpack in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (1.0.0)
Requirement already satisfied: appdirs in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (1.4.3)
Requirement already satisfied: chardet\PYGZgt{}=3.0.4 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (3.0.4)
Requirement already satisfied: keyring\PYGZgt{}=8.0 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (21.4.0)
Requirement already satisfied: GitPython\PYGZgt{}=2.1.12 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (3.1.0)
Requirement already satisfied: fasteners in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (0.15)
Requirement already satisfied: jsmin in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (2.2.2)
Requirement already satisfied: iso8601 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (0.1.12)
Requirement already satisfied: keyrings.alt in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (3.4.0)
Requirement already satisfied: patool\PYGZgt{}=1.7 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (1.12)
Requirement already satisfied: wrapt in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (1.11.2)
Requirement already satisfied: tqdm in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (4.48.2)
Requirement already satisfied: whoosh in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (2.7.4)
Requirement already satisfied: boto in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (2.49.0)
Requirement already satisfied: simplejson in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (3.17.0)
Requirement already satisfied: PyGithub in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (1.47)
Requirement already satisfied: humanize in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (2.4.0)
Requirement already satisfied: requests\PYGZgt{}=1.2 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from datalad) (2.24.0)
Requirement already satisfied: importlib\PYGZhy{}metadata; python\PYGZus{}version \PYGZlt{} \PYGZdq{}3.8\PYGZdq{} in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from keyring\PYGZgt{}=8.0\PYGZhy{}\PYGZgt{}datalad) (1.7.0)
Requirement already satisfied: gitdb\PYGZlt{}5,\PYGZgt{}=4.0.1 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from GitPython\PYGZgt{}=2.1.12\PYGZhy{}\PYGZgt{}datalad) (4.0.2)
Requirement already satisfied: monotonic\PYGZgt{}=0.1 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from fasteners\PYGZhy{}\PYGZgt{}datalad) (1.5)
Requirement already satisfied: six in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from fasteners\PYGZhy{}\PYGZgt{}datalad) (1.15.0)
Requirement already satisfied: pyjwt in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from PyGithub\PYGZhy{}\PYGZgt{}datalad) (1.7.1)
Requirement already satisfied: deprecated in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from PyGithub\PYGZhy{}\PYGZgt{}datalad) (1.2.9)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\PYGZlt{}1.26,\PYGZgt{}=1.21.1 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from requests\PYGZgt{}=1.2\PYGZhy{}\PYGZgt{}datalad) (1.25.10)
Requirement already satisfied: certifi\PYGZgt{}=2017.4.17 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from requests\PYGZgt{}=1.2\PYGZhy{}\PYGZgt{}datalad) (2020.6.20)
Requirement already satisfied: idna\PYGZlt{}3,\PYGZgt{}=2.5 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from requests\PYGZgt{}=1.2\PYGZhy{}\PYGZgt{}datalad) (2.10)
Requirement already satisfied: zipp\PYGZgt{}=0.5 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from importlib\PYGZhy{}metadata; python\PYGZus{}version \PYGZlt{} \PYGZdq{}3.8\PYGZdq{}\PYGZhy{}\PYGZgt{}keyring\PYGZgt{}=8.0\PYGZhy{}\PYGZgt{}datalad) (3.1.0)
Requirement already satisfied: smmap\PYGZlt{}4,\PYGZgt{}=3.0.1 in /Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages (from gitdb\PYGZlt{}5,\PYGZgt{}=4.0.1\PYGZhy{}\PYGZgt{}GitPython\PYGZgt{}=2.1.12\PYGZhy{}\PYGZgt{}datalad) (3.0.1)
\end{sphinxVerbatim}


\subsubsection{Download Data with DataLad}
\label{\detokenize{content/Download_Data:download-data-with-datalad}}
The Pinel localizer dataset can be accessed at the following location https://gin.g\sphinxhyphen{}node.org/ljchang/Localizer/. To download the Localizer dataset run \sphinxcode{\sphinxupquote{datalad install https://gin.g\sphinxhyphen{}node.org/ljchang/Localizer}} in a terminal in the location where you would like to install the dataset. Don’t forget to change the directory to a folder on your local computer. The full dataset is approximately 42gb.

You can run this from the notebook using the \sphinxcode{\sphinxupquote{!}} cell magic.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{cd} \PYGZti{}/Dropbox/Dartbrains/data

\PYG{o}{!}datalad install https://gin.g\PYGZhy{}node.org/ljchang/Localizer
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/Dropbox/Dartbrains/data

\end{sphinxVerbatim}


\subsection{Datalad Basics}
\label{\detokenize{content/Download_Data:datalad-basics}}
You might be surprised to find that after cloning the dataset that it barely takes up any space \sphinxcode{\sphinxupquote{du \sphinxhyphen{}sh}}. This is because cloning only downloads the metadata of the dataset to see what files are included.

You can check to see how big the entire dataset would be if you downloaded everything using \sphinxcode{\sphinxupquote{datalad status}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{cd} \PYGZti{}/Dropbox/Dartbrains/data/Localizer

\PYG{o}{!}datalad status \PYGZhy{}\PYGZhy{}annex
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/Dropbox/Dartbrains/data/Localizer
1794 annex\PYGZsq{}d files (42.1 GB recorded total size)

\end{sphinxVerbatim}


\subsubsection{Getting Data}
\label{\detokenize{content/Download_Data:getting-data}}
One of the really nice features of datalad is that you can see all of the data without actually storing it on your computer. When you want a specific file you use \sphinxcode{\sphinxupquote{datalad get \textless{}filename\textgreater{}}} to download that specific file. Importantly, you do not need to download all of the dat at once, only when you need it.

Now that we have cloned the repository we can grab individual files. For example, suppose we wanted to grab the first subject’s confound regressors generated by fmriprep.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{!}datalad get participants.tsv
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}

Now we can check and see how much of the total dataset we have downloaded using \sphinxcode{\sphinxupquote{datalad status}}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{!}datalad status \PYGZhy{}\PYGZhy{}annex all
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1794 annex\PYGZsq{}d files (0.0 B/42.1 GB present/total size)

\end{sphinxVerbatim}

If you would like to download all of the files you can use \sphinxcode{\sphinxupquote{datalad get .}}. Depending on the size of the dataset and the speed of your internet connection, this might take awhile. One really nice thing about datalad is that if your connection is interrupted you can simply run \sphinxcode{\sphinxupquote{datalad get .}} again, and it will resume where it left off.

You can also install the dataset and download all of the files with a single command \sphinxcode{\sphinxupquote{datalad install \sphinxhyphen{}g https://gin.g\sphinxhyphen{}node.org/ljchang/Localizer}}. You may want to do this if you have a lot of storage available and a fast internet connection. For most people, we recommend only downloading the files you need for a specific tutorial.


\subsubsection{Dropping Data}
\label{\detokenize{content/Download_Data:dropping-data}}
Most people do not have unlimited space on their hard drives and are constantly looking for ways to free up space when they are no longer actively working with files. Any file in a dataset can be removed using \sphinxcode{\sphinxupquote{datalad drop}}. Importantly, this does not delete the file, but rather removes it from your computer. You will still be able to see file metadata after it has been dropped in case you want to download it again in the future.

As an example, let’s drop the Localizer participants .tsv file.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{!}datalad drop participants.tsv
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\end{sphinxVerbatim}


\subsection{Datalad has a Python API!}
\label{\detokenize{content/Download_Data:datalad-has-a-python-api}}
One particularly nice aspect of datalad is that it has a Python API, which means that anything you would like to do with datalad in the commandline, can also be run in Python. See the details of the datalad \sphinxhref{http://docs.datalad.org/en/latest/modref.html}{Python API}.

For example, suppose you would like to clone a data repository, such as the Localizer dataset. You can run \sphinxcode{\sphinxupquote{dl.clone(source=url, path=location)}}. Make sure you set \sphinxcode{\sphinxupquote{localizer\_path}} to the location where you would like the Localizer repository installed.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{datalad}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k}{as} \PYG{n+nn}{dl}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{n}{localizer\PYGZus{}path} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/Users/lukechang/Dropbox/Dartbrains/data/Localizer}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{dl}\PYG{o}{.}\PYG{n}{clone}\PYG{p}{(}\PYG{n}{source}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://gin.g\PYGZhy{}node.org/ljchang/Localizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{n}{localizer\PYGZus{}path}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[WARNING] realpath of PWD=/ is / whenever os.getcwd()=/Users/lukechang/Dropbox/Dartbrains/data/Localizer. From now on will be returning os.getcwd(). Directory symlinks in the paths will be resolved 
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Dataset path=/Users/lukechang/Dropbox/Dartbrains/data/Localizer\PYGZgt{}
\end{sphinxVerbatim}

We can now create a dataset instance using \sphinxcode{\sphinxupquote{dl.Dataset(path\_to\_data)}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ds} \PYG{o}{=} \PYG{n}{dl}\PYG{o}{.}\PYG{n}{Dataset}\PYG{p}{(}\PYG{n}{localizer\PYGZus{}path}\PYG{p}{)}
\end{sphinxVerbatim}

How much of the dataset have we downloaded?  We can check the status of the annex using \sphinxcode{\sphinxupquote{ds.status(annex=\textquotesingle{}all\textquotesingle{})}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{results} \PYG{o}{=} \PYG{n}{ds}\PYG{o}{.}\PYG{n}{status}\PYG{p}{(}\PYG{n}{annex}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{all}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1794 annex\PYGZsq{}d files (0.0 B/42.1 GB present/total size)
1794 annex\PYGZsq{}d files (0.0 B/42.1 GB present/total size)
\end{sphinxVerbatim}

Looks like it’s empty, which makes sense since we only cloned the dataset.

Now we need to get some data. Let’s start with something small to play with first.

Let’s use \sphinxcode{\sphinxupquote{glob}} to find all of the tab\sphinxhyphen{}delimited confound data generated by fmriprep.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{localizer\PYGZus{}path}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{file\PYGZus{}list}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S01/func/sub\PYGZhy{}S01\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}confounds\PYGZus{}regressors.tsv\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S02/func/sub\PYGZhy{}S02\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}confounds\PYGZus{}regressors.tsv\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S03/func/sub\PYGZhy{}S03\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}confounds\PYGZus{}regressors.tsv\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S04/func/sub\PYGZhy{}S04\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}confounds\PYGZus{}regressors.tsv\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S05/func/sub\PYGZhy{}S05\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}confounds\PYGZus{}regressors.tsv\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S06/func/sub\PYGZhy{}S06\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}confounds\PYGZus{}regressors.tsv\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S07/func/sub\PYGZhy{}S07\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}confounds\PYGZus{}regressors.tsv\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S08/func/sub\PYGZhy{}S08\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}confounds\PYGZus{}regressors.tsv\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S09/func/sub\PYGZhy{}S09\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}confounds\PYGZus{}regressors.tsv\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S10/func/sub\PYGZhy{}S10\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}confounds\PYGZus{}regressors.tsv\PYGZsq{}]
\end{sphinxVerbatim}

glob can search the filetree and see all of the relevant data even though none of it has been downloaded yet.

Let’s now download the first subjects confound regressor file and load it using pandas.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{result} \PYG{o}{=} \PYG{n}{ds}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{file\PYGZus{}list}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{confounds} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{file\PYGZus{}list}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{confounds}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           csf  csf\PYGZus{}derivative1  csf\PYGZus{}derivative1\PYGZus{}power2    csf\PYGZus{}power2  \PYGZbs{}
0  5164.630182              NaN                     NaN  2.667340e+07   
1  5178.481411        13.851229              191.856548  2.681667e+07   
2  5161.040643       \PYGZhy{}17.440768              304.180395  2.663634e+07   
3  5150.604178       \PYGZhy{}10.436465              108.919794  2.652872e+07   
4  5172.441161        21.836983              476.853810  2.675415e+07   

   white\PYGZus{}matter  white\PYGZus{}matter\PYGZus{}derivative1  white\PYGZus{}matter\PYGZus{}power2  \PYGZbs{}
0   4006.007667                       NaN         1.604810e+07   
1   4011.819383                  5.811716         1.609469e+07   
2   4006.766409                 \PYGZhy{}5.052974         1.605418e+07   
3   4008.586021                  1.819612         1.606876e+07   
4   4007.189291                 \PYGZhy{}1.396730         1.605757e+07   

   white\PYGZus{}matter\PYGZus{}derivative1\PYGZus{}power2  global\PYGZus{}signal  global\PYGZus{}signal\PYGZus{}derivative1  \PYGZbs{}
0                              NaN    3753.537871                        NaN   
1                        33.776043    3760.408417                   6.870546   
2                        25.532548    3756.426086                  \PYGZhy{}3.982332   
3                         3.310987    3751.566090                  \PYGZhy{}4.859996   
4                         1.950854    3746.298200                  \PYGZhy{}5.267890   

   ...  rot\PYGZus{}x\PYGZus{}derivative1\PYGZus{}power2  rot\PYGZus{}x\PYGZus{}power2     rot\PYGZus{}y  rot\PYGZus{}y\PYGZus{}derivative1  \PYGZbs{}
0  ...                       NaN  4.016403e\PYGZhy{}07  0.000344                NaN   
1  ...              8.622980e\PYGZhy{}09  2.925631e\PYGZhy{}07  0.000569           0.000225   
2  ...              6.975673e\PYGZhy{}08  6.480347e\PYGZhy{}07  0.000655           0.000086   
3  ...              1.673784e\PYGZhy{}07  1.567265e\PYGZhy{}07  0.000554          \PYGZhy{}0.000101   
4  ...              2.102616e\PYGZhy{}08  2.925631e\PYGZhy{}07  0.000997           0.000443   

   rot\PYGZus{}y\PYGZus{}derivative1\PYGZus{}power2  rot\PYGZus{}y\PYGZus{}power2     rot\PYGZus{}z  rot\PYGZus{}z\PYGZus{}derivative1  \PYGZbs{}
0                       NaN  1.180596e\PYGZhy{}07 \PYGZhy{}0.000701                NaN   
1              5.063355e\PYGZhy{}08  3.233253e\PYGZhy{}07 \PYGZhy{}0.000776          \PYGZhy{}0.000075   
2              7.409422e\PYGZhy{}09  4.286255e\PYGZhy{}07 \PYGZhy{}0.000524           0.000253   
3              1.011674e\PYGZhy{}08  3.070412e\PYGZhy{}07 \PYGZhy{}0.000605          \PYGZhy{}0.000082   
4              1.959195e\PYGZhy{}07  9.934926e\PYGZhy{}07 \PYGZhy{}0.000840          \PYGZhy{}0.000235   

   rot\PYGZus{}z\PYGZus{}derivative1\PYGZus{}power2  rot\PYGZus{}z\PYGZus{}power2  
0                       NaN  4.914346e\PYGZhy{}07  
1              5.666476e\PYGZhy{}09  6.026417e\PYGZhy{}07  
2              6.390582e\PYGZhy{}08  2.740564e\PYGZhy{}07  
3              6.722360e\PYGZhy{}09  3.666230e\PYGZhy{}07  
4              5.510428e\PYGZhy{}08  7.059982e\PYGZhy{}07  

[5 rows x 136 columns]
\end{sphinxVerbatim}

What if we wanted to drop that file? Just like the CLI, we can use \sphinxcode{\sphinxupquote{ds.drop(file\_name)}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{result} \PYG{o}{=} \PYG{n}{ds}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{file\PYGZus{}list}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

To confirm that it is actually removed, let’s try to load it again with pandas.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{confounds} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{file\PYGZus{}list}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

Looks like it was successfully removed.

We can also load the entire dataset in one command if want using \sphinxcode{\sphinxupquote{ds.get(dataset=\textquotesingle{}.\textquotesingle{}, recursive=True)}}. We are not going to do it right now as this will take awhile and require lots of free hard disk space.

Let’s actually download one of the files we will be using in the tutorial. First, let’s use glob to get a list of all of the functional data that has been preprocessed by fmriprep, denoised, and smoothed.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{localizer\PYGZus{}path}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{file\PYGZus{}list}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S01/func/sub\PYGZhy{}S01\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S02/func/sub\PYGZhy{}S02\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S03/func/sub\PYGZhy{}S03\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S04/func/sub\PYGZhy{}S04\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S05/func/sub\PYGZhy{}S05\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S06/func/sub\PYGZhy{}S06\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S07/func/sub\PYGZhy{}S07\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S08/func/sub\PYGZhy{}S08\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S09/func/sub\PYGZhy{}S09\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S10/func/sub\PYGZhy{}S10\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S11/func/sub\PYGZhy{}S11\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S12/func/sub\PYGZhy{}S12\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S13/func/sub\PYGZhy{}S13\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S14/func/sub\PYGZhy{}S14\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S15/func/sub\PYGZhy{}S15\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S16/func/sub\PYGZhy{}S16\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S17/func/sub\PYGZhy{}S17\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S18/func/sub\PYGZhy{}S18\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S19/func/sub\PYGZhy{}S19\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S20/func/sub\PYGZhy{}S20\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S21/func/sub\PYGZhy{}S21\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S22/func/sub\PYGZhy{}S22\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S23/func/sub\PYGZhy{}S23\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S24/func/sub\PYGZhy{}S24\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S25/func/sub\PYGZhy{}S25\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S26/func/sub\PYGZhy{}S26\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S27/func/sub\PYGZhy{}S27\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S28/func/sub\PYGZhy{}S28\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S29/func/sub\PYGZhy{}S29\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S30/func/sub\PYGZhy{}S30\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S31/func/sub\PYGZhy{}S31\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S32/func/sub\PYGZhy{}S32\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S33/func/sub\PYGZhy{}S33\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S34/func/sub\PYGZhy{}S34\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S35/func/sub\PYGZhy{}S35\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S36/func/sub\PYGZhy{}S36\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S37/func/sub\PYGZhy{}S37\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S38/func/sub\PYGZhy{}S38\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S39/func/sub\PYGZhy{}S39\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S40/func/sub\PYGZhy{}S40\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S41/func/sub\PYGZhy{}S41\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S42/func/sub\PYGZhy{}S42\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S43/func/sub\PYGZhy{}S43\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S44/func/sub\PYGZhy{}S44\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S45/func/sub\PYGZhy{}S45\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S46/func/sub\PYGZhy{}S46\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S47/func/sub\PYGZhy{}S47\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S48/func/sub\PYGZhy{}S48\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S49/func/sub\PYGZhy{}S49\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S50/func/sub\PYGZhy{}S50\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S51/func/sub\PYGZhy{}S51\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S52/func/sub\PYGZhy{}S52\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S53/func/sub\PYGZhy{}S53\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S54/func/sub\PYGZhy{}S54\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S55/func/sub\PYGZhy{}S55\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S56/func/sub\PYGZhy{}S56\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S57/func/sub\PYGZhy{}S57\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S58/func/sub\PYGZhy{}S58\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S59/func/sub\PYGZhy{}S59\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S60/func/sub\PYGZhy{}S60\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S61/func/sub\PYGZhy{}S61\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S62/func/sub\PYGZhy{}S62\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S63/func/sub\PYGZhy{}S63\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S64/func/sub\PYGZhy{}S64\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S65/func/sub\PYGZhy{}S65\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S66/func/sub\PYGZhy{}S66\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S67/func/sub\PYGZhy{}S67\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S68/func/sub\PYGZhy{}S68\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S69/func/sub\PYGZhy{}S69\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S70/func/sub\PYGZhy{}S70\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S71/func/sub\PYGZhy{}S71\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S72/func/sub\PYGZhy{}S72\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S73/func/sub\PYGZhy{}S73\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S74/func/sub\PYGZhy{}S74\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S75/func/sub\PYGZhy{}S75\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S76/func/sub\PYGZhy{}S76\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S77/func/sub\PYGZhy{}S77\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S78/func/sub\PYGZhy{}S78\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S79/func/sub\PYGZhy{}S79\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S80/func/sub\PYGZhy{}S80\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S81/func/sub\PYGZhy{}S81\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S82/func/sub\PYGZhy{}S82\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S83/func/sub\PYGZhy{}S83\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S84/func/sub\PYGZhy{}S84\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S85/func/sub\PYGZhy{}S85\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S86/func/sub\PYGZhy{}S86\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S87/func/sub\PYGZhy{}S87\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S88/func/sub\PYGZhy{}S88\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S89/func/sub\PYGZhy{}S89\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S90/func/sub\PYGZhy{}S90\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S91/func/sub\PYGZhy{}S91\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S92/func/sub\PYGZhy{}S92\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S93/func/sub\PYGZhy{}S93\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/derivatives/fmriprep/sub\PYGZhy{}S94/func/sub\PYGZhy{}S94\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz\PYGZsq{}]
\end{sphinxVerbatim}

Now let’s download the first subject’s file using \sphinxcode{\sphinxupquote{ds.get()}}. This file is 825mb, so this might take a few minutes depending on your internet speed.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{result} \PYG{o}{=} \PYG{n}{ds}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{file\PYGZus{}list}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bold.nii.gz\PYGZsq{}, max=112128274.0, style=Progr…
\end{sphinxVerbatim}

How much of the dataset have we downloaded?  We can check the status of the annex using \sphinxcode{\sphinxupquote{ds.status(annex=\textquotesingle{}all\textquotesingle{})}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{result} \PYG{o}{=} \PYG{n}{ds}\PYG{o}{.}\PYG{n}{status}\PYG{p}{(}\PYG{n}{annex}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{all}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1794 annex\PYGZsq{}d files (106.9 MB/42.1 GB present/total size)
1794 annex\PYGZsq{}d files (106.9 MB/42.1 GB present/total size)
\end{sphinxVerbatim}

Now let’s download the preprocessed data for the first 15 subjects including the fmriprep reports.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{localizer\PYGZus{}path}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{f} \PYG{o+ow}{in} \PYG{n}{file\PYGZus{}list}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{30}\PYG{p}{]}\PYG{p}{:}
    \PYG{n}{result} \PYG{o}{=} \PYG{n}{ds}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. mage\PYGZus{}xfm.h5\PYGZsq{}, max=102374852.0, style=Progr…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}Total\PYGZsq{}, max=407384688.0, style=ProgressStyle(description\PYGZus{}…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. mask.nii.gz\PYGZsq{}, max=161755.0, style=Progress…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. \PYGZus{}T1w.nii.gz\PYGZsq{}, max=21641910.0, style=Progre…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. dseg.nii.gz\PYGZsq{}, max=737494.0, style=Progress…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. mage\PYGZus{}xfm.h5\PYGZsq{}, max=102374852.0, style=Progr…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. mage\PYGZus{}xfm.h5\PYGZsq{}, max=102374852.0, style=Progr…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=5297845.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=5659876.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=5435089.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. \PYGZus{}T1w.nii.gz\PYGZsq{}, max=24866183.0, style=Progre…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. dseg.nii.gz\PYGZsq{}, max=311348.0, style=Progress…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=7621704.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=7646881.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=7490904.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. dref.nii.gz\PYGZsq{}, max=3057434.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bold.nii.gz\PYGZsq{}, max=112592506.0, style=Progr…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}Total\PYGZsq{}, max=404027852.0, style=ProgressStyle(description\PYGZus{}…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. mask.nii.gz\PYGZsq{}, max=141605.0, style=Progress…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. \PYGZus{}T1w.nii.gz\PYGZsq{}, max=21441183.0, style=Progre…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. dseg.nii.gz\PYGZsq{}, max=574495.0, style=Progress…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. mage\PYGZus{}xfm.h5\PYGZsq{}, max=102374852.0, style=Progr…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. mage\PYGZus{}xfm.h5\PYGZsq{}, max=102374852.0, style=Progr…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=3834278.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=4131484.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=3942856.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. \PYGZus{}T1w.nii.gz\PYGZsq{}, max=27185969.0, style=Progre…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. dseg.nii.gz\PYGZsq{}, max=297333.0, style=Progress…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=7681869.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=7681727.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bseg.nii.gz\PYGZsq{}, max=7501005.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. dref.nii.gz\PYGZsq{}, max=3355704.0, style=Progres…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HBox(children=(FloatProgress(value=0.0, description=\PYGZsq{}derivatives .. bold.nii.gz\PYGZsq{}, max=111399969.0, style=Progr…
\end{sphinxVerbatim}

Ok, that concludes our tutorial for how to download data for this course with datalad using both the command line interface and also the Python API.


\section{Introduction to programming}
\label{\detokenize{content/Introduction_to_Programming:introduction-to-programming}}\label{\detokenize{content/Introduction_to_Programming::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

In this notebook we will begin to learn how to use Python.  There are many different ways to install Python, but we recommend starting using Anaconda which is preconfigured for scientific computing.  Start with installing \sphinxhref{https://www.anaconda.com/distribution/}{Python 3.7}.  For those who prefer a more configurable IDE, \sphinxhref{https://www.jetbrains.com/pycharm/}{Pycharm} is a nice option.  Python is a modular interpreted language with an intuitive minimal syntax that is quickly becoming one of the most popular languages for \sphinxhref{http://www.talyarkoni.org/blog/2013/11/18/the-homogenization-of-scientific-computing-or-why-python-is-steadily-eating-other-languages-lunch/}{conducting research}.  You can use python for \sphinxhref{http://www.psychopy.org/}{stimulus presentation}, \sphinxhref{http://statsmodels.sourceforge.net/}{data analysis}, \sphinxhref{http://scikit-learn.org/stable/}{machine\sphinxhyphen{}learning}, \sphinxhref{https://www.crummy.com/software/BeautifulSoup/}{scraping data}, creating websites with \sphinxhref{http://flask.pocoo.org/}{flask} or \sphinxhref{https://www.djangoproject.com/}{django}, or \sphinxhref{http://nipy.org/}{neuroimaging data analysis}.

There are lots of free useful resources to learn how to use python and various modules.  See \sphinxhref{https://github.com/ContextLab/cs-for-psych}{Jeremy Manning’s} or \sphinxhref{https://github.com/dartmouth-pbs/psyc161}{Yaroslav Halchenko’s} excellent Dartmouth courses.  \sphinxhref{https://www.codecademy.com/}{Codeacademy} is a great interactive tutorial.  \sphinxhref{http://stackoverflow.com/}{Stack Overflow} is an incredibly useful resource for asking specific questions and seeing responses to others that have been rated by the development community.

\sphinxincludegraphics{{programming_growth}.png}


\subsection{Jupyter Notebooks}
\label{\detokenize{content/Introduction_to_Programming:jupyter-notebooks}}
We will primarily be using \sphinxhref{http://jupyter.org/}{Jupyter Notebooks} to interface with Python.  A Jupyter notebook consists of \sphinxstylestrong{cells}. The two main types of cells you will use are code cells and markdown cells.

A \sphinxstylestrong{\sphinxstyleemphasis{code cell}} contains actual code that you want to run. You can specify a cell as a code cell using the pulldown menu in the toolbar in your Jupyter notebook. Otherwise, you can can hit esc and then y (denoted “esc, y”) while a cell is selected to specify that it is a code cell. Note that you will have to hit enter after doing this to start editing it.
If you want to execute the code in a code cell, hit “shift + enter.” Note that code cells are executed in the order you execute them. That is to say, the ordering of the cells for which you hit “shift + enter” is the order in which the code is executed. If you did not explicitly execute a cell early in the document, its results are now known to the Python interpreter.

\sphinxstylestrong{\sphinxstyleemphasis{Markdown cells}} contain text. The text is written in markdown, a lightweight markup language. You can read about its syntax \sphinxhref{http://daringfireball.net/projects/markdown/syntax}{here}. Note that you can also insert HTML into markdown cells, and this will be rendered properly. As you are typing the contents of these cells, the results appear as text. Hitting “shift + enter” renders the text in the formatting you specify.  You can specify a cell as being a markdown cell in the Jupyter toolbar, or by hitting “esc, m” in the cell. Again, you have to hit enter after using the quick keys to bring the cell into edit mode.

In general, when you want to add a new cell, you can use the “Insert” pulldown menu from the Jupyter toolbar. The shortcut to insert a cell below is “esc, b” and to insert a cell above is “esc, a.” Alternatively, you can execute a cell and automatically add a new one below it by hitting “alt + enter.”

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Hello World}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Package Management}
\label{\detokenize{content/Introduction_to_Programming:package-management}}
Package managment in Python has been dramatically improving.  Anaconda has it’s own package manager called ‘conda’.  Use this if you would like to install a new module as it is optimized to work with anaconda.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
!conda install *package*
\end{sphinxVerbatim}

However, sometimes conda doesn’t have a particular package.  In this case use the default python package manager called ‘pip’.

These commands can be run in your unix terminal or you can send them to the shell from a Jupyter notebook by starting the line with \sphinxcode{\sphinxupquote{!}}

It is easy to get help on how to use the package managers

\begin{sphinxVerbatim}[commandchars=\\\{\}]
!pip help install
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{!}pip \PYG{n+nb}{help} install
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{!}pip list \PYGZhy{}\PYGZhy{}outdated
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{!}pip install setuptools \PYGZhy{}\PYGZhy{}upgrade
\end{sphinxVerbatim}


\subsection{Variables}
\label{\detokenize{content/Introduction_to_Programming:variables}}
Python is a dynamically typed language, which means that you can easily change the datatype associated with a variable. There are several built\sphinxhyphen{}in datatypes that are good to be aware of.
\begin{itemize}
\item {} 
Built\sphinxhyphen{}in
\begin{itemize}
\item {} 
Numeric types:
\begin{itemize}
\item {} 
\sphinxstylestrong{int}, \sphinxstylestrong{float}, \sphinxstylestrong{long}, complex

\end{itemize}

\item {} 
String: \sphinxstylestrong{str}

\item {} 
Boolean: \sphinxstylestrong{bool}
\begin{itemize}
\item {} 
True / False

\end{itemize}

\item {} 
\sphinxstylestrong{NoneType}

\end{itemize}

\item {} 
User defined

\item {} 
Use the type() function to find the type for a value or variable

\item {} 
Data can be converted using cast commands

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Integer}
\PYG{n}{a} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Float}
\PYG{n}{b} \PYG{o}{=} \PYG{l+m+mf}{1.0}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{b}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} String}
\PYG{n}{c} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hello}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{c}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Boolean}
\PYG{n}{d} \PYG{o}{=} \PYG{k+kc}{True}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} None}
\PYG{n}{e} \PYG{o}{=} \PYG{k+kc}{None}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{e}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Cast integer to string}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}int\PYGZsq{}\PYGZgt{}
\PYGZlt{}class \PYGZsq{}float\PYGZsq{}\PYGZgt{}
\PYGZlt{}class \PYGZsq{}str\PYGZsq{}\PYGZgt{}
\PYGZlt{}class \PYGZsq{}bool\PYGZsq{}\PYGZgt{}
\PYGZlt{}class \PYGZsq{}NoneType\PYGZsq{}\PYGZgt{}
\PYGZlt{}class \PYGZsq{}str\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}


\subsection{Math Operators}
\label{\detokenize{content/Introduction_to_Programming:math-operators}}\begin{itemize}
\item {} 
+, \sphinxhyphen{}, *, and /

\item {} 
Exponentiation **

\item {} 
Modulo \%

\item {} 
Note that division with integers in Python 2.7 automatically rounds, which may not be intended.  It is recommended to import the division module from python3 \sphinxcode{\sphinxupquote{from \_\_future\_\_ import division}}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Addition}
\PYG{n}{a} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{+} \PYG{l+m+mi}{7}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Subtraction}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{a} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{5}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{b}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Multiplication}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{b}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Exponentiation}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{b}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Modulo}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{o}{\PYGZpc{}}\PYG{k}{9})

\PYG{c+c1}{\PYGZsh{} Division}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{o}{/}\PYG{l+m+mi}{9}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
9
4
8
16
4
0.4444444444444444
\end{sphinxVerbatim}


\subsection{String Operators}
\label{\detokenize{content/Introduction_to_Programming:string-operators}}\begin{itemize}
\item {} 
Some of the arithmetic operators also have meaning for strings. E.g. for string concatenation use \sphinxcode{\sphinxupquote{+}} sign

\item {} 
String repetition: Use \sphinxcode{\sphinxupquote{*}} sign with a number of repetitions

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Combine string}
\PYG{n}{a} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hello}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{b} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{World}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a} \PYG{o}{+} \PYG{n}{b}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Repeat String}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
HelloWorld
HelloHelloHelloHelloHello
\end{sphinxVerbatim}


\subsection{Logical Operators}
\label{\detokenize{content/Introduction_to_Programming:logical-operators}}
Perform logical comparison and return Boolean value

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{==} \PYG{n}{y} \PYG{c+c1}{\PYGZsh{} x is equal to y}
\PYG{n}{x} \PYG{o}{!=} \PYG{n}{y} \PYG{c+c1}{\PYGZsh{} x is not equal to y}
\PYG{n}{x} \PYG{o}{\PYGZgt{}} \PYG{n}{y} \PYG{c+c1}{\PYGZsh{} x is greater than y}
\PYG{n}{x} \PYG{o}{\PYGZlt{}} \PYG{n}{y} \PYG{c+c1}{\PYGZsh{} x is less than y}
\PYG{n}{x} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{y} \PYG{c+c1}{\PYGZsh{} x is greater than or equal to y }
\PYG{n}{x} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{y} \PYG{c+c1}{\PYGZsh{} x is less than or equal to y}
\end{sphinxVerbatim}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
X
&\sphinxstyletheadfamily 
not X
\\
\hline
True
&
False
\\
\hline
False
&
True
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
X
&\sphinxstyletheadfamily 
Y
&\sphinxstyletheadfamily 
X AND Y
&\sphinxstyletheadfamily 
X OR Y
\\
\hline
True
&
True
&
True
&
True
\\
\hline
True
&
False
&
False
&
True
\\
\hline
False
&
True
&
False
&
True
\\
\hline
False
&
False
&
False
&
False
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Works for string}
\PYG{n}{a} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hello}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{b} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{world}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{c} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hello}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{o}{==}\PYG{n}{b}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{o}{==}\PYG{n}{c}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{o}{!=}\PYG{n}{b}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Works for numeric}
\PYG{n}{d} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{e} \PYG{o}{=} \PYG{l+m+mi}{8}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{d} \PYG{o}{\PYGZlt{}} \PYG{n}{e}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
False
False
True
True
\end{sphinxVerbatim}


\subsection{Conditional Logic (if…)}
\label{\detokenize{content/Introduction_to_Programming:conditional-logic-if}}
Unlike most other languages, Python uses tab formatting rather than closing conditional statements (e.g., end).
\begin{itemize}
\item {} 
Syntax:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{n}{condition}\PYG{p}{:} 
    \PYG{n}{do} \PYG{n}{something}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
Implicit conversion of the value to bool() happens if \sphinxcode{\sphinxupquote{condition}} is of a different type than \sphinxstylestrong{bool}, thus all of the following should work:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{n}{condition}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}something}
\PYG{k}{elif} \PYG{n}{condition}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}alternative1}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}otherwise} \PYG{c+c1}{\PYGZsh{} often reserved to report an error}
                 \PYG{c+c1}{\PYGZsh{} after a long list of options}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{k}{if} \PYG{n}{n}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n is non\PYGZhy{}0}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{if} \PYG{n}{n} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n is None}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    
\PYG{k}{if} \PYG{n}{n} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n is not None}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
n is non\PYGZhy{}0
n is not None
\end{sphinxVerbatim}


\subsection{Loops}
\label{\detokenize{content/Introduction_to_Programming:loops}}\begin{itemize}
\item {} 
\sphinxstylestrong{for} loop is probably the most popular loop construct in Python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{target} \PYG{o+ow}{in} \PYG{n}{sequence}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}statements}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
However, it’s also possible to use a \sphinxstylestrong{while} loop to repeat statements while \sphinxcode{\sphinxupquote{condition}} remains True:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{while} \PYG{n}{condition} \PYG{n}{do}\PYG{p}{:}
    \PYG{n}{do\PYGZus{}statements}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{string} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Python is going to make conducting research easier}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{k}{for} \PYG{n}{c} \PYG{o+ow}{in} \PYG{n}{string}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{c}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
P
y
t
h
o
n
 
i
s
 
g
o
i
n
g
 
t
o
 
m
a
k
e
 
c
o
n
d
u
c
t
i
n
g
 
r
e
s
e
a
r
c
h
 
e
a
s
i
e
r
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{n}{end} \PYG{o}{=} \PYG{l+m+mi}{10}

\PYG{n}{csum} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{k}{while} \PYG{n}{x} \PYG{o}{\PYGZlt{}} \PYG{n}{end}\PYG{p}{:}
    \PYG{n}{csum} \PYG{o}{+}\PYG{o}{=} \PYG{n}{x}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{csum}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Exited with x==}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{x}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0 0
1 1
2 3
3 6
4 10
5 15
6 21
7 28
8 36
9 45
Exited with x==10
\end{sphinxVerbatim}


\subsection{Functions}
\label{\detokenize{content/Introduction_to_Programming:functions}}
A \sphinxstylestrong{function} is a named sequence of statements that performs a computation.  You define the function by giving it a name, specify a sequence of statements, and optionally values to return.  Later, you can “call” the function by name.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}upper\PYGZus{}case}\PYG{p}{(}\PYG{n}{text}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{text}\PYG{o}{.}\PYG{n}{upper}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
The expression in the parenthesis is the \sphinxstylestrong{argument}.

\item {} 
It is common to say that a function \sphinxstylestrong{“takes” an argument} and \sphinxstylestrong{“returns” a result}.

\item {} 
The result is called the \sphinxstylestrong{return value}.

\end{itemize}

The first line of the function definition is called the \sphinxstylestrong{header}; the rest is called the \sphinxstylestrong{body}.

The header has to end with a colon and the body has to be indented.
It is a common practice to use 4 spaces for indentation, and to avoid mixing with tabs.

Function body in Python ends whenever statement begins at the original level of indentation.  There is no \sphinxstylestrong{end} or \sphinxstylestrong{fed} or any other identify to signal the end of function.  Indentation is part of the the language syntax in Python, making it more readable and less cluttered.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}upper\PYGZus{}case}\PYG{p}{(}\PYG{n}{text}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{text}\PYG{o}{.}\PYG{n}{upper}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{string} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Python is going to make conducting research easier}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{make\PYGZus{}upper\PYGZus{}case}\PYG{p}{(}\PYG{n}{string}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
PYTHON IS GOING TO MAKE CONDUCTING RESEARCH EASIER
\end{sphinxVerbatim}


\subsection{Python Containers}
\label{\detokenize{content/Introduction_to_Programming:python-containers}}
There are 4 main types of builtin containers for storing data in Python:
\begin{itemize}
\item {} 
list

\item {} 
tuple

\item {} 
dict

\item {} 
set

\end{itemize}


\subsubsection{Lists}
\label{\detokenize{content/Introduction_to_Programming:lists}}
In Python, a list is a mutable sequence of values.  Mutable means that we can change separate entries within a list. For a more in depth tutorial on lists look \sphinxhref{http://nbviewer.jupyter.org/github/dartmouth-pbs/psyc161/blob/master/classes/02d-Python-Fundamentals-Containers-Lists.ipynb}{here}
\begin{itemize}
\item {} 
Each value in the list is an element or item

\item {} 
Elements can be any Python data type

\item {} 
Lists can mix data types

\item {} 
Lists are initialized with \sphinxcode{\sphinxupquote{{[}{]}}} or \sphinxcode{\sphinxupquote{list()}}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{l} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\end{itemize}

Elements within a list are indexed (\sphinxstylestrong{starting with 0})

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{l}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\end{itemize}

Elements can be nested lists

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nested} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\end{itemize}

Lists can be \sphinxstyleemphasis{sliced}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{l}\PYG{p}{[}\PYG{n}{start}\PYG{p}{:}\PYG{n}{stop}\PYG{p}{:}\PYG{n}{stride}\PYG{p}{]}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
Like all python containers, lists have many useful methods that can be applied

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{index}\PYG{p}{,}\PYG{n}{new} \PYG{n}{element}\PYG{p}{)}
\PYG{n}{a}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{element} \PYG{n}{to} \PYG{n}{add} \PYG{n}{at} \PYG{n}{end}\PYG{p}{)}
\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\end{itemize}

List comprehension is a \sphinxstyleemphasis{Very} powerful technique allowing for efficient construction of new lists.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{a} \PYG{k}{for} \PYG{n}{a} \PYG{o+ow}{in} \PYG{n}{l}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Indexing and Slicing}
\PYG{n}{a} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lists}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{are}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{arrays}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} List methods}
\PYG{n}{a}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{python}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{a}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} List Comprehension}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{o}{.}\PYG{n}{upper}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{a}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
lists
[\PYGZsq{}are\PYGZsq{}, \PYGZsq{}arrays\PYGZsq{}]
[\PYGZsq{}lists\PYGZsq{}, \PYGZsq{}are\PYGZsq{}, \PYGZsq{}python\PYGZsq{}, \PYGZsq{}arrays\PYGZsq{}, \PYGZsq{}.\PYGZsq{}]
5
[\PYGZsq{}LISTS\PYGZsq{}, \PYGZsq{}ARE\PYGZsq{}, \PYGZsq{}PYTHON\PYGZsq{}, \PYGZsq{}ARRAYS\PYGZsq{}, \PYGZsq{}.\PYGZsq{}]
\end{sphinxVerbatim}


\subsubsection{Dictionaries}
\label{\detokenize{content/Introduction_to_Programming:dictionaries}}\begin{itemize}
\item {} 
In Python, a dictionary (or \sphinxcode{\sphinxupquote{dict}}) is mapping between a set of
indices (\sphinxstylestrong{keys}) and a set of \sphinxstylestrong{values}

\item {} 
The items in a dictionary are key\sphinxhyphen{}value pairs

\item {} 
Keys can be any Python data type

\item {} 
Dictionaries are unordered

\item {} 
Here is a more indepth tutorial on \sphinxhref{http://nbviewer.jupyter.org/github/dartmouth-pbs/psyc161/blob/master/classes/03c-Python-Fundamentals-Containers-Dicts.ipynb}{dictionaries}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Dictionaries}
\PYG{n}{eng2sp} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{n}{eng2sp}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{one}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{uno}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{eng2sp}\PYG{p}{)}

\PYG{n}{eng2sp} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{one}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{uno}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{two}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dos}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{three}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tres}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{eng2sp}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{eng2sp}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{eng2sp}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}\PYGZsq{}one\PYGZsq{}: \PYGZsq{}uno\PYGZsq{}\PYGZcb{}
\PYGZob{}\PYGZsq{}one\PYGZsq{}: \PYGZsq{}uno\PYGZsq{}, \PYGZsq{}two\PYGZsq{}: \PYGZsq{}dos\PYGZsq{}, \PYGZsq{}three\PYGZsq{}: \PYGZsq{}tres\PYGZsq{}\PYGZcb{}
dict\PYGZus{}keys([\PYGZsq{}one\PYGZsq{}, \PYGZsq{}two\PYGZsq{}, \PYGZsq{}three\PYGZsq{}])
dict\PYGZus{}values([\PYGZsq{}uno\PYGZsq{}, \PYGZsq{}dos\PYGZsq{}, \PYGZsq{}tres\PYGZsq{}])
\end{sphinxVerbatim}


\subsubsection{Tuples}
\label{\detokenize{content/Introduction_to_Programming:tuples}}
In Python, a \sphinxstylestrong{tuple} is an immutable sequence of values, meaning they can’t be changed
\begin{itemize}
\item {} 
Each value in the tuple is an element or item

\item {} 
Elements can be any Python data type

\item {} 
Tuples can mix data types

\item {} 
Elements can be nested tuples

\item {} 
\sphinxstylestrong{Essentially tuples are immutable lists}

\end{itemize}

Here is a nice tutorial on \sphinxhref{http://nbviewer.jupyter.org/github/dartmouth-pbs/psyc161/blob/master/classes/03b-Python-Fundamentals-Containers-Tuples.ipynb}{tuples}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{numbers} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{numbers}\PYG{p}{)}

\PYG{n}{t2} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{t2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(1, 2, 3, 4)
(1, 2)
\end{sphinxVerbatim}


\subsection{sets}
\label{\detokenize{content/Introduction_to_Programming:sets}}
In Python, a \sphinxcode{\sphinxupquote{set}} is an efficient storage for “membership” checking
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{set}} is like a \sphinxcode{\sphinxupquote{dict}} but only with keys and without values

\item {} 
a \sphinxcode{\sphinxupquote{set}} can also perform set operations (e.g., union intersection)

\item {} 
Here is more info on \sphinxhref{http://nbviewer.jupyter.org/github/dartmouth-pbs/psyc161/blob/master/classes/03d-Python-Fundamentals-Containers-Sets.ipynb}{sets}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Union}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dad}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}} \PYG{o}{|} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Intersection}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dad}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}} \PYG{o}{\PYGZam{}} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Difference}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dad}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}} \PYG{o}{\PYGZhy{}} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}1, 2, 3, \PYGZsq{}mom\PYGZsq{}, 10, \PYGZsq{}dad\PYGZsq{}\PYGZcb{}
\PYGZob{}2, 3\PYGZcb{}
\PYGZob{}1, \PYGZsq{}mom\PYGZsq{}, \PYGZsq{}dad\PYGZsq{}\PYGZcb{}
\end{sphinxVerbatim}


\subsection{Modules}
\label{\detokenize{content/Introduction_to_Programming:modules}}
A \sphinxstyleemphasis{Module} is a python file that contains a collection of related definitions. Python has \sphinxstyleemphasis{hundreds} of standard modules.  These are organized into what is known as the \sphinxhref{http://docs.python.org/library/}{Python Standard Library}.  You can also create and use your own modules.  To use functionality from a module, you first have to import the entire module or parts of it into your namespace
\begin{itemize}
\item {} 
To import the entire module, use

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{module\PYGZus{}name}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
You can also import a module using a specific name

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{module\PYGZus{}name} \PYG{k}{as} \PYG{n+nn}{new\PYGZus{}module\PYGZus{}name}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
To import specific definitions (e.g. functions, variables, etc) from the module into your local namespace, use

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{module\PYGZus{}name} \PYG{k+kn}{import} \PYG{n}{name1}\PYG{p}{,} \PYG{n}{name2}
\end{sphinxVerbatim}

which will make those available directly in your \sphinxcode{\sphinxupquote{namespace}}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{from} \PYG{n+nn}{glob} \PYG{k+kn}{import} \PYG{n}{glob}
\end{sphinxVerbatim}

Here let’s try and get the path of the current working directory using functions from the \sphinxcode{\sphinxupquote{os}} module

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{abspath}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{curdir}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Notebooks\PYGZsq{}
\end{sphinxVerbatim}

It looks like we are currently in the notebooks folder of the github repository.  Let’s use glob, a pattern matching function, to list all of the csv files in the Data folder.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data\PYGZus{}file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../..}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{data\PYGZus{}file\PYGZus{}list}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[]
\end{sphinxVerbatim}

This gives us a list of the files including the relative path from the current directory.  What if we wanted just the filenames?  There are several different ways to do this.  First, we can use the the \sphinxcode{\sphinxupquote{os.path.basename}} function.  We loop over every file, grab the base file name and then append it to a new list.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{file\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{f} \PYG{o+ow}{in} \PYG{n}{data\PYGZus{}file\PYGZus{}list}\PYG{p}{:}
    \PYG{n}{file\PYGZus{}list}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{basename}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{file\PYGZus{}list}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}salary\PYGZus{}exercise.csv\PYGZsq{}, \PYGZsq{}salary.csv\PYGZsq{}]
\end{sphinxVerbatim}

Alternatively, we could loop over all files and split on the \sphinxcode{\sphinxupquote{/}} character.  This will create a new list where each element is whatever characters are separated by the splitting character.  We can then take the last element of each list.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{file\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{f} \PYG{o+ow}{in} \PYG{n}{data\PYGZus{}file\PYGZus{}list}\PYG{p}{:}
    \PYG{n}{file\PYGZus{}list}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{f}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{file\PYGZus{}list}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}salary\PYGZus{}exercise.csv\PYGZsq{}, \PYGZsq{}salary.csv\PYGZsq{}]
\end{sphinxVerbatim}

It is also sometimes even cleaner to do this as a list comprehension

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{basename}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{data\PYGZus{}file\PYGZus{}list}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}salary\PYGZus{}exercise.csv\PYGZsq{}, \PYGZsq{}salary.csv\PYGZsq{}]
\end{sphinxVerbatim}


\subsection{Exercises}
\label{\detokenize{content/Introduction_to_Programming:exercises}}

\subsubsection{Find Even Numbers}
\label{\detokenize{content/Introduction_to_Programming:find-even-numbers}}
Let’s say I give you a list saved in a variable: a = {[}1, 4, 9, 16, 25, 36, 49, 64, 81, 100{]}. Make a new list that has only the even elements of this list in it.


\subsubsection{Find Maximal Range}
\label{\detokenize{content/Introduction_to_Programming:find-maximal-range}}
Given an array length 1 or more of ints, return the difference between the largest and smallest values in the array.


\subsubsection{Duplicated Numbers}
\label{\detokenize{content/Introduction_to_Programming:duplicated-numbers}}
Find the numbers in list a that are also in list b

a = {[}0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361{]}

b = {[}0, 4, 16, 36, 64, 100, 144, 196, 256, 324{]}


\subsubsection{Speeding Ticket Fine}
\label{\detokenize{content/Introduction_to_Programming:speeding-ticket-fine}}
You are driving a little too fast on the highway, and a police officer stops you. Write a function that takes the speed as an input and returns the fine.

If speed is 60 or less, the result is \sphinxcode{\sphinxupquote{\$0}}. If speed is between 61 and 80 inclusive, the result is \sphinxcode{\sphinxupquote{\$100}}. If speed is 81 or more, the result is \sphinxcode{\sphinxupquote{\$500}}.


\section{Introduction to Pandas}
\label{\detokenize{content/Introduction_to_Pandas:introduction-to-pandas}}\label{\detokenize{content/Introduction_to_Pandas::doc}}
\sphinxstyleemphasis{Written by Luke Chang \& Jin Cheong}

Analyzing data requires being facile with manipulating and transforming datasets to be able to test specific hypotheses. Data come in all different types of flavors and there are many different tools in the Python ecosystem to work with pretty much any type of data you might encounter. For example, you might be interested in working with functional neuroimaging data that is four dimensional. Three dimensional matrices contain brain activations in space, and these data can change over time in the 4th dimension. This type of data is well suited for \sphinxhref{https://numpy.org/}{numpy} and specialized brain imaging packages such as \sphinxhref{https://nilearn.github.io/}{nilearn}. The majority of data, however, is typically in some version of a two\sphinxhyphen{}dimensional observations by features format as might be seen in an excel spreadsheet, a SQL table, or in a comma delimited format (i.e., csv).

In Python, the \sphinxhref{https://pandas.pydata.org/}{Pandas} library is a powerful tool to work with this type of data. This is a very large library with a tremendous amount of functionality. In this tutorial, we will cover the basics of how to load and manipulate data and will focus on common to data munging tasks.

For those interested in diving deeper into Pandas, there are many online resources. There is the \sphinxhref{https://pandas.pydata.org/}{Pandas online documention}, \sphinxhref{https://stackoverflow.com/questions/tagged/pandas}{stackoverflow}, and \sphinxhref{https://medium.com/search?q=pandas}{medium blogposts}. I highly recommend  Jake Vanderplas’s terrific \sphinxhref{https://jakevdp.github.io/PythonDataScienceHandbook/}{Python Data Science Handbook}.   In addition, here is a brief \sphinxhref{https://neurohackademy.org/course/complex-data-structures/}{video} by Tal Yarkoni providing a useful introduction to pandas.

After the tutorial you will have the chance to apply the methods to a new set of data.


\subsection{Pandas Objects}
\label{\detokenize{content/Introduction_to_Pandas:pandas-objects}}
Pandas has several objects that are commonly used (i.e., Series, DataFrame, Index). At it’s core, Pandas Objects are enhanced numpy arrays where columns and rows can have special names and there are lots of methods to operate on the data. See Jake Vanderplas’s \sphinxhref{https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.01-Introducing-Pandas-Objects.ipynb}{tutorial} for a more in depth overview.


\subsubsection{Series}
\label{\detokenize{content/Introduction_to_Pandas:series}}
A pandas \sphinxcode{\sphinxupquote{Series}} is a one\sphinxhyphen{}dimensional array of indexed data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0    1
1    2
2    3
3    4
4    5
dtype: int64
\end{sphinxVerbatim}

The indices can be integers like in the example above. Alternatively, the indices can be labels.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
a    1
b    2
c    3
dtype: int64
\end{sphinxVerbatim}

Also, \sphinxcode{\sphinxupquote{Series}} can be easily created from dictionaries

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{data}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
A    5
B    3
C    1
dtype: int64
\end{sphinxVerbatim}


\subsubsection{DataFrame}
\label{\detokenize{content/Introduction_to_Pandas:dataframe}}
If a \sphinxcode{\sphinxupquote{Series}} is a one\sphinxhyphen{}dimensional indexed array, the \sphinxcode{\sphinxupquote{DataFrame}} is a two\sphinxhyphen{}dimensional indexed array. It can be thought of as a collection of Series objects, where each Series represents a column, or as an enhanced 2D numpy array.

In a \sphinxcode{\sphinxupquote{DataFrame}}, the index refers to labels for each row, while columns describe each column.

First, let’s create a \sphinxcode{\sphinxupquote{DataFrame}} using random numbers generated from numpy.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{data}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          0         1         2
0  0.126463  0.901287  0.487618
1  0.033185  0.287304  0.773930
2  0.069269  0.793656  0.025462
3  0.228061  0.559880  0.473307
4  0.261735  0.353904  0.880351
\end{sphinxVerbatim}

We could also initialize with column names

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          A         B         C
0  0.720483  0.350045  0.538831
1  0.877044  0.308761  0.786224
2  0.399717  0.683241  0.878268
3  0.565219  0.719122  0.410798
4  0.165874  0.667780  0.820636
\end{sphinxVerbatim}

Alternatively, we could create a \sphinxcode{\sphinxupquote{DataFrame}} from multiple \sphinxcode{\sphinxupquote{Series}} objects.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Numbers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{a}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Letters}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{b}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{data}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Numbers Letters
0        1       a
1        2       b
2        3       c
3        4       d
\end{sphinxVerbatim}

Or a python dictionary

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{California}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Colorado}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{New Hampshire}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} 
                     \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Capital}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sacremento}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Denver}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Concord}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{data}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           State     Capital
0     California  Sacremento
1       Colorado      Denver
2  New Hampshire     Concord
\end{sphinxVerbatim}


\subsection{Loading Data}
\label{\detokenize{content/Introduction_to_Pandas:loading-data}}
Loading data is fairly straightfoward in Pandas. Type \sphinxcode{\sphinxupquote{pd.read}} then press tab to see a list of functions that can load specific file formats such as: csv, excel, spss, and sql.

In this example, we will use \sphinxcode{\sphinxupquote{pd.read\_csv}} to load a .csv file into a dataframe.
Note that read\_csv() has many options that can be used to make sure you load the data correctly. You can explore the docstrings for a function to get more information about the inputs and general useage guidelines by running \sphinxcode{\sphinxupquote{pd.read\_csv?}}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pd.read\PYGZus{}csv\PYG{o}{?}
\end{sphinxVerbatim}

To load a csv file we will need to specify either the relative or absolute path to the file.

The command \sphinxcode{\sphinxupquote{pwd}} will print the path of the current working directory.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pwd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Notebooks\PYGZsq{}
\end{sphinxVerbatim}

We will now load the Pandas has many ways to read data different data formats into a dataframe.  Here we will use the \sphinxcode{\sphinxupquote{pd.read\_csv}} function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/Users/lukechang/Dropbox/Dartbrains/Data/salary/salary.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sep} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} df = pd.read\PYGZus{}csv(\PYGZsq{}psych60/data/salary/salary.csv\PYGZsq{}, sep = \PYGZsq{},\PYGZsq{})}
\end{sphinxVerbatim}


\subsubsection{Ways to check the dataframe}
\label{\detokenize{content/Introduction_to_Pandas:ways-to-check-the-dataframe}}
There are many ways to examine your dataframe. One easy way is to just call the dataframe variable itself.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    salary  gender departm  years   age  publications
0    86285       0     bio   26.0  64.0            72
1    77125       0     bio   28.0  58.0            43
2    71922       0     bio   10.0  38.0            23
3    70499       0     bio   16.0  46.0            64
4    66624       0     bio   11.0  41.0            23
..     ...     ...     ...    ...   ...           ...
72   53662       1   neuro    1.0  31.0             3
73   57185       1    stat    9.0  39.0             7
74   52254       1    stat    2.0  32.0             9
75   61885       1    math   23.0  60.0             9
76   49542       1    math    3.0  33.0             5

[77 rows x 6 columns]
\end{sphinxVerbatim}

However, often the dataframes can be large and we may be only interested in seeing the first few rows.  \sphinxcode{\sphinxupquote{df.head()}} is useful for this purpose.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   salary  gender departm  years   age  publications
0   86285       0     bio   26.0  64.0            72
1   77125       0     bio   28.0  58.0            43
2   71922       0     bio   10.0  38.0            23
3   70499       0     bio   16.0  46.0            64
4   66624       0     bio   11.0  41.0            23
\end{sphinxVerbatim}

On the top row, you have column names, that can be called like a dictionary (a dataframe can be essentially thought of as a dictionary with column names as the keys). The left most column (0,1,2,3,4…) is called the index of the dataframe. The default index is sequential integers, but it can be set to anything as long as each row is unique (e.g., subject IDs)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Indexes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Columns}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Columns are like keys of a dictionary}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Indexes
RangeIndex(start=0, stop=77, step=1)
Columns
Index([\PYGZsq{}salary\PYGZsq{}, \PYGZsq{}gender\PYGZsq{}, \PYGZsq{}departm\PYGZsq{}, \PYGZsq{}years\PYGZsq{}, \PYGZsq{}age\PYGZsq{}, \PYGZsq{}publications\PYGZsq{}], dtype=\PYGZsq{}object\PYGZsq{})
Columns are like keys of a dictionary
Index([\PYGZsq{}salary\PYGZsq{}, \PYGZsq{}gender\PYGZsq{}, \PYGZsq{}departm\PYGZsq{}, \PYGZsq{}years\PYGZsq{}, \PYGZsq{}age\PYGZsq{}, \PYGZsq{}publications\PYGZsq{}], dtype=\PYGZsq{}object\PYGZsq{})
\end{sphinxVerbatim}

You can access the values of a column by calling it directly. Single bracket returns a \sphinxcode{\sphinxupquote{Series}} and double bracket returns a \sphinxcode{\sphinxupquote{dataframe}}.

Let’s return the first 10 rows of salary.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0    86285
1    77125
2    71922
3    70499
4    66624
5    64451
6    64366
7    59344
8    58560
9    58294
Name: salary, dtype: int64
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{shape}} is another useful method for getting the dimensions of the matrix.

We will print the number of rows and columns in this data set using fstring formatting. First, you need to specify a string starting with ‘f’, like this \sphinxcode{\sphinxupquote{f\textquotesingle{}anything\textquotesingle{}}}. It is easy to insert variables with curly brackets like this \sphinxcode{\sphinxupquote{f\textquotesingle{}rows: \{rows\}\textquotesingle{}}}.

\sphinxhref{https://realpython.com/python-f-strings/}{Here} is more info about formatting text.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rows}\PYG{p}{,} \PYG{n}{cols} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{shape}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{There are }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{rows}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ rows and }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{cols}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ columns in this data set}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} 
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
There are 77 rows and 6 columns in this data set
\end{sphinxVerbatim}


\subsubsection{Describing the data}
\label{\detokenize{content/Introduction_to_Pandas:describing-the-data}}
We can use the \sphinxcode{\sphinxupquote{.describe()}} method to get a quick summary of the continuous values of the data frame. We will \sphinxcode{\sphinxupquote{.transpose()}} the output to make it slightly easier to read.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
              count          mean           std      min      25\PYGZpc{}      50\PYGZpc{}  \PYGZbs{}
salary         77.0  67748.519481  15100.581435  44687.0  57185.0  62607.0   
gender         77.0      0.142857      0.387783      0.0      0.0      0.0   
years          76.0     14.973684      8.617770      1.0      8.0     14.0   
age            76.0     45.486842      9.005914     31.0     38.0     44.0   
publications   77.0     21.831169     15.240530      3.0      9.0     19.0   

                  75\PYGZpc{}       max  
salary        75382.0  112800.0  
gender            0.0       2.0  
years            23.0      34.0  
age              53.0      65.0  
publications     33.0      72.0  
\end{sphinxVerbatim}

We can also get quick summary of a pandas series, or specific column of a pandas dataframe.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{departm}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
count      77
unique      7
top       bio
freq       16
Name: departm, dtype: object
\end{sphinxVerbatim}

Sometimes, you will want to know how many data points are associated with a specific variable for categorical data. The \sphinxcode{\sphinxupquote{value\_counts}} method can be used for this goal.

For example, how many males and females are in this dataset?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0    67
1     9
2     1
Name: gender, dtype: int64
\end{sphinxVerbatim}

You can see that there are more than 2 genders specified in our data.

This is likely an error in the data collection process. It’s always up to the data analyst to decide what to do in these cases. Because we don’t know what the true value should have been, let’s just remove the row from the dataframe by finding all rows that are not ‘2’.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{!=}\PYG{l+m+mi}{2}\PYG{p}{]}

\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0    67
1     9
Name: gender, dtype: int64
\end{sphinxVerbatim}


\subsubsection{Dealing with missing values}
\label{\detokenize{content/Introduction_to_Pandas:dealing-with-missing-values}}
Data are always messy and often have lots of missing values. There are many different ways, in which missing data might present \sphinxcode{\sphinxupquote{NaN}}, \sphinxcode{\sphinxupquote{None}}, or \sphinxcode{\sphinxupquote{NA}}, Sometimes researchers code missing values with specific numeric codes such as 999999. It is important to find these as they can screw up your analyses if they are hiding in your data.

If the missing values are using a standard pandas or numpy value such as \sphinxcode{\sphinxupquote{NaN}}, \sphinxcode{\sphinxupquote{None}}, or \sphinxcode{\sphinxupquote{NA}}, we can identify where the missing values are as booleans using the \sphinxcode{\sphinxupquote{isnull()}} method.

The \sphinxcode{\sphinxupquote{isnull()}} method will return a dataframe with True/False values on whether a datapoint is null or not a number (nan).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{isnull}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    salary  gender  departm  years    age  publications
0    False   False    False  False  False         False
1    False   False    False  False  False         False
2    False   False    False  False  False         False
3    False   False    False  False  False         False
4    False   False    False  False  False         False
..     ...     ...      ...    ...    ...           ...
72   False   False    False  False  False         False
73   False   False    False  False  False         False
74   False   False    False  False  False         False
75   False   False    False  False  False         False
76   False   False    False  False  False         False

[76 rows x 6 columns]
\end{sphinxVerbatim}

Suppose we wanted to count the number of missing values for each column in the dataset.

One thing that is  nice about Python is that you can chain commands, which means that the output of one method can be the input into the next method.  This allows us to write intuitive and concise code.  Notice how we take the \sphinxcode{\sphinxupquote{sum()}} of all of the null cases.
We can chain the \sphinxcode{\sphinxupquote{.null()}} and \sphinxcode{\sphinxupquote{.sum()}} methods to see how many null values are added up in each column.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{isnull}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
salary          0
gender          0
departm         0
years           1
age             1
publications    0
dtype: int64
\end{sphinxVerbatim}

You can use the boolean indexing once again to see the datapoints that have missing values. We chained the method \sphinxcode{\sphinxupquote{.any()}} which will check if there are any True values for a given axis.  Axis=0 indicates rows, while Axis=1 indicates columns.  So here we are creating a boolean index for row where \sphinxstyleemphasis{any} column has a missing value.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{isnull}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    salary  gender departm  years   age  publications
18   64762       0    chem   25.0   NaN            29
24  104828       0    geol    NaN  50.0            44
\end{sphinxVerbatim}

You may look at where the values are not null. Note that indexes 18, and 24 are missing.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{df}\PYG{o}{.}\PYG{n}{isnull}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    salary  gender departm  years   age  publications
0    86285       0     bio   26.0  64.0            72
1    77125       0     bio   28.0  58.0            43
2    71922       0     bio   10.0  38.0            23
3    70499       0     bio   16.0  46.0            64
4    66624       0     bio   11.0  41.0            23
..     ...     ...     ...    ...   ...           ...
72   53662       1   neuro    1.0  31.0             3
73   57185       1    stat    9.0  39.0             7
74   52254       1    stat    2.0  32.0             9
75   61885       1    math   23.0  60.0             9
76   49542       1    math    3.0  33.0             5

[74 rows x 6 columns]
\end{sphinxVerbatim}

There are different techniques for dealing with missing data.  An easy one is to simply remove rows that have any missing values using the \sphinxcode{\sphinxupquote{dropna()}} method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/ipykernel\PYGZus{}launcher.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas\PYGZhy{}docs/stable/user\PYGZus{}guide/indexing.html\PYGZsh{}returning\PYGZhy{}a\PYGZhy{}view\PYGZhy{}versus\PYGZhy{}a\PYGZhy{}copy
  \PYGZdq{}\PYGZdq{}\PYGZdq{}Entry point for launching an IPython kernel.
\end{sphinxVerbatim}

Now we can check to make sure the missing rows are removed.  Let’s also check the new dimensions of the dataframe.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rows}\PYG{p}{,} \PYG{n}{cols} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{shape}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{There are }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{rows}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ rows and }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{cols}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ columns in this data set}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} 

\PYG{n}{df}\PYG{o}{.}\PYG{n}{isnull}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
There are 74 rows and 6 columns in this data set
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
salary          0
gender          0
departm         0
years           0
age             0
publications    0
dtype: int64
\end{sphinxVerbatim}


\subsubsection{Create New Columns}
\label{\detokenize{content/Introduction_to_Pandas:create-new-columns}}
You can create new columns to fit your needs.
For instance you can set initialize a new column with zeros.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pubperyear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{0}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/ipykernel\PYGZus{}launcher.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row\PYGZus{}indexer,col\PYGZus{}indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas\PYGZhy{}docs/stable/user\PYGZus{}guide/indexing.html\PYGZsh{}returning\PYGZhy{}a\PYGZhy{}view\PYGZhy{}versus\PYGZhy{}a\PYGZhy{}copy
  \PYGZdq{}\PYGZdq{}\PYGZdq{}Entry point for launching an IPython kernel.
\end{sphinxVerbatim}

Here we can create a new column pubperyear, which is the ratio of the number of papers published per year

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pubperyear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{publications}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{/}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{years}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/ipykernel\PYGZus{}launcher.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row\PYGZus{}indexer,col\PYGZus{}indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas\PYGZhy{}docs/stable/user\PYGZus{}guide/indexing.html\PYGZsh{}returning\PYGZhy{}a\PYGZhy{}view\PYGZhy{}versus\PYGZhy{}a\PYGZhy{}copy
  \PYGZdq{}\PYGZdq{}\PYGZdq{}Entry point for launching an IPython kernel.
\end{sphinxVerbatim}


\subsection{Indexing and slicing Data}
\label{\detokenize{content/Introduction_to_Pandas:indexing-and-slicing-data}}
Indexing in Pandas can be tricky. There are many ways to index in pandas, for this tutorial we will focus on four: loc, iloc, boolean, and indexing numpy values. For a more in depth overview see Jake Vanderplas’s tutorial{]}(https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.02\sphinxhyphen{}Data\sphinxhyphen{}Indexing\sphinxhyphen{}and\sphinxhyphen{}Selection.ipynb), where he also covers more advanced topics, such as \sphinxhref{https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.05-Hierarchical-Indexing.ipynb}{hierarchical indexing}.


\subsubsection{Indexing with Keys}
\label{\detokenize{content/Introduction_to_Pandas:indexing-with-keys}}
First, we will cover indexing with keys using the \sphinxcode{\sphinxupquote{.loc}} method. This method references the explicit index with a key name. It works for both index names and also column names. Note that often the keys for rows are integers by default.

In this example, we will return rows 10\sphinxhyphen{}20 on the salary column.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
10    56092
11    54452
12    54269
13    55125
14    97630
15    82444
16    76291
17    75382
19    62607
20    60373
Name: salary, dtype: int64
\end{sphinxVerbatim}

You can return multiple columns using a list.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{publications}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    salary  publications
0    86285            72
1    77125            43
2    71922            23
3    70499            64
4    66624            23
5    64451            44
6    64366            22
7    59344            11
8    58560             8
9    58294            12
10   56092             4
\end{sphinxVerbatim}


\subsubsection{Indexing with Integers}
\label{\detokenize{content/Introduction_to_Pandas:indexing-with-integers}}
Next we wil try \sphinxcode{\sphinxupquote{.iloc}}.  This method references the implicit python index using integer indexing (starting from 0, exclusive of last number).  You can think of this like row by column indexing using integers.

For example, let’s grab the first 3 rows and columns.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   salary  gender departm
0   86285       0     bio
1   77125       0     bio
2   71922       0     bio
\end{sphinxVerbatim}

Let’s make a new data frame with just Males and another for just Females. Notice, how we added the \sphinxcode{\sphinxupquote{.reset\_index(drop=True)}} method?   This is because assigning a new dataframe based on indexing another dataframe will retain the \sphinxstyleemphasis{original} index.  We need to explicitly tell pandas to reset the index if we want it to start from zero.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{male\PYGZus{}df} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{gender} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{n}{drop}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{female\PYGZus{}df} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{gender} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{n}{drop}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Indexing with booleans}
\label{\detokenize{content/Introduction_to_Pandas:indexing-with-booleans}}
Boolean or logical indexing is useful if you need to sort the data based on some True or False value.

For instance, who are the people with salaries greater than 90K but lower than 100K ?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[} \PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{salary} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{90000}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{salary} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{100000}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    salary  gender  departm  years   age  publications  pubperyear
14   97630       0     chem   34.0  64.0            43    1.264706
30   92951       0    neuro   11.0  41.0            20    1.818182
54   96936       0  physics   15.0  50.0            17    1.133333
\end{sphinxVerbatim}

This also works with the \sphinxcode{\sphinxupquote{.loc}} method, which is what you need to do if you want to return specific columns

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[} \PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{salary} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{90000}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{salary} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{100000}\PYG{p}{)}\PYG{p}{,} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    salary  gender
14   97630       0
30   92951       0
54   96936       0
\end{sphinxVerbatim}


\subsubsection{Numpy indexing}
\label{\detokenize{content/Introduction_to_Pandas:numpy-indexing}}
Finally, you can also return a numpy matrix from a pandas data frame by accessing the \sphinxcode{\sphinxupquote{.values}} property. This returns a numpy array that can be indexed using numpy integer indexing and slicing.

As an example, let’s grab the last 10 rows and the first 3 columns.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{values}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[53638, 0, \PYGZsq{}math\PYGZsq{}],
       [59139, 1, \PYGZsq{}bio\PYGZsq{}],
       [52968, 1, \PYGZsq{}bio\PYGZsq{}],
       [55949, 1, \PYGZsq{}chem\PYGZsq{}],
       [58893, 1, \PYGZsq{}neuro\PYGZsq{}],
       [53662, 1, \PYGZsq{}neuro\PYGZsq{}],
       [57185, 1, \PYGZsq{}stat\PYGZsq{}],
       [52254, 1, \PYGZsq{}stat\PYGZsq{}],
       [61885, 1, \PYGZsq{}math\PYGZsq{}],
       [49542, 1, \PYGZsq{}math\PYGZsq{}]], dtype=object)
\end{sphinxVerbatim}


\subsection{Renaming}
\label{\detokenize{content/Introduction_to_Pandas:renaming}}
Part of cleaning up the data is renaming with more sensible names. This is easy to do with Pandas.


\subsubsection{Renaming Columns}
\label{\detokenize{content/Introduction_to_Pandas:renaming-columns}}
We can rename columns with the \sphinxcode{\sphinxupquote{.rename}} method by passing in a dictionary using the \sphinxcode{\sphinxupquote{\{\textquotesingle{}Old Name\textquotesingle{}:\textquotesingle{}New Name\textquotesingle{}\}}}. We either need to assigne the result to a new variable or add \sphinxcode{\sphinxupquote{inplace=True}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{rename}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{departm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{department}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pubperyear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pub\PYGZus{}per\PYGZus{}year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/pandas/core/frame.py:4133: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas\PYGZhy{}docs/stable/user\PYGZus{}guide/indexing.html\PYGZsh{}returning\PYGZhy{}a\PYGZhy{}view\PYGZhy{}versus\PYGZhy{}a\PYGZhy{}copy
  errors=errors,
\end{sphinxVerbatim}


\subsubsection{Renaming Rows}
\label{\detokenize{content/Introduction_to_Pandas:renaming-rows}}
Often we may want to change the coding scheme for a variable. For example, it is hard to remember what zeros and ones mean in the gender variable. We can make this easier by changing these with a dictionary \sphinxcode{\sphinxupquote{\{0:\textquotesingle{}male\textquotesingle{}, 1:\textquotesingle{}female\textquotesingle{}\}}} with the \sphinxcode{\sphinxupquote{replace}} method. We can do this \sphinxcode{\sphinxupquote{inplace=True}} or we can assign it to a new variable. As an example, we will assign this to a new variable to also retain the original lablels.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{male}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{female}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/ipykernel\PYGZus{}launcher.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row\PYGZus{}indexer,col\PYGZus{}indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas\PYGZhy{}docs/stable/user\PYGZus{}guide/indexing.html\PYGZsh{}returning\PYGZhy{}a\PYGZhy{}view\PYGZhy{}versus\PYGZhy{}a\PYGZhy{}copy
  \PYGZdq{}\PYGZdq{}\PYGZdq{}Entry point for launching an IPython kernel.
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   salary  gender department  years   age  publications  pub\PYGZus{}per\PYGZus{}year  \PYGZbs{}
0   86285       0        bio   26.0  64.0            72      2.769231   
1   77125       0        bio   28.0  58.0            43      1.535714   
2   71922       0        bio   10.0  38.0            23      2.300000   
3   70499       0        bio   16.0  46.0            64      4.000000   
4   66624       0        bio   11.0  41.0            23      2.090909   

  gender\PYGZus{}name  
0        male  
1        male  
2        male  
3        male  
4        male  
\end{sphinxVerbatim}


\subsection{Operations}
\label{\detokenize{content/Introduction_to_Pandas:operations}}
One of the really fun things about pandas once you get the hang of it is how easy it is to perform operations on the data. It is trivial to compute simple summaries of the data. We can also leverage the object\sphinxhyphen{}oriented nature of a pandas object, we can chain together multiple commands.

For example, let’s grab the mean of a few columns.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{years}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{publications}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
years           14.972973
age             45.567568
publications    21.662162
dtype: float64
\end{sphinxVerbatim}

We can also turn these values into a plot with the \sphinxcode{\sphinxupquote{plot}} method, which we will cover in more detail in future tutorials.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{years}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{publications}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bar}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7fea789a8810\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Pandas_79_1}.png}

Perhaps we want to see if there are any correlations in our dataset. We can do this with the \sphinxcode{\sphinxupquote{.corr}} method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                salary    gender     years       age  publications  \PYGZbs{}
salary        1.000000 \PYGZhy{}0.300071  0.303275  0.275534      0.427426   
gender       \PYGZhy{}0.300071  1.000000 \PYGZhy{}0.275468 \PYGZhy{}0.277098     \PYGZhy{}0.249410   
years         0.303275 \PYGZhy{}0.275468  1.000000  0.958181      0.323965   
age           0.275534 \PYGZhy{}0.277098  0.958181  1.000000      0.328285   
publications  0.427426 \PYGZhy{}0.249410  0.323965  0.328285      1.000000   
pub\PYGZus{}per\PYGZus{}year \PYGZhy{}0.016988  0.024210 \PYGZhy{}0.541125 \PYGZhy{}0.498825      0.399865   

              pub\PYGZus{}per\PYGZus{}year  
salary           \PYGZhy{}0.016988  
gender            0.024210  
years            \PYGZhy{}0.541125  
age              \PYGZhy{}0.498825  
publications      0.399865  
pub\PYGZus{}per\PYGZus{}year      1.000000  
\end{sphinxVerbatim}


\subsection{Merging Data}
\label{\detokenize{content/Introduction_to_Pandas:merging-data}}
Another common data manipulation goal is to merge datasets. There are multiple ways to do this in pandas, we will cover concatenation, append, and merge.


\subsubsection{Concatenation}
\label{\detokenize{content/Introduction_to_Pandas:concatenation}}
Concatenation describes the process of \sphinxstyleemphasis{stacking} dataframes together. The main thing to consider is to make sure that the shapes of the two dataframes are the same as well as the index labels. For example, if we wanted to vertically stack two dataframe, they need to have the same column names.

Remember that we previously created two separate dataframes for males and females?  Let’s put them back together using the \sphinxcode{\sphinxupquote{pd.concat}} method. Note how the index of this output retains the old index.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{combined\PYGZus{}data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{female\PYGZus{}df}\PYG{p}{,} \PYG{n}{male\PYGZus{}df}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

We can reset the index using the \sphinxcode{\sphinxupquote{reset\_index}} method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{male\PYGZus{}df}\PYG{p}{,} \PYG{n}{female\PYGZus{}df}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{n}{drop}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    salary  gender departm  years   age  publications  pubperyear
0    86285       0     bio   26.0  64.0            72    2.769231
1    77125       0     bio   28.0  58.0            43    1.535714
2    71922       0     bio   10.0  38.0            23    2.300000
3    70499       0     bio   16.0  46.0            64    4.000000
4    66624       0     bio   11.0  41.0            23    2.090909
..     ...     ...     ...    ...   ...           ...         ...
69   53662       1   neuro    1.0  31.0             3    3.000000
70   57185       1    stat    9.0  39.0             7    0.777778
71   52254       1    stat    2.0  32.0             9    4.500000
72   61885       1    math   23.0  60.0             9    0.391304
73   49542       1    math    3.0  33.0             5    1.666667

[74 rows x 7 columns]
\end{sphinxVerbatim}

We can also concatenate columns in addition to rows. Make sure that the number of rows are the same in each dataframe. For this example, we will just create two new data frames with a subset of the columns and then combine them again.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df1} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{df2} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{publications}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{df3} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{df1}\PYG{p}{,} \PYG{n}{df2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{df3}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   salary  gender   age  publications
0   86285       0  64.0            72
1   77125       0  58.0            43
2   71922       0  38.0            23
3   70499       0  46.0            64
4   66624       0  41.0            23
\end{sphinxVerbatim}


\subsubsection{Append}
\label{\detokenize{content/Introduction_to_Pandas:append}}
We can also combine datasets by appending new data to the end of a dataframe.

Suppose we want to append a new data entry of an additional participant onto the \sphinxcode{\sphinxupquote{df3}} dataframe. Notice that we need to specify to \sphinxcode{\sphinxupquote{ignore\_index=True}} and also that we need to assign the new dataframe back to a variable. This operation is not done in place.

For more information about concatenation and appending see Jake Vanderplas’s \sphinxhref{https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.06-Concat-And-Append.ipynb}{tutorial}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{new\PYGZus{}data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{100000}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{38}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{publications}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{46}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{df3} \PYG{o}{=} \PYG{n}{df3}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{new\PYGZus{}data}\PYG{p}{,} \PYG{n}{ignore\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{df3}\PYG{o}{.}\PYG{n}{tail}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    salary  gender   age  publications
70   57185       1  39.0             7
71   52254       1  32.0             9
72   61885       1  60.0             9
73   49542       1  33.0             5
74  100000       1  38.0            46
\end{sphinxVerbatim}


\subsubsection{Merge}
\label{\detokenize{content/Introduction_to_Pandas:merge}}
The most powerful method of merging data is using the \sphinxcode{\sphinxupquote{pd.merge}} method. This allows you to merge datasets of different shapes and sizes on specific variables that match. This is very common when you need to merge multiple sql tables together for example.

In this example, we are creating two separate data frames that have different states and columns and will merge on the \sphinxcode{\sphinxupquote{State}} column.

First, we will only retain rows where there is a match across dataframes, using \sphinxcode{\sphinxupquote{how=inner}}. This is equivalent to an ‘and’ join in sql.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df1} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{California}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Colorado}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{New Hampshire}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} 
                    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Capital}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sacremento}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Denver}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Concord}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{df2} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{California}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{New Hampshire}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{New York}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} 
                    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{39512223}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1359711}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{19453561}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{df3} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{left}\PYG{o}{=}\PYG{n}{df1}\PYG{p}{,} \PYG{n}{right}\PYG{o}{=}\PYG{n}{df2}\PYG{p}{,} \PYG{n}{on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{inner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df3}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           State     Capital Population
0     California  Sacremento   39512223
1  New Hampshire     Concord    1359711
\end{sphinxVerbatim}

Notice how there are only two rows in the merged dataframe.

We can also be more inclusive and match on \sphinxcode{\sphinxupquote{State}} column, but retain all rows. This is equivalent to an ‘or’ join.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df3} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{left}\PYG{o}{=}\PYG{n}{df1}\PYG{p}{,} \PYG{n}{right}\PYG{o}{=}\PYG{n}{df2}\PYG{p}{,} \PYG{n}{on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{State}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df3}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           State     Capital Population
0     California  Sacremento   39512223
1       Colorado      Denver        NaN
2  New Hampshire     Concord    1359711
3       New York         NaN   19453561
\end{sphinxVerbatim}

This is a very handy way to merge data when you have lots of files with missing data.  See Jake Vanderplas’s \sphinxhref{https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.07-Merge-and-Join.ipynb}{tutorial} for a more in depth overview.


\subsection{Grouping}
\label{\detokenize{content/Introduction_to_Pandas:grouping}}
We’ve seen above that it is very easy to summarize data over columns using the builtin functions such as \sphinxcode{\sphinxupquote{pd.mean()}}. Sometimes we are interested in summarizing data over different groups of rows. For example, what is the mean of participants in Condition A compared to Condition B?

This is suprisingly easy to compute in pandas using the \sphinxcode{\sphinxupquote{groupby}} operator, where we aggregate data using a specific operation over different labels.

One useful way to conceptualize this is using the \sphinxstylestrong{Split, Apply, Combine} operation (similar to map\sphinxhyphen{}reduce).

\sphinxincludegraphics{{split-apply-combine}.png}

This figure is taken from Jake Vanderplas’s tutorial and highlights how input data can be \sphinxstyleemphasis{split} on some key and then an operation such as sum can be \sphinxstyleemphasis{applied} separately to each split. Finally, the results of the applied function for each key can be \sphinxstyleemphasis{combined} into a new data frame.


\subsubsection{Groupby}
\label{\detokenize{content/Introduction_to_Pandas:groupby}}
In this example, we will use the \sphinxcode{\sphinxupquote{groupby}} operator to split the data based on gender labels and separately calculate the mean for each group.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                   salary  gender      years        age  publications  \PYGZbs{}
gender\PYGZus{}name                                                             
female       55719.666667     1.0   8.666667  38.888889     11.555556   
male         69108.492308     0.0  15.846154  46.492308     23.061538   

             pub\PYGZus{}per\PYGZus{}year  
gender\PYGZus{}name                
female           2.043170  
male             1.924709  
\end{sphinxVerbatim}

Other default aggregation methods include \sphinxcode{\sphinxupquote{.count()}}, \sphinxcode{\sphinxupquote{.mean()}}, \sphinxcode{\sphinxupquote{.median()}}, \sphinxcode{\sphinxupquote{.min()}}, \sphinxcode{\sphinxupquote{.max()}}, \sphinxcode{\sphinxupquote{.std()}}, \sphinxcode{\sphinxupquote{.var()}}, and \sphinxcode{\sphinxupquote{.sum()}}


\subsubsection{Transform}
\label{\detokenize{content/Introduction_to_Pandas:transform}}
While the split, apply, combine operation that we just demonstrated is extremely usefuly to quickly summarize data based on a grouping key, the resulting data frame is compressed to one row per grouping label.

Sometimes, we would like to perform an operation over groups, but retain the original data shape. One common example is standardizing data within a subject or grouping variable. Normally, you might think to loop over subject ids and separately z\sphinxhyphen{}score or center a variable and then recombine the subject data using a vertical concatenation operation.

The \sphinxcode{\sphinxupquote{transform}} method in pandas can make this much easier and faster!

Suppose we want to compute the standardized salary separately for each department. We can standardize using a z\sphinxhyphen{}score which requires subtracting the departmental mean from each professor’s salary in that department, and then dividing it by the departmental standard deviation.

We can do this by using the \sphinxcode{\sphinxupquote{groupby(key)}} method chained with the \sphinxcode{\sphinxupquote{.transform(function)}} method. It will group the dataframe by the key column, perform the “function” transformation of the data and return data in same format. We can then assign the results to a new column in the dataframe.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary\PYGZus{}dept\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{department}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{/}\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{department}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{std}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/ipykernel\PYGZus{}launcher.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row\PYGZus{}indexer,col\PYGZus{}indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas\PYGZhy{}docs/stable/user\PYGZus{}guide/indexing.html\PYGZsh{}returning\PYGZhy{}a\PYGZhy{}view\PYGZhy{}versus\PYGZhy{}a\PYGZhy{}copy
  \PYGZdq{}\PYGZdq{}\PYGZdq{}Entry point for launching an IPython kernel.
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   salary  gender department  years   age  publications  pub\PYGZus{}per\PYGZus{}year  \PYGZbs{}
0   86285       0        bio   26.0  64.0            72      2.769231   
1   77125       0        bio   28.0  58.0            43      1.535714   
2   71922       0        bio   10.0  38.0            23      2.300000   
3   70499       0        bio   16.0  46.0            64      4.000000   
4   66624       0        bio   11.0  41.0            23      2.090909   

  gender\PYGZus{}name  salary\PYGZus{}dept\PYGZus{}z  
0        male       2.468065  
1        male       1.493198  
2        male       0.939461  
3        male       0.788016  
4        male       0.375613  
\end{sphinxVerbatim}

This worked well, but is also pretty verbose. We can simplify the syntax a little bit more using a \sphinxcode{\sphinxupquote{lambda}} function, where we can define the zscore function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{calc\PYGZus{}zscore} \PYG{o}{=} \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{x}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{/} \PYG{n}{x}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary\PYGZus{}dept\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{department}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{calc\PYGZus{}zscore}\PYG{p}{)}

\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/ipykernel\PYGZus{}launcher.py:3: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row\PYGZus{}indexer,col\PYGZus{}indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas\PYGZhy{}docs/stable/user\PYGZus{}guide/indexing.html\PYGZsh{}returning\PYGZhy{}a\PYGZhy{}view\PYGZhy{}versus\PYGZhy{}a\PYGZhy{}copy
  This is separate from the ipykernel package so we can avoid doing imports until
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   salary  gender department  years   age  publications  pub\PYGZus{}per\PYGZus{}year  \PYGZbs{}
0   86285       0        bio   26.0  64.0            72      2.769231   
1   77125       0        bio   28.0  58.0            43      1.535714   
2   71922       0        bio   10.0  38.0            23      2.300000   
3   70499       0        bio   16.0  46.0            64      4.000000   
4   66624       0        bio   11.0  41.0            23      2.090909   

  gender\PYGZus{}name  salary\PYGZus{}dept\PYGZus{}z  
0        male       2.468065  
1        male       1.493198  
2        male       0.939461  
3        male       0.788016  
4        male       0.375613  
\end{sphinxVerbatim}

For a more in depth overview of data aggregation and grouping, check out Jake Vanderplas’s \sphinxhref{http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb}{tutorial}


\subsection{Reshaping Data}
\label{\detokenize{content/Introduction_to_Pandas:reshaping-data}}
The last topic we will cover in this tutorial is reshaping data. Data is often in the form of observations by features, in which there is a single row for each independent observation of data and a separate column for each feature of the data. This is commonly referred to as as the \sphinxstyleemphasis{wide} format. However, when running regression or plotting in libraries such as seaborn, we often want our data in the \sphinxstyleemphasis{long} format, in which each grouping variable is specified in a separate column.

In this section we cover how to go from wide to long using the \sphinxcode{\sphinxupquote{melt}} operation and from long to wide using the \sphinxcode{\sphinxupquote{pivot}} function.


\subsubsection{Melt}
\label{\detokenize{content/Introduction_to_Pandas:melt}}
To \sphinxcode{\sphinxupquote{melt}} a dataframe into the long format, we need to specify which variables are the \sphinxcode{\sphinxupquote{id\_vars}}, which ones should be combined into a \sphinxcode{\sphinxupquote{value\_var}}, and finally, what we should label the column name for the \sphinxcode{\sphinxupquote{value\_var}}, and also for the \sphinxcode{\sphinxupquote{var\_name}}. We will call the values ‘Ratings’ and variables ‘Condition’.

First, we need to create a dataset to play with.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    ID         A         B         C
0  1.0  0.330666  0.893955  0.524217
1  2.0  0.881766  0.722020  0.895915
2  3.0  0.804144  0.316754  0.357837
3  4.0  0.830743  0.550464  0.704257
4  5.0  0.881331  0.755053  0.811475
\end{sphinxVerbatim}

Now, let’s melt the dataframe into the long format.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}long} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{melt}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,}\PYG{n}{id\PYGZus{}vars}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{value\PYGZus{}vars}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{var\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Condition}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{value\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df\PYGZus{}long}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     ID Condition    Rating
0   1.0         A  0.330666
1   2.0         A  0.881766
2   3.0         A  0.804144
3   4.0         A  0.830743
4   5.0         A  0.881331
5   1.0         B  0.893955
6   2.0         B  0.722020
7   3.0         B  0.316754
8   4.0         B  0.550464
9   5.0         B  0.755053
10  1.0         C  0.524217
11  2.0         C  0.895915
12  3.0         C  0.357837
13  4.0         C  0.704257
14  5.0         C  0.811475
\end{sphinxVerbatim}

Notice how the id variable is repeated for each condition?


\subsubsection{Pivot}
\label{\detokenize{content/Introduction_to_Pandas:pivot}}
We can also go back to the wide data format from a long dataframe using \sphinxcode{\sphinxupquote{pivot}}. We just need to specify the variable containing the labels which will become the \sphinxcode{\sphinxupquote{columns}} and the \sphinxcode{\sphinxupquote{values}} column that will be broken into separate columns.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}wide} \PYG{o}{=} \PYG{n}{df\PYGZus{}long}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Condition}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df\PYGZus{}wide}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Condition         A         B         C
ID                                     
1.0        0.330666  0.893955  0.524217
2.0        0.881766  0.722020  0.895915
3.0        0.804144  0.316754  0.357837
4.0        0.830743  0.550464  0.704257
5.0        0.881331  0.755053  0.811475
\end{sphinxVerbatim}


\subsection{Exercises ( Homework)}
\label{\detokenize{content/Introduction_to_Pandas:exercises-homework}}
The following exercises uses the dataset “salary\_exercise.csv” adapted from material available \sphinxhref{http://data.princeton.edu/wws509/datasets/\#salary}{here}

These are the salary data used in Weisberg’s book, consisting of observations on six variables for 52 tenure\sphinxhyphen{}track professors in a small college. The variables are:
\begin{itemize}
\item {} 
sx = Sex, coded 1 for female and 0 for male

\item {} 
rk = Rank, coded

\item {} 
1 for assistant professor,

\item {} 
2 for associate professor, and

\item {} 
3 for full professor

\item {} 
yr = Number of years in current rank

\item {} 
dg = Highest degree, coded 1 if doctorate, 0 if masters

\item {} 
yd = Number of years since highest degree was earned

\item {} 
sl = Academic year salary, in dollars.

\end{itemize}

Reference: S. Weisberg (1985). Applied Linear Regression, Second Edition. New York: John Wiley and Sons. Page 194.


\subsubsection{Exercise 1}
\label{\detokenize{content/Introduction_to_Pandas:exercise-1}}
Read the salary\_exercise.csv into a dataframe, and change the column names to a more readable format such as sex, rank, yearsinrank, degree, yearssinceHD, and salary.

Clean the data by excluding rows with any missing value.

What are the overall mean, standard deviation, min, and maximum of professors’ salary?


\subsubsection{Exercise 2}
\label{\detokenize{content/Introduction_to_Pandas:exercise-2}}
Create two separate dataframes based on the type of degree. Now calculate the mean salary of the 5 oldest professors of each degree type.


\subsubsection{Exercise 3}
\label{\detokenize{content/Introduction_to_Pandas:exercise-3}}
What is the correlation between the standardized salary \sphinxstyleemphasis{across} all ranks and the standardized salary \sphinxstyleemphasis{within} ranks?


\section{Introduction to Plotting}
\label{\detokenize{content/Introduction_to_Plotting:introduction-to-plotting}}\label{\detokenize{content/Introduction_to_Plotting::doc}}
\sphinxstyleemphasis{Written by Luke Chang \& Jin Cheong}

In this lab, we will introduce the basics of plotting in python using the \sphinxcode{\sphinxupquote{matplotlib}} and \sphinxcode{\sphinxupquote{seaborn}} packages. Matlotlib is probably the most popular python package for 2D graphics and has a nice tradeoff between ease of use and customizability. We will be working with the \sphinxcode{\sphinxupquote{pyplot}} interface, which is an object\sphinxhyphen{}oriented plotting library based on plotting in Matlab. Many graphics libraries are built on top of matplotlib, and have tried to make plotting even easier. One library that is very nice to generate plots similar to how analyses are performed is \sphinxhref{https://seaborn.pydata.org/examples/index.html}{seaborn}. There are many great tutorials online. Here are a few that I found to be helpful from \sphinxhref{https://github.com/neurohackademy/visualization-in-python/blob/master/visualization-in-python.ipynb}{neurohackademy}, \sphinxhref{http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb}{Jake Vanderplas}, and \sphinxhref{https://github.com/rougier/matplotlib-tutorial}{rougier}.


\subsection{Matplotlib Key Concepts}
\label{\detokenize{content/Introduction_to_Plotting:matplotlib-key-concepts}}
There are a few different types of concepts in the \sphinxcode{\sphinxupquote{matplotlib.pyplot}} framework.
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{Figure}}: Essentially the canvas, which contains the plots

\item {} 
\sphinxcode{\sphinxupquote{Axes}}: An individual plot within a figure. Each Axes has a title, an x\sphinxhyphen{}label, and a y\sphinxhyphen{}label

\item {} 
\sphinxcode{\sphinxupquote{Axis}}: These contain the graph limits and tickmarks

\item {} 
\sphinxcode{\sphinxupquote{Artist}}: Everything that is visiable on the figure is an artist (e.g., Text objects, Line2D object, collection objects). Artists are typically tied to a specific Axes.

\end{itemize}

\sphinxstylestrong{Note}: \sphinxcode{\sphinxupquote{\%matplotlib inline}} is an example of ‘cell magic’ and enables plotting \sphinxstyleemphasis{within} the notebook and not opening a separate window. In addition, you may want to try using \sphinxcode{\sphinxupquote{\%matplotlib notebook}}, which will allow more interactive plotting.

Let’s get started by loading the modules we will use for this tutorial.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline 

\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\end{sphinxVerbatim}


\subsubsection{Lineplot}
\label{\detokenize{content/Introduction_to_Plotting:lineplot}}
First, let’s generate some numbers and create a basic lineplot.

In this example, we plot:

\(y = x^2\)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{x} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZlt{}matplotlib.lines.Line2D at 0x7f7f1053e450\PYGZgt{}]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_3_1}.png}


\subsubsection{Scatterplot}
\label{\detokenize{content/Introduction_to_Plotting:scatterplot}}
We can also plot associations between two different variables using a scatterplot. In this example, we will simulate correlated data with \(\mathcal{N}(0,1)\) using \sphinxcode{\sphinxupquote{np.random.multivariate\_normal}} and create a scatterplot. Try playing with the covariance values to change the degree of association.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{500}
\PYG{n}{r} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{9}
\PYG{n}{mu} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{cov} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
    \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{r}\PYG{p}{]}\PYG{p}{,}
    \PYG{p}{[}\PYG{n}{r}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{multivariate\PYGZus{}normal}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{cov}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.collections.PathCollection at 0x7f7f4b304550\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_5_1}.png}


\subsubsection{Histogram}
\label{\detokenize{content/Introduction_to_Plotting:histogram}}
We can also plot the distribution of a variable using a histogram.

Let’s see if the data we simulated above are actually normally distributed.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(array([ 13.,  27.,  58.,  97., 111.,  95.,  53.,  31.,  12.,   3.]),
 array([\PYGZhy{}2.49785684, \PYGZhy{}1.95098132, \PYGZhy{}1.40410579, \PYGZhy{}0.85723027, \PYGZhy{}0.31035474,
         0.23652078,  0.78339631,  1.33027183,  1.87714736,  2.42402288,
         2.9708984 ]),
 \PYGZlt{}a list of 10 Patch objects\PYGZgt{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_7_1}.png}

We can also plot overlaying histograms. Let’s simulate more data, but shift the mean of one by 3.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{r} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{9}
\PYG{n}{mu} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{cov} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
    \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{r}\PYG{p}{]}\PYG{p}{,}
    \PYG{p}{[}\PYG{n}{r}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{multivariate\PYGZus{}normal}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{cov}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{7}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(array([[ 17.,  45., 143., 175.,  95.,  24.,   1.,   0.,   0.,   0.],
        [  0.,   0.,   0.,   7.,  25.,  90., 167., 143.,  62.,   6.]]),
 array([\PYGZhy{}2.77829094, \PYGZhy{}1.93245112, \PYGZhy{}1.0866113 , \PYGZhy{}0.24077149,  0.60506833,
         1.45090815,  2.29674797,  3.14258779,  3.9884276 ,  4.83426742,
         5.68010724]),
 \PYGZlt{}a list of 2 Lists of Patches objects\PYGZgt{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_9_1}.png}


\subsubsection{Bar Plot}
\label{\detokenize{content/Introduction_to_Plotting:bar-plot}}
We can also plot the same data as a bar plot to emphasize the difference in the means of the distributions. To create a bar plot, we need to specify the bar names and the heights of the bars.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}BarContainer object of 2 artists\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_11_1}.png}


\subsubsection{3D Plots}
\label{\detokenize{content/Introduction_to_Plotting:d-plots}}
We can also plot in 3 dimensions with \sphinxcode{\sphinxupquote{mplot3d}}. Here we will simulate 3 different variables with different correlations.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{mpl\PYGZus{}toolkits} \PYG{k+kn}{import} \PYG{n}{mplot3d}

\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{500}

\PYG{n}{r1} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{1}
\PYG{n}{r2} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{5}
\PYG{n}{r3} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{9}

\PYG{n}{mu} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{5.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{10.0}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{cov} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
        \PYG{p}{[}  \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{r1}\PYG{p}{,} \PYG{n}{r2}\PYG{p}{]}\PYG{p}{,}
        \PYG{p}{[} \PYG{n}{r1}\PYG{p}{,}  \PYG{l+m+mi}{1}\PYG{p}{,}  \PYG{n}{r3}\PYG{p}{]}\PYG{p}{,}
        \PYG{p}{[} \PYG{n}{r2}\PYG{p}{,}  \PYG{n}{r3}\PYG{p}{,}  \PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{p}{]}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{multivariate\PYGZus{}normal}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{cov}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}


\PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axes}\PYG{p}{(}\PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter3D}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{4}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}mpl\PYGZus{}toolkits.mplot3d.art3d.Path3DCollection at 0x7f7f087ed750\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_13_1}.png}


\subsubsection{Customization}
\label{\detokenize{content/Introduction_to_Plotting:customization}}
One of the nice things about matplotlib is that everything is customizable.

Let’s go back to our first scatterplot and show how we can customize it to make it easier to read.

We can specify the type of \sphinxcode{\sphinxupquote{marker}}, the \sphinxcode{\sphinxupquote{color}}, the \sphinxcode{\sphinxupquote{alpha}} transparency, and the size of the dots with \sphinxcode{\sphinxupquote{s}}.

We can also label the axes with \sphinxcode{\sphinxupquote{xlabel}} and \sphinxcode{\sphinxupquote{ylabel}}, and add a \sphinxcode{\sphinxupquote{title}}.

Finally, we can add text annotations, such as the strength of the correlation with \sphinxcode{\sphinxupquote{annotate}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{500}
\PYG{n}{r} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{9}
\PYG{n}{mu} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{cov} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
    \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{r}\PYG{p}{]}\PYG{p}{,}
    \PYG{p}{[}\PYG{n}{r}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{multivariate\PYGZus{}normal}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{cov}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{25}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scatterplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{r}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{xy}\PYG{o}{=}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{xycoords}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{14}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(\PYGZhy{}2, 2, \PYGZsq{}r=0.9\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_15_1}.png}


\subsubsection{Layouts}
\label{\detokenize{content/Introduction_to_Plotting:layouts}}
The easiest way to make customized layouts that can include multiple panels of a plot are with \sphinxcode{\sphinxupquote{subplot}}.

\sphinxincludegraphics{{subplot}.png}

There are two different ways to index. One is by adding a subplot. The first number is the number of rows, the second is the number of columns, and the third is the index number.

I personally prefer to index directly into the \sphinxcode{\sphinxupquote{ax}} object with rows and columns as I find it more intuitive.

You can do even more advanced layouts with panels of different sizes using \sphinxhref{https://matplotlib.org/3.2.1/tutorials/intermediate/gridspec.html}{gridspec}.

Let’s make our simulation code into a function and use subplots to plot multiple panels.

We specify the number of rows and columns when we initialize the plot. We can also play with the size of the plot. Here we tell matplotlib that the x and y axes will be shared across the different panels. Finally, \sphinxcode{\sphinxupquote{plt.tight\_layout()}} helps keep everything formatted and organized nicely.

When modifying \sphinxcode{\sphinxupquote{axes}} we need to use the \sphinxcode{\sphinxupquote{set\_\{\}}} command rather than just the command itself. For example, \sphinxcode{\sphinxupquote{ax{[}0,0{]}.set\_xlabel(\textquotesingle{}X\textquotesingle{})}} rather than \sphinxcode{\sphinxupquote{plt.xlabel(\textquotesingle{}X\textquotesingle{})}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{mu} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{cov} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
        \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{r}\PYG{p}{]}\PYG{p}{,}
        \PYG{p}{[}\PYG{n}{r}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{multivariate\PYGZus{}normal}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{cov}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}

\PYG{n}{f}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{sharey}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)} 

\PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{1}
\PYG{n}{sim1} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{r}\PYG{o}{=}\PYG{n}{r}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{sim1}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sim1}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{25}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{r}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{3}
\PYG{n}{sim1} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{r}\PYG{o}{=}\PYG{n}{r}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{sim1}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sim1}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{25}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{r}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{6}
\PYG{n}{sim1} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{r}\PYG{o}{=}\PYG{n}{r}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{sim1}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sim1}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{25}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{r}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{9}
\PYG{n}{sim1} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{r}\PYG{o}{=}\PYG{n}{r}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{sim1}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sim1}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{25}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{r}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_17_0}.png}


\subsubsection{Saving Plots}
\label{\detokenize{content/Introduction_to_Plotting:saving-plots}}
Plots can be saved to disk using the \sphinxcode{\sphinxupquote{savefig}} command. There are lots of ways to customize the saved plot. I typically save rasterized versions as \sphinxcode{\sphinxupquote{.png}} and vectorized versions as \sphinxcode{\sphinxupquote{.pdf}}. Don’t forget to specify a path where you want the file written to.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{savefig}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MyFirstPlot.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{savefig}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/Users/lukechang/Downloads/MyFirstPlot.pdf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Figure size 432x288 with 0 Axes\PYGZgt{}
\end{sphinxVerbatim}


\subsection{Seaborn}
\label{\detokenize{content/Introduction_to_Plotting:seaborn}}
\sphinxhref{https://seaborn.pydata.org/index.html}{Seaborn} is a plotting library built on Matplotlib that has many pre\sphinxhyphen{}configured plots that are often used for visualization.
Other great tutorials about seaborn are \sphinxhref{http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.14-Visualization-With-Seaborn.ipynb}{here}

Most seaborn plots can be customized using the standard matplotlib commands, though be sure to look at the docstrings first as often there are already keywords within each type of plot to do what you want.


\subsubsection{Scatterplots}
\label{\detokenize{content/Introduction_to_Plotting:scatterplots}}
There are many variants of each type of plot that you might like to use. For example, \sphinxcode{\sphinxupquote{scatterplot}}, \sphinxcode{\sphinxupquote{regplot}}, \sphinxcode{\sphinxupquote{jointplot}}.

Let’s update our simulation code a little bit to make this easier.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{mu} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{cov} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
        \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{r}\PYG{p}{]}\PYG{p}{,}
        \PYG{p}{[}\PYG{n}{r}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{multivariate\PYGZus{}normal}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{cov}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scatterplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Scatterplot\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_22_1}.png}

We can add regression lines with \sphinxcode{\sphinxupquote{regplot}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scatterplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Scatterplot\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_24_1}.png}

We can add histograms to show the distributions of x and y with \sphinxcode{\sphinxupquote{jointplot}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{jointplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jointplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Jointplot\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_26_1}.png}

We can create a quick way to view relations between multiple variables using \sphinxcode{\sphinxupquote{pairplot}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{,}\PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{V1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{V2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{V3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{V4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{pairplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}seaborn.axisgrid.PairGrid at 0x7f7f28b75d10\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_28_1}.png}


\subsubsection{Factor plots}
\label{\detokenize{content/Introduction_to_Plotting:factor-plots}}
Factor plots allow you to visualize the distribution of parameters in different forms such as point, bar, or violin graphs.

One important thing to note is that the data needs to be in the long format. Let’s quickly simulate some data to plot.

Here are some possible values for kind : \{point, bar, count, box, violin, strip\}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data1} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{9}\PYG{p}{)}
\PYG{n}{data1}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group1}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{data1}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{data1}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{l+m+mi}{1}

\PYG{n}{data2} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{2}
\PYG{n}{data2}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group2}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{data1}\PYG{p}{,} \PYG{n}{data2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{data\PYGZus{}long} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{melt}\PYG{p}{(}\PYG{n}{id\PYGZus{}vars}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{value\PYGZus{}vars}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{var\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{value\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{catplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data\PYGZus{}long}\PYG{p}{,} \PYG{n}{ci}\PYG{o}{=}\PYG{l+m+mi}{68}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bar}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Catplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Catplot\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_30_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{catplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data\PYGZus{}long}\PYG{p}{,} \PYG{n}{ci}\PYG{o}{=}\PYG{l+m+mi}{68}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{swarm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Swarmplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Swarmplot\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_31_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{catplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data\PYGZus{}long}\PYG{p}{,} \PYG{n}{ci}\PYG{o}{=}\PYG{l+m+mi}{68}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{strip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Stripplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Swarmplot\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_32_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{catplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variable}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data\PYGZus{}long}\PYG{p}{,} \PYG{n}{ci}\PYG{o}{=}\PYG{l+m+mi}{68}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{violin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Violinplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Violinplot\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_33_1}.png}


\subsubsection{Heatmaps}
\label{\detokenize{content/Introduction_to_Plotting:heatmaps}}
Heatmap plots allow you to visualize matrices such as correlation matrices that show relationships across multiple variables.

Let’s create a dataset with different relationships between variables. We can quickly calculate the correlation between these variables and visualize it with a heatmap

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{,} \PYG{n}{simulate\PYGZus{}xy}\PYG{p}{(}\PYG{n}{r}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{data}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{V1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{V2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{V3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{V4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{corr} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{corr}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f7f09b933d0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_35_1}.png}

Like all other plots, we can also customize heatmaps.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{corr}\PYG{p}{,} \PYG{n}{square}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{annot}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{linewidths}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RdBu\PYGZus{}r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Correlation Matrix}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Correlation Matrix\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_37_1}.png}

There is also a neat type of heatmap that will also reorganize your variable based on clustering.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{clustermap}\PYG{p}{(}\PYG{n}{corr}\PYG{p}{,} \PYG{n}{square}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{annot}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RdBu\PYGZus{}r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Clustermap}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Clustermap\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_39_1}.png}


\subsection{Pandas}
\label{\detokenize{content/Introduction_to_Plotting:pandas}}
We introduced \sphinxcode{\sphinxupquote{Pandas}} in a previous tutorial. It can also call matplotlib to quickly generate plots.

We will use the same dataset used in that tutorial to generate some plots using \sphinxcode{\sphinxupquote{pd.Dataframe}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../../data/salary.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sep} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{header}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{infer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{!=}\PYG{l+m+mi}{2}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} key: We use the departm as the grouping factor. }
\PYG{n}{key} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{departm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Let\PYGZsq{}s create an anonmyous function for calculating zscores using lambda:}
\PYG{c+c1}{\PYGZsh{} We want to standardize salary for each department.}
\PYG{n}{zscore} \PYG{o}{=} \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{x}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{/} \PYG{n}{x}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Now let\PYGZsq{}s calculate zscores separately within each department}
\PYG{n}{transformed} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{zscore}\PYG{p}{)}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary\PYGZus{}in\PYGZus{}departm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{transformed}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

Now we have \sphinxcode{\sphinxupquote{salary\_in\_departm}} column showing standardized salary per department.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x1a1d97f320\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_44_1}.png}


\subsubsection{Scatterplot}
\label{\detokenize{content/Introduction_to_Plotting:id1}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{years}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scatter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{years}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x1a1dc9df28\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_46_1}.png}

Now plot all four categories

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{sharey}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{f}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Salary in relation to other variables}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scatter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scatter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dept\PYGZus{}num}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scatter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{years}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{scatter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x1a1dedb128\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_48_1}.png}

The problem is that it treats department as a continuous variable.


\subsubsection{Generating bar \sphinxhyphen{} errorbar plots in Pandas}
\label{\detokenize{content/Introduction_to_Plotting:generating-bar-errorbar-plots-in-pandas}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{means} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{errors} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{salary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{means}\PYG{o}{.}\PYG{n}{plot}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{yerr}\PYG{o}{=}\PYG{n}{errors}\PYG{p}{,}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Plotting_51_0}.png}


\subsection{Interactive Plots}
\label{\detokenize{content/Introduction_to_Plotting:interactive-plots}}
Interactive data visualizations are an exciting development and are likely to continue to grow in popularity with the rapid developments in web frontend frameworks. Covering these libraries is beyond the scope of this tutorial, but I highly encourage you to check them out. Some of them are surprisingly easy to use and make exploring your data and sharing your results much more fun.

It is possible to add some basic interactivity to the plotting libraries covered in this tutorial in jupyter notebooks with \sphinxhref{https://ipywidgets.readthedocs.io/en/latest/}{ipywidgets}. You will see a few examples of this in other tutorials.


\subsubsection{Plotly}
\label{\detokenize{content/Introduction_to_Plotting:plotly}}
\sphinxhref{https://plotly.com/python/}{Plotly} is an graphing library to make interactive plots.


\subsubsection{Bokeh}
\label{\detokenize{content/Introduction_to_Plotting:bokeh}}
\sphinxhref{https://docs.bokeh.org/en/latest/index.html}{Bokeh} is an interactive visualization library


\subsubsection{Altair}
\label{\detokenize{content/Introduction_to_Plotting:altair}}
\sphinxhref{https://altair-viz.github.io/}{Altair} is a declarative statistical visualization library for Python based on Vega


\subsection{Exercises}
\label{\detokenize{content/Introduction_to_Plotting:exercises}}
The following exercises uses the dataset “salary\_exercise.csv” adapted from material available \sphinxhref{http://data.princeton.edu/wws509/datasets/\#salary}{here}

These are the salary data used in Weisberg’s book, consisting of observations on six variables for 52 tenure\sphinxhyphen{}track professors in a small college. The variables are:
\begin{itemize}
\item {} 
sx = Sex, coded 1 for female and 0 for male

\item {} 
rk = Rank, coded

\item {} 
1 for assistant professor,

\item {} 
2 for associate professor, and

\item {} 
3 for full professor

\item {} 
yr = Number of years in current rank

\item {} 
dg = Highest degree, coded 1 if doctorate, 0 if masters

\item {} 
yd = Number of years since highest degree was earned

\item {} 
sl = Academic year salary, in dollars.

\end{itemize}

Reference: S. Weisberg (1985). Applied Linear Regression, Second Edition. New York: John Wiley and Sons. Page 194.


\subsubsection{Exercise 1}
\label{\detokenize{content/Introduction_to_Plotting:exercise-1}}
Recreate the plot shown in figure.On the left is a correlation of all parameters of only the male professors.On the right is the same but only for female professors.The colormap code used is \sphinxcode{\sphinxupquote{RdBu\_r}}. Read the Docstrings on sns.heatmap or search the internet to figure out how to change the colormap, scale the colorbar, and create square line boundaries.Place titles for each plot as shown, and your name as the main title.

\sphinxincludegraphics{{hw2-3}.png}


\subsubsection{Exercise 2}
\label{\detokenize{content/Introduction_to_Plotting:exercise-2}}
Recreate the following plot from the salary\_exercise.csv dataset.Create a 1 x 2 subplot.On the left is a bar\sphinxhyphen{}errorbar of salary per gender.On the right is a scatterplot of salary on y\sphinxhyphen{}axis and years in rank on the x\sphinxhyphen{}axis.Set the axis limits as shown in the picture and modify their lables.Add axis label names.Add a legend for the scatterplot and place it at a bottom\sphinxhyphen{}right location.Add your name as the main title of the plot.

\sphinxincludegraphics{{hw2-4}.png}


\section{Glossary}
\label{\detokenize{content/Glossary:glossary}}\label{\detokenize{content/Glossary::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

Throughout this course we will use a variety of different functions available in the base Python library, but also many other libraries in the scientific computing stack. Here we provide a list of all of the functions that are used across the various notebooks. It can be a helpful reference when you are learning Python about the types of things you can do with various packages. Remember you can always view the docstrings for any function by adding a \sphinxcode{\sphinxupquote{?}} to the end of the function name.


\subsection{Jupyter Cell Magic}
\label{\detokenize{content/Glossary:jupyter-cell-magic}}
Magics are specific to and provided by the IPython kernel. Whether Magics are available on a kernel is a decision that is made by the kernel developer on a per\sphinxhyphen{}kernel basis. To work properly, Magics must use a syntax element which is not valid in the underlying language. For example, the IPython kernel uses the \sphinxcode{\sphinxupquote{\%}} syntax element for Magics as \sphinxcode{\sphinxupquote{\%}} is not a valid unary operator in Python. However, \sphinxcode{\sphinxupquote{\%}} might have meaning in other languages.

\sphinxhref{https://ipython.readthedocs.io/en/stable/interactive/magics.html\#magic-conda}{\%conda}: Run the conda package manager within the current kernel.

\sphinxhref{https://ipython.readthedocs.io/en/stable/interactive/magics.html\#magic-debug}{\%debug}: Activate the interactive debugger. This magic command support two ways of activating debugger. One is to activate debugger before executing code. This way, you can set a break point, to step through the code from the point. You can use this mode by giving statements to execute and optionally a breakpoint. The other one is to activate debugger in post\sphinxhyphen{}mortem mode. You can activate this mode simply running \%debug without any argument. If an exception has just occurred, this lets you inspect its stack frames interactively. Note that this will always work only on the last traceback that occurred, so you must call this quickly after an exception that you wish to inspect has fired, because if another one occurs, it clobbers the previous one.

\sphinxhref{https://ipython.readthedocs.io/en/stable/interactive/magics.html\#magic-matplotlib}{\%matplotlib}: Set up matplotlib to work interactively. Example: \sphinxcode{\sphinxupquote{\%matplotlib inline}}

This function lets you activate matplotlib interactive support at any point during an IPython session. It does not import anything into the interactive namespace.

\sphinxhref{https://ipython.readthedocs.io/en/stable/interactive/magics.html\#magic-timeit}{\%timeit}: Time execution of a Python statement or expression using the timeit module. This function can be used both as a line and cell magic

\sphinxhref{https://ipython.readthedocs.io/en/stable/interactive/magics.html\#magic-system}{!}: Shell execute \sphinxhyphen{} run shell command and capture output (!! is short\sphinxhyphen{}hand). Example: \sphinxcode{\sphinxupquote{!pip}}.


\subsection{Base Python Functions}
\label{\detokenize{content/Glossary:base-python-functions}}
These functions are all bundled with Python

\sphinxhref{https://docs.python.org/2/library/stdtypes.html\#truth-value-testing}{any}: Test if any of the elements are true.

\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}: Cast as boolean type

\sphinxhref{https://docs.python.org/3/library/functions.html\#func-dict}{dict}: Cast as dictionary type

\sphinxhref{https://docs.python.org/3/library/functions.html\#enumerate}{enumerate}: Return an enumerate object. iterable must be a sequence, an iterator, or some other object which supports iteration. The \sphinxstylestrong{next}() method of the iterator returned by enumerate() returns a tuple containing a count (from start which defaults to 0) and the values obtained from iterating over iterable.

\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}: Return a floating point number constructed from a number or string x.

\sphinxhref{https://docs.python.org/3/reference/import.html}{import}: Import python module into namespace.

\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}: Cast as integer type

\sphinxhref{https://docs.python.org/3/library/functions.html\#len}{len}: Return the length (the number of items) of an object. The argument may be a sequence (such as a string, bytes, tuple, list, or range) or a collection (such as a dictionary, set, or frozen set).

\sphinxhref{https://docs.python.org/3/library/glob.html}{glob.glob}: The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order. No tilde expansion is done, but \sphinxcode{\sphinxupquote{*}}, \sphinxcode{\sphinxupquote{?}}, and character ranges expressed with \sphinxcode{\sphinxupquote{{[}{]}}} will be correctly matched. This is done by using the \sphinxcode{\sphinxupquote{os.scandir()}} and \sphinxcode{\sphinxupquote{fnmatch.fnmatch()}} functions in concert, and not by actually invoking a subshell.

\sphinxhref{https://docs.python.org/3/library/functions.html\#func-list}{list}: Cast as list type

\sphinxhref{https://docs.python.org/3/library/functions.html\#max}{max}: Return the largest item in an iterable or the largest of two or more arguments.

\sphinxhref{https://docs.python.org/3/library/functions.html\#min}{min}: Return the smallest item in an iterable or the smallest of two or more arguments.

\sphinxhref{https://docs.python.org/2/library/os.path.html\#os.path.basename}{os.path.basename}: Return the base name of pathname path. This is the second element of the pair returned by passing path to the function split(). Note that the result of this function is different from the Unix basename program; where basename for ‘/foo/bar/’ returns ‘bar’, the basename() function returns an empty string (‘’).

\sphinxhref{https://docs.python.org/2/library/os.path.html\#os.path.join}{os.path.join}: Join one or more path components intelligently. The return value is the concatenation of path and any members of paths with exactly one directory separator (os.sep) following each non\sphinxhyphen{}empty part except the last, meaning that the result will only end in a separator if the last part is empty. If a component is an absolute path, all previous components are thrown away and joining continues from the absolute path component.

\sphinxhref{https://docs.python.org/3/tutorial/inputoutput.html}{print}: Print strings. Recommend using f\sphinxhyphen{}strings formatting. Example, \sphinxcode{\sphinxupquote{print(f\textquotesingle{}Results: \{variable\}\textquotesingle{})}}.

\sphinxhref{https://ipython.readthedocs.io/en/stable/interactive/magics.html\#magic-pwd}{pwd}: Print current working directory

\sphinxhref{https://docs.python.org/3/library/functions.html\#sorted}{sorted}: Return a new sorted list from the items in iterable.

\sphinxhref{https://docs.python.org/3/library/functions.html\#func-str}{str}: For more information on static methods, see \sphinxhref{https://docs.python.org/3/reference/datamodel.html\#types}{The standard type hierarchy}.

\sphinxhref{https://docs.python.org/3/library/functions.html\#func-range}{range}: Rather than being a function, range is actually an immutable sequence type, as documented in Ranges and Sequence Types — list, tuple, range.

\sphinxhref{https://docs.python.org/3/library/functions.html\#func-tuple}{tuple}: Cast as tuple type

\sphinxhref{https://docs.python.org/3/library/functions.html\#type}{type}: Return the type of the object.

\sphinxhref{https://docs.python.org/3/library/functions.html\#zip}{zip}: Make an iterator that aggregates elements from each of the iterables.


\subsection{Pandas}
\label{\detokenize{content/Glossary:pandas}}
\sphinxhref{https://pandas.pydata.org/}{pandas} is an open source, BSD\sphinxhyphen{}licensed library providing high\sphinxhyphen{}performance, easy\sphinxhyphen{}to\sphinxhyphen{}use data structures and data analysis tools for the Python programming language.

\sphinxcode{\sphinxupquote{import pandas as pd}}


\bigskip\hrule\bigskip


\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read\_csv.html}{pd.read\_csv}: Read a comma\sphinxhyphen{}separated values (csv) file into DataFrame.

\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html}{pd.concat}: Concatenate pandas objects along a particular axis with optional set logic along the other axes.

\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html}{pd.DataFrame.isnull}: Detect missing values.

\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html}{pd.DataFrame.mean}: Return the mean of the values for the requested axis.

\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.std.html}{pd.DataFrame.std}: Return sample standard deviation over requested axis.

\sphinxhref{https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.plot.html}{pd.DataFrame.plot}: Plot data using matplotlib

\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html}{pd.DataFrame.map}: Map values of Series according to input correspondence.

\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html}{pd.DataFrame.groupby}: Group DataFrame or Series using a mapper or by a Series of columns.

\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html}{pd.DataFrame.fillna}: Fill NA/NaN values using the specified method.

\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html}{pd.DataFrame.replace}: Replace values given in to\_replace with value.


\subsection{NumPy}
\label{\detokenize{content/Glossary:numpy}}
\sphinxhref{https://numpy.org/}{NumPy} is the fundamental package for scientific computing with Python. It contains among other things:
\begin{itemize}
\item {} 
a powerful N\sphinxhyphen{}dimensional array object

\item {} 
sophisticated (broadcasting) functions

\item {} 
tools for integrating C/C++ and Fortran code

\item {} 
useful linear algebra, Fourier transform, and random number capabilities

\end{itemize}

Besides its obvious scientific uses, NumPy can also be used as an efficient multi\sphinxhyphen{}dimensional container of generic data. Arbitrary data\sphinxhyphen{}types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.

\sphinxcode{\sphinxupquote{import numpy as np}}


\bigskip\hrule\bigskip


\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html}{np.arange}: Return evenly spaced values within a given interval.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html}{np.array}: Create an array

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html}{np.convolve}: Returns the discrete, linear convolution of two one\sphinxhyphen{}dimensional sequences.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.cos.html}{np.cos}: Trigonometric cosine element\sphinxhyphen{}wise.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.diag.html}{np.diag}: Extract a diagonal or construct a diagonal array.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.diag\_indices.html}{np.diag\_indices}: Return the indices to access the main diagonal of an array.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html}{np.dot}: Dot product of two arrays.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html}{np.exp}: Calculate the exponential of all elements in the input array.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fft.html}{np.fft.fft}: Compute the one\sphinxhyphen{}dimensional discrete Fourier Transform.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.ifft.html}{np.fft.ifft}: Compute the one\sphinxhyphen{}dimensional inverse discrete Fourier Transform.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html}{np.hstack}: Stack arrays in sequence horizontally (column wise).

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html}{np.linalg.pinv}: Compute the (Moore\sphinxhyphen{}Penrose) pseudo\sphinxhyphen{}inverse of a matrix.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html}{np.mean}: Compute the arithmetic mean along the specified axis.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/constants.html\#numpy.NaN}{np.nan}: IEEE 754 floating point representation of Not a Number (NaN).

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html}{np.ones}: Return a new array of given shape and type, filled with ones.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/constants.html\#numpy.pi}{np.pi}: Return pi 3.1415926535897932384626433…

\sphinxhref{https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randint.html}{np.random.randint}: Return random integers from low (inclusive) to high (exclusive).

\sphinxhref{https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html}{np.random.randn}: Return a sample (or samples) from the “standard normal” distribution.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.real.html}{np.real}: Return the real part of the complex argument.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.sin.html}{np.sin}: Trigonometric sine, element\sphinxhyphen{}wise.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html}{np.sqrt}: Return the non\sphinxhyphen{}negative square\sphinxhyphen{}root of an array, element\sphinxhyphen{}wise.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.squeeze.html}{np.squeeze}: Remove single\sphinxhyphen{}dimensional entries from the shape of an array.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html\#numpy.std}{np.std}: Compute the standard deviation along the specified axis.

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html\#numpy.vstack}{np.vstack}: Stack arrays in sequence vertically (row wise).

\sphinxhref{https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html}{np.zeros}: Return a new array of given shape and type, filled with zeros.


\subsection{SciPy}
\label{\detokenize{content/Glossary:scipy}}
\sphinxhref{https://www.scipy.org/scipylib/index.html}{SciPy} is one of the core packages that make up the SciPy stack. It provides many user\sphinxhyphen{}friendly and efficient numerical routines, such as routines for numerical integration, interpolation, optimization, linear algebra, and statistics.


\bigskip\hrule\bigskip


\sphinxhref{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html}{scipy.stats.binom}: A binomial discrete random variable.

\sphinxhref{https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html}{scipy.signal.butter}: Butterworth digital and analog filter design.

\sphinxhref{https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.filtfilt.html}{scipy.signal.filtfilt}: Apply a digital filter forward and backward to a signal.

\sphinxhref{https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.freqz.html}{scipy.signal.freqz}: Compute the frequency response of a digital filter.

\sphinxhref{https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.sosfreqz.html}{scipy.signal.sosfreqz}: Compute the frequency response of a digital filter in SOS format.

\sphinxhref{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest\_1samp.html}{scipy.stats.ttest\_1samp}: Calculate the T\sphinxhyphen{}test for the mean of ONE group of scores.


\subsection{Matplotlib}
\label{\detokenize{content/Glossary:matplotlib}}
\sphinxhref{https://matplotlib.org/}{Matplotlib} is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shells, the Jupyter notebook, web application servers, and four graphical user interface toolkits.

\sphinxcode{\sphinxupquote{import matplotlib.pyplot as plt}}


\bigskip\hrule\bigskip


\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.pyplot.bar.html}{plt.bar}: Make a bar plot.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.pyplot.figure.html}{plt.figure}: Create a new figure.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.pyplot.hist.html}{plt.hist}: Plot a histogram.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.pyplot.imshow.html}{plt.imshow}: Display an image, i.e. data on a 2D regular raster.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.pyplot.legend.html}{plt.legend}: Place a legend on the axes.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.pyplot.savefig.html}{plt.savefig}: Save the current figure.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.pyplot.scatter.html}{plt.scatter}: A scatter plot of y vs x with varying marker size and/or color.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.pyplot.subplots.html}{plt.subplots}: Create a figure and a set of subplots.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.pyplot.tight\_layout.html}{plt.tight\_layout}: Automatically adjust subplot parameters to give specified padding.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.axes.Axes.axvline.html}{ax.axvline}: Add a vertical line across the axes.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.axes.Axes.set\_xlabel.html}{ax.set\_xlabel}: Set the label for the x\sphinxhyphen{}axis.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.axes.Axes.set\_xlim.html}{ax.set\_xlim}: Set the x\sphinxhyphen{}axis view limits.

\sphinxhref{https://matplotlib.org/api/\_as\_gen/matplotlib.axes.Axes.set\_xticklabels.html}{ax.set\_xticklabels}: Set the x\sphinxhyphen{}tick labels with list of string labels.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.axes.Axes.set\_ylim.html}{ax.set\_ylim}: Set the y\sphinxhyphen{}axis view limits.

\sphinxhref{https://matplotlib.org/api/\_as\_gen/matplotlib.axes.Axes.set\_yticklabels.html}{ax.set\_yticklabels}: Set the y\sphinxhyphen{}tick labels with list of string labels.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.axes.Axes.set\_ylabel.html}{ax.set\_ylabel}: Set the label for the y\sphinxhyphen{}axis.

\sphinxhref{https://matplotlib.org/3.1.1/api/\_as\_gen/matplotlib.axes.Axes.set\_title.html}{ax.set\_title}: Set a title for the axes.


\subsection{Seaborn}
\label{\detokenize{content/Glossary:seaborn}}
\sphinxhref{https://seaborn.pydata.org/}{Seaborn} is a Python data visualization library based on matplotlib. It provides a high\sphinxhyphen{}level interface for drawing attractive and informative statistical graphics.

\sphinxcode{\sphinxupquote{import seaborn as sns}}


\bigskip\hrule\bigskip


\sphinxhref{https://seaborn.pydata.org/generated/seaborn.heatmap.html}{sns.heatmap}: Plot rectangular data as a color\sphinxhyphen{}encoded matrix.

\sphinxhref{https://seaborn.pydata.org/generated/seaborn.catplot.html}{sns.catplot}: Figure\sphinxhyphen{}level interface for drawing categorical plots onto a FacetGrid.

\sphinxhref{https://seaborn.pydata.org/generated/seaborn.jointplot.html}{sns.jointplot}: Draw a plot of two variables with bivariate and univariate graphs.

\sphinxhref{https://seaborn.pydata.org/generated/seaborn.regplot.html}{sns.regplot}: Plot data and a linear regression model fit.


\subsection{scikit\sphinxhyphen{}learn}
\label{\detokenize{content/Glossary:scikit-learn}}
\sphinxhref{https://scikit-learn.org/}{Scikit\sphinxhyphen{}learn} is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.


\bigskip\hrule\bigskip


\sphinxhref{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise\_distances.html}{sklearn.metrics.pairwise\_distances}: This method takes either a vector array or a distance matrix, and returns a distance matrix. If the input is a vector array, the distances are computed. If the input is a distances matrix, it is returned instead.

\sphinxhref{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced\_accuracy\_score.html}{sklearn.metrics.balanced\_accuracy\_score}: Compute the balanced accuracy. The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.


\subsection{networkx}
\label{\detokenize{content/Glossary:networkx}}
\sphinxhref{https://networkx.github.io/}{NetworkX} is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.

\sphinxcode{\sphinxupquote{import networkx as nx}}


\bigskip\hrule\bigskip


\sphinxhref{https://networkx.github.io/documentation/stable/reference/generated/networkx.drawing.nx\_pylab.draw\_kamada\_kawai.html}{nx.draw\_kamada\_kawai}: Draw the graph G with a Kamada\sphinxhyphen{}Kawai force\sphinxhyphen{}directed layout.

\sphinxhref{https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.DiGraph.degree.html}{nx.degree}: Return the degree of a node or nodes. The node degree is the number of edges adjacent to that node.


\subsection{NiBabel}
\label{\detokenize{content/Glossary:nibabel}}
\sphinxhref{https://nipy.org/nibabel/}{nibabel} is a package to help Read / write access to some common neuroimaging file formats, including: ANALYZE (plain, SPM99, SPM2 and later), GIFTI, NIfTI1, NIfTI2, CIFTI\sphinxhyphen{}2, MINC1, MINC2, AFNI BRIK/HEAD, MGH and ECAT as well as Philips PAR/REC. We can read and write FreeSurfer geometry, annotation and morphometry files. There is some very limited support for DICOM. NiBabel is the successor of PyNIfTI.

The various image format classes give full or selective access to header (meta) information and access to the image data is made available via NumPy arrays.

\sphinxcode{\sphinxupquote{import nibabel as nib}}


\bigskip\hrule\bigskip


\sphinxhref{https://nipy.org/nibabel/reference/nibabel.loadsave.html\#module-nibabel.loadsave}{nib.load}: Load file given filename, guessing at file type

\sphinxhref{https://nipy.org/nibabel/reference/nibabel.loadsave.html\#nibabel.loadsave.save}{nib.save}: Save an image to file adapting format to filename

\sphinxhref{https://nipy.org/nibabel/reference/nibabel.dataobj\_images.html}{data.get\_data}: Return image data from image with any necessary scaling applied

\sphinxhref{https://nipy.org/nibabel/reference/nibabel.dataobj\_images.html}{data.get\_shape}: Return shape for image

\sphinxhref{https://nipy.org/nibabel/nibabel\_images.html}{data.header}: The header of an image contains the image metadata. The information in the header will differ between different image formats. For example, the header information for a NIfTI1 format file differs from the header information for a MINC format file.

\sphinxhref{https://nipy.org/nibabel/reference/nibabel.nifti1.html\#nibabel.nifti1.Nifti1Image}{data.affine}: homogenous affine giving relationship between voxel coordinates and world coordinates. Affine can also be None. In this case, obj.affine also returns None, and the affine as written to disk will depend on the file format.


\subsection{NiLearn}
\label{\detokenize{content/Glossary:nilearn}}
\sphinxhref{https://nilearn.github.io/}{nilearn} is a Python module for fast and easy statistical learning on NeuroImaging data.

It leverages the scikit\sphinxhyphen{}learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.


\bigskip\hrule\bigskip


\sphinxhref{https://nilearn.github.io/modules/generated/nilearn.plotting.plot\_anat.html}{nilearn.plotting.plot\_anat}: Plot cuts of an anatomical image (by default 3 cuts: Frontal, Axial, and Lateral)

\sphinxhref{https://nilearn.github.io/modules/generated/nilearn.plotting.view\_img.html}{nilearn.plotting.view\_img}:Interactive html viewer of a statistical map, with optional background

\sphinxhref{https://nilearn.github.io/modules/generated/nilearn.plotting.plot\_glass\_brain.html}{nilearn.plotting.plot\_glass\_brain}:  Plot 2d projections of an ROI/mask image (by default 3 projections: Frontal, Axial, and Lateral). The brain glass schematics are added on top of the image.

\sphinxhref{https://nilearn.github.io/modules/generated/nilearn.plotting.plot\_stat\_map.html}{nilearn.plotting.plot\_stat\_map}: Plot cuts of an ROI/mask image (by default 3 cuts: Frontal, Axial, and Lateral)


\subsection{nltools}
\label{\detokenize{content/Glossary:nltools}}
\sphinxhref{https://neurolearn.readthedocs.io/en/latest/}{NLTools} is a Python package for analyzing neuroimaging data. It is the analysis engine powering neuro\sphinxhyphen{}learn There are tools to perform data manipulation and analyses such as univariate GLMs, predictive multivariate modeling, and representational similarity analyses.


\bigskip\hrule\bigskip



\subsubsection{Data Classes}
\label{\detokenize{content/Glossary:data-classes}}

\paragraph{Adjacency}
\label{\detokenize{content/Glossary:adjacency}}
\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Adjacency}{Adjacency} is a class to represent Adjacency matrices as a vector rather than a 2\sphinxhyphen{}dimensional matrix. This makes it easier to perform data manipulation and analyses. This tool is particularly useful for performing Representational Similarity Analyses.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Adjacency.distance}{Adjacency.distance}: Calculate distance between images within an Adjacency() instance.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Adjacency.distance\_to\_similarity}{Adjacency.distance\_to\_similarity}: Convert distance matrix to similarity matrix

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Adjacency.plot}{Adjacency.plot}: Create Heatmap of Adjacency Matrix

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Adjacency.plot\_mds}{Adjacency.plot\_mds}: Plot Multidimensional Scaling

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Adjacency.to\_graph}{Adjacency.to\_graph}: Convert Adjacency into networkx graph. only works on single\_matrix for now.


\subsubsection{Brain\_Data}
\label{\detokenize{content/Glossary:brain-data}}
\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools-data-data-types}{Brain\_Data} is a class to represent neuroimaging data in python as a vector rather than a 3\sphinxhyphen{}dimensional matrix.This makes it easier to perform data manipulation and analyses. This is the main tool for working with neuroimaging data.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.append}{Brain\_Data.append}: Append data to Brain\_Data instance

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.apply\_mask}{apply\_mask}: Mask Brain\_Data instance

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.copy}{Brain\_Data.copy}: Create a copy of a Brain\_Data instance.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.decompose}{Brain\_Data.decompose}: Decompose Brain\_Data object

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.distance}{Brain\_Data.distance}: Calculate distance between images within a Brain\_Data() instance.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.extract\_roi}{Brain\_Data.extract\_roi}: Extract activity from mask

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.find\_spikes}{Brain\_Data.find\_spikes}: Function to identify spikes from Time Series Data

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.iplot}{Brain\_Data.iplot}: Create an interactive brain viewer for the current brain data instance.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.mean}{Brain\_Data.mean}: Get mean of each voxel across images.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.plot}{Brain\_Data.plot}: Create a quick plot of self.data. Will plot each image separately

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.predict}{Brain\_Data.predict}: Run prediction

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.regress}{Brain\_Data.regress}: Run a mass\sphinxhyphen{}univariate regression across voxels. Three types of regressions can be run: 1) Standard OLS (default) 2) Robust OLS (heteroscedasticty and/or auto\sphinxhyphen{}correlation robust errors), i.e. OLS with “sandwich estimators” 3) ARMA (auto\sphinxhyphen{}regressive and moving\sphinxhyphen{}average lags = 1 by default; experimental)

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.shape}{Brain\_Data.shape}: Get images by voxels shape.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.similaritye}{Brain\_Data.similarity}: Calculate similarity of Brain\_Data() instance with single Brain\_Data or Nibabel image

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.smooth}{Brain\_Data.smooth}: Apply spatial smoothing using nilearn smooth\_img()

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.std}{Brain\_Data.std}: Get standard deviation of each voxel across images.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.threshold}{Brain\_Data.threshold}: Threshold Brain\_Data instance.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.to\_nifti}{Brain\_Data.to\_nifti}: Convert Brain\_Data Instance into Nifti Object

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.ttest}{Brain\_Data.ttest}: Calculate one sample t\sphinxhyphen{}test across each voxel (two\sphinxhyphen{}sided)

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Brain\_Data.write}{Brain\_Data.write}: Write out Brain\_Data object to Nifti or HDF5 File.


\subsubsection{Design\_Matrix}
\label{\detokenize{content/Glossary:design-matrix}}
\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Design\_Matrix}{Design\_Matrix} is a class to represent design matrices with special methods for data processing (e.g. convolution, upsampling, downsampling) and also intelligent and flexible and intelligent appending (e.g. auto\sphinxhyphen{}matically keep certain columns or polynomial terms separated during concatentation). It plays nicely with Brain\_Data and can be used to build an experimental design to pass to Brain\_Data’s X attribute. It is essentially an enhanced pandas df, with extra attributes and methods. Methods always return a new design matrix instance (copy). Column names are always string types. Inherits most methods on pandas DataFrames.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Design\_Matrix.add\_dct\_basis}{Design\_Matrix.add\_dct\_basis}: Adds unit scaled cosine basis functions to Design\_Matrix columns, based on spm\sphinxhyphen{}style discrete cosine transform for use in high\sphinxhyphen{}pass filtering. Does not add intercept/constant. Care is recommended if using this along with .add\_poly(), as some columns will be highly\sphinxhyphen{}correlated.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Design\_Matrix.add\_poly}{Design\_Matrix.add\_poly}: Add nth order Legendre polynomial terms as columns to design matrix. Good for adding constant/intercept to model (order = 0) and accounting for slow\sphinxhyphen{}frequency nuisance artifacts e.g. linear, quadratic, etc drifts. Care is recommended when using this with .add\_dct\_basis() as some columns will be highly correlated.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Design\_Matrix.clean}{Design\_Matrix.clean}: Method to fill NaNs in Design Matrix and remove duplicate columns based on data values, NOT names. Columns are dropped if they are correlated \textgreater{}= the requested threshold (default = .95). In this case, only the first instance of that column will be retained and all others will be dropped.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Design\_Matrix.convolve}{Design\_Matrix.convolve}: Perform convolution using an arbitrary function.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Design\_Matrix.heatmap}{Design\_Matrix.heatmap}: Visualize Design Matrix spm style. Use .plot() for typical pandas plotting functionality. Can pass optional keyword args to seaborn heatmap.

\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html}{Design\_Matrix.head}: This function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it.

\sphinxhref{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html}{Design\_Matrix.info}: Print a concise summary of a DataFrame.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Design\_Matrix.vif}{Design\_Matrix.vif}: Compute variance inflation factor amongst columns of design matrix, ignoring polynomial terms. Much faster that statsmodel and more reliable too. Uses the same method as Matlab and R (diagonal elements of the inverted correlation matrix).

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.data.Design\_Matrix.zscore}{Design\_Matrix.zscore}: nltools.stats.downsample, but ensures that returned object is a design matrix.


\subsubsection{Statistics Functions}
\label{\detokenize{content/Glossary:statistics-functions}}
\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.stats.fdr}{stats.fdr}: Determine FDR threshold given a p value array and desired false discovery rate q.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.stats.find\_spikes}{stats.find\_spikes}: Function to identify spikes from fMRI Time Series Data

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.stats.fisher\_r\_to\_z}{stats.fisher\_r\_to\_z}: Use Fisher transformation to convert correlation to z score

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.stats.one\_sample\_permutation}{stats.one\_sample\_permutation}: One sample permutation test using randomization.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.stats.threshold}{stats.threshold}: Threshold test image by p\sphinxhyphen{}value from p image

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.stats.regress}{stats.regress}: This is a flexible function to run several types of regression models provided X and Y numpy arrays. Y can be a 1d numpy array or 2d numpy array. In the latter case, results will be output with shape 1 x Y.shape{[}1{]}, in other words fitting a separate regression model to each column of Y.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.stats.zscore}{stats.zscore}: zscore every column in a pandas dataframe or series.


\subsubsection{Miscellaneous Functions}
\label{\detokenize{content/Glossary:miscellaneous-functions}}
\sphinxhref{https://github.com/cosanlab/nltools/blob/master/nltools/simulator.py}{SimulateGrid}: A class to simulate signal and noise within 2D grid. Need to update link to nltools documentation once it is built.

\sphinxhref{https://nistats.github.io/modules/generated/nistats.hemodynamic\_models.glover\_hrf.html}{external.hrf.glover\_hrf}: Implementation of the Glover hemodynamic response function (HRF) model.

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.datasets.fetch\_pain}{datasets.fetch\_pain}: Download and loads pain dataset from neurovault

\sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html\#nltools.datasets.fetch\_localizer}{datasets.fetch\_localizer}: Download and load Brainomics Localizer dataset (94 subjects).


\section{Introduction to Neuroimaging}
\label{\detokenize{content/Intro_to_Neuroimaging:introduction-to-neuroimaging}}\label{\detokenize{content/Intro_to_Neuroimaging::doc}}
Measuring in\sphinxhyphen{}vivo brain activity from humans is an extraordinary feat. What is Neuroimaging? and What can we learn about the brain using this technique?

In this section, we will provide an overview of the course and a very introductory overview of the different types neuroimaging and a few examples of different types of things we can do with BOLD fMRI.


\subsection{Lecture}
\label{\detokenize{content/Intro_to_Neuroimaging:lecture}}
The lecture for this section can be viewed \sphinxcode{\sphinxupquote{here}}.


\section{Measurement and Signal}
\label{\detokenize{content/Signal_Measurement:measurement-and-signal}}\label{\detokenize{content/Signal_Measurement::doc}}
Measuring in\sphinxhyphen{}vivo brain activity from humans is an extraordinary feat. How do scanners work? and what exactly are we measuring?

In this section, we will provide a very introductory overview of the basics of MR Physics and the physiology underlying the signal we are measuring with BOLD fMRI.


\subsection{Lecture}
\label{\detokenize{content/Signal_Measurement:lecture}}
The lecture for this section can be viewed \sphinxcode{\sphinxupquote{here}}.


\subsection{Readings}
\label{\detokenize{content/Signal_Measurement:readings}}
Please read Chapter 7 of the \sphinxhref{http://sites.sinauer.com/fmri3e/sq07.html}{Huettel, Song, \& McCarthy textbook} on the BOLD signal


\subsection{Videos}
\label{\detokenize{content/Signal_Measurement:videos}}
There are many fantastic lectures on this topic. We recommend watching the short videos by Tor Wager and Martin Lindquist from their Principles of fMRI Coursera class.
\begin{itemize}
\item {} 
\sphinxhref{https://www.youtube.com/watch?v=XsDXxgjEJVY\&list=PLfXA4opIOVrGHncHRxI3Qa5GeCSudwmxM\&index=7\&t=0s}{Basic MR Physics}

\item {} 
\sphinxhref{https://www.youtube.com/watch?v=PxqDjhO9FUs\&list=PLfXA4opIOVrGHncHRxI3Qa5GeCSudwmxM\&index=8\&t=0s}{Image Formation}

\item {} 
\sphinxhref{https://www.youtube.com/watch?v=FI5frNsRTI4\&list=PLfXA4opIOVrGHncHRxI3Qa5GeCSudwmxM\&index=9\&t=436s}{K\sphinxhyphen{}Space}

\item {} 
\sphinxhref{https://www.youtube.com/watch?v=jG2WQpgpnMs\&list=PLfXA4opIOVrGHncHRxI3Qa5GeCSudwmxM\&index=10\&t=0s}{Signal \& BOLD Physiology}

\end{itemize}


\section{Separating Signal From Noise With ICA}
\label{\detokenize{content/ICA:separating-signal-from-noise-with-ica}}\label{\detokenize{content/ICA::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

In this tutorial we will use ICA to explore which signals in our imaging data might be real signal or artifacts.

For a brief overview of types of artifacts that might be present in your data, I recommend watching this video by Tor Wager and Martin Lindquist.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{7Kk\PYGZus{}RsGycHs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{ICA_1_0}.jpg}


\subsection{Preprocessing Data}
\label{\detokenize{content/ICA:preprocessing-data}}
To run this tutorial, you must have run preprocessing on at least one participant. \sphinxstyleemphasis{If you are in Psych60, this has already been done for you and you can just skip to \sphinxstylestrong{Loading Data}}. If you reading this online, then I recommend preprocessing your data with \sphinxhref{https://fmriprep.readthedocs.io/en/stable/}{fmriprep}, which is a robust, but opinionated automated preprocessing pipeline developed by \sphinxhref{https://poldracklab.stanford.edu/}{Russ Poldrack’s group at Stanford University}. The developer’s have made a number of choices about how to preprocess your fMRI data using best practices and have created an automated pipeline using multiple software packages that are all distributed via a \sphinxhref{https://fmriprep.readthedocs.io/en/stable/docker.html}{docker container}.

In theory, this is extraodinarily straightforward to run:
\begin{itemize}
\item {} \begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Install \sphinxhref{https://www.docker.com/}{Docker} and download image

\end{enumerate}

\sphinxcode{\sphinxupquote{docker pull poldracklab/fmriprep:\textless{}latest\sphinxhyphen{}version\textgreater{}}}

\item {} \begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Run a single command in the terminal specifying the location of the data, the location of the output, the participant id, and a few specific flags depending on specific details of how you want to run the preprocessing.

\end{enumerate}

\sphinxcode{\sphinxupquote{fmriprep\sphinxhyphen{}docker /Users/lukechang/Dropbox/Dartbrains/Data/localizer /Users/lukechang/Dropbox/Dartbrains/Data/preproc participant \sphinxhyphen{}\sphinxhyphen{}participant\_label sub\sphinxhyphen{}S01 \sphinxhyphen{}\sphinxhyphen{}write\sphinxhyphen{}graph \sphinxhyphen{}\sphinxhyphen{}fs\sphinxhyphen{}no\sphinxhyphen{}reconall \sphinxhyphen{}\sphinxhyphen{}notrack \sphinxhyphen{}\sphinxhyphen{}fs\sphinxhyphen{}license\sphinxhyphen{}file \textasciitilde{}/Dropbox/Dartbrains/License/license.txt \sphinxhyphen{}\sphinxhyphen{}work\sphinxhyphen{}dir /Users/lukechang/Dropbox/Dartbrains/Data/work}}

\end{itemize}

In practice, it’s alway a little bit finicky to get everything set up on a particular system. Sometimes you might run into issues with a specific missing file like the \sphinxhref{https://fmriprep.readthedocs.io/en/stable/usage.html\#the-freesurfer-license}{freesurfer license} even if you’re not using it. You might also run into issues with the format of the data that might have some conflicts with the \sphinxhref{https://github.com/bids-standard/bids-validator}{bids\sphinxhyphen{}validator}. In our experience, there is always some frustrations getting this to work, but it’s very nice once it’s done.


\subsection{Loading Data}
\label{\detokenize{content/ICA:loading-data}}
Ok, once you’ve finished preprocessing some of your data with fmriprep, we can load a subject and run an ICA to explore signals that are present. Since we have completed preprocessing, our data should be realigned and also normalized to MNI stereotactic space. We will use the \sphinxhref{https://neurolearn.readthedocs.io/en/latest/}{nltools} package to work with this data in python.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Brain\PYGZus{}Data}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{plotting} \PYG{k+kn}{import} \PYG{n}{component\PYGZus{}viewer}

\PYG{n}{base\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../data/localizer/derivatives/preproc/fmriprep}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{base\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/Users/lukechang/Dropbox/Dartbrains/Data/preproc/fmriprep}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{sub} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{base\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sub}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sub}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZus{}task\PYGZhy{}localizer\PYGZus{}space\PYGZhy{}MNI152NLin2009cAsym\PYGZus{}desc\PYGZhy{}preproc\PYGZus{}bold.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{More Preprocessing}
\label{\detokenize{content/ICA:more-preprocessing}}
Even though, we have technically already run most of the preprocessing there are a couple of more steps that will help make the ICA cleaner.

First, we will run a high pass filter to remove any low frequency scanner drift. We will pick a fairly arbitrary filter size of 0.0078hz (1/128s). We will also run spatial smoothing with a 6mm FWHM gaussian kernel to increase a signal to noise ratio at each voxel. These steps are very easy to run using nltools after the data has been loaded.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{filter}\PYG{p}{(}\PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mf}{2.4}\PYG{p}{,} \PYG{n}{high\PYGZus{}pass}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{128}\PYG{p}{)}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{smooth}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Independent Component Analysis (ICA)}
\label{\detokenize{content/ICA:independent-component-analysis-ica}}
Ok, we are finally ready to run an ICA analysis on our data.

ICA attempts to perform blind source separation by decomposing a multivariate signal into additive subcomponents that are maximally independent.

We will be using the \sphinxcode{\sphinxupquote{decompose()}} method on our \sphinxcode{\sphinxupquote{Brain\_Data}} instance. This runs the \sphinxhref{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.fastica.html}{FastICA} algorithm implemented by scikit\sphinxhyphen{}learn. You can choose whether you want to run spatial ICA by setting \sphinxcode{\sphinxupquote{axis=\textquotesingle{}voxels}} or temporal ICA by setting \sphinxcode{\sphinxupquote{axis=\textquotesingle{}images\textquotesingle{}}}. We also recommend running the whitening flat \sphinxcode{\sphinxupquote{whiten=True}}. By default \sphinxcode{\sphinxupquote{decompose}} will estimate the maximum components that are possible given the data. We recommend using a completely arbitrary heuristic of 20\sphinxhyphen{}30 components.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tr} \PYG{o}{=} \PYG{l+m+mf}{2.4}
\PYG{n}{output} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{decompose}\PYG{p}{(}\PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ica}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{whiten}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Viewing Components}
\label{\detokenize{content/ICA:viewing-components}}
We will use the interactive \sphinxcode{\sphinxupquote{component\_viewer}} from nltools to explore the results of the analysis. This viewer uses ipywidgets to select the \sphinxcode{\sphinxupquote{Component}} to view and also the threshold. You can manually enter a component number to view or scroll up and down.

Components have been standardized, this allows us to threshold the brain in terms of standard deviations. For example, the default threshold of 2.0, means that any voxel that loads on the component greater or less than 2 standard deviations will be overlaid on the standard brain. You can play with different thresholds to be more or less inclusive \sphinxhyphen{} a threshold of 0 will overlay all of the voxels. If you play with any of the numbers, make sure you press tab to update the plot.

The second plot is the time course of the voxels that load on the component. The x\sphinxhyphen{}axis is in TRs, which for this dataset is 2.4 sec.

The third plot is the powerspectrum of the timecourse. There is not a large range of possible values as we can only observe signals at the nyquist frequency, which is half of our sampling frequency of 1/2.4s (approximately 0.21hz) to a lower bound of 0.0078hz based on our high pass filter. There might be systematic oscillatory signals. Remember, that signals that oscillate a faster frequency than the nyquist frequency will be aliased. This includes physiological artifacts such as respiration and cardiac signals.

It is important to note that ICA cannot resolve the sign of the component. So make sure you consider signals that are positive as well as negative.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{component\PYGZus{}viewer}\PYG{p}{(}\PYG{n}{output}\PYG{p}{,} \PYG{n}{tr}\PYG{o}{=}\PYG{l+m+mf}{2.4}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(BoundedIntText(value=0, description=\PYGZsq{}Component\PYGZsq{}, max=29), BoundedFloatText(value=2.0, de…
\end{sphinxVerbatim}

\sphinxincludegraphics{{ica_viewer_demo}.gif}


\subsection{Exercises}
\label{\detokenize{content/ICA:exercises}}
For this tutorial, try to guess which components are signal and which are noise. Also, be sure to label the type of noise you think you might be seeing (e.g., head motion, scanner spikes, cardiac, respiration, etc.) Do this for subjects \sphinxcode{\sphinxupquote{s01}} and \sphinxcode{\sphinxupquote{s02}}.

What features do you think are important to consider when making this judgment?  Does the spatial map provide any useful information? What about the timecourse of the component? Does it map on to the plausible timecourse of the task.What about the power spectrum?


\section{Introduction to Neuroimaging Data}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:introduction-to-neuroimaging-data}}\label{\detokenize{content/Introduction_to_Neuroimaging_Data::doc}}
In this tutorial we will learn the basics of the organization of data folders, and how to load, plot, and manipulate neuroimaging data in Python.

To introduce the basics of fMRI data structures, watch this short video by Martin Lindquist.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OuRdQJMU5ro}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Neuroimaging_Data_1_0}.jpg}


\subsection{Software Packages}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:software-packages}}
There are many different software packages to analyze neuroimaging data. Most of them are open source and free to use (with the exception of \sphinxhref{https://www.brainvoyager.com/}{BrainVoyager}). The most popular ones (\sphinxhref{https://www.fil.ion.ucl.ac.uk/spm/}{SPM}, \sphinxhref{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki}{FSL}, \& \sphinxhref{https://afni.nimh.nih.gov/}{AFNI}) have been around a long time and are where many new methods are developed and distributed. These packages have focused on implementing what they believe are the best statistical methods, ease of use, and computational efficiency. They have very large user bases so many bugs have been identified and fixed over the years. There are also lots of publicly available documentation, listserves, and online tutorials, which makes it very easy to get started using these tools.

There are also many more boutique packages that focus on specific types of preprocessing step and analyses such as spatial normalization with \sphinxhref{http://stnava.github.io/ANTs/}{ANTs}, connectivity analyses with the \sphinxhref{https://web.conn-toolbox.org/}{conn\sphinxhyphen{}toolbox}, representational similarity analyses with the \sphinxhref{https://github.com/rsagroup/rsatoolbox}{rsaToolbox}, and prediction/classification with \sphinxhref{http://www.pymvpa.org/}{pyMVPA}.

Many packages have been developed within proprietary software such as \sphinxhref{https://www.mathworks.com/products/matlab.html}{Matlab} (e.g., SPM, Conn, RSAToolbox, etc). Unfortunately, this requires that your university has site license for Matlab and many individual add\sphinxhyphen{}on toolboxes. If you are not affiliated with a University, you may have to pay for Matlab, which can be fairly expensive. There are free alternatives such as \sphinxhref{https://www.gnu.org/software/octave/}{octave}, but octave does not include many of the add\sphinxhyphen{}on toolboxes offered by matlab that may be required for a specific package. Because of this restrictive licensing, it is difficult to run matlab on cloud computing servers and to use with free online courses such as dartbrains. Other packages have been written in C/C++/C\# and need to be compiled to run on your specific computer and operating system. While these tools are typically highly computationally efficient, it can sometimes be challenging to get them to install and work on specific computers and operating systems.

There has been a growing trend to adopt the open source Python framework in the data science and scientific computing communities, which has lead to an explosion in the number of new packages available for statistics, visualization, machine learning, and web development. \sphinxhref{http://www.pymvpa.org/}{pyMVPA} was an early leader in this trend, and there are many great tools that are being actively developed such as \sphinxhref{https://nilearn.github.io/}{nilearn}, \sphinxhref{https://brainiak.org/}{brainiak}, \sphinxhref{https://github.com/neurosynth/neurosynth}{neurosynth}, \sphinxhref{https://nipype.readthedocs.io/en/latest/}{nipype}, \sphinxhref{https://fmriprep.readthedocs.io/en/stable/}{fmriprep}, and many more. One exciting thing is that these newer developments have built on the expertise of decades of experience with imaging analyses, and leverage changes in high performance computing. There is also a very tight integration with many cutting edge developments in adjacent communities such as machine learning with \sphinxhref{https://scikit-learn.org/stable/}{scikit\sphinxhyphen{}learn}, \sphinxhref{https://www.tensorflow.org/}{tensorflow}, and \sphinxhref{https://pytorch.org/}{pytorch}, which has made new types of analyses much more accessible to the neuroimaging community. There has also been an influx of younger contributors with software development expertise. You might be surprised to know that many of the popular tools being used had core contributors originating from the neuroimaging community (e.g., scikit\sphinxhyphen{}learn, seaborn, and many more).

For this course, I have chosen to focus on tools developed in Python as it is an easy to learn programming language, has excellent tools, works well on distributed computing systems, has great ways to disseminate information (e.g., jupyter notebooks, jupyter\sphinxhyphen{}book, etc), and is free! If you are just getting started, I would spend some time working with \sphinxhref{https://nilearn.github.io/}{NiLearn} and \sphinxhref{https://brainiak.org/}{Brainiak}, which have a lot of functionality, are very well tested, are reasonably computationally efficient, and most importantly have lots of documentation and tutorials to get started.

We will be using many packages throughout the course such as \sphinxhref{https://bids-standard.github.io/pybids/}{PyBids} to navigate neuroimaging datasets, \sphinxhref{https://fmriprep.readthedocs.io/en/stable/}{fmriprep} to perform preprocessing, and \sphinxhref{https://neurolearn.readthedocs.io/en/latest/}{nltools}, which is a package developed in my lab, to do basic data manipulation and analysis. NLtools is built using many other toolboxes such as \sphinxhref{https://nipy.org/nibabel/}{nibabel} and \sphinxhref{https://nilearn.github.io/}{nilearn}, and we will also be using these frequently throughout the course.


\subsection{BIDS: Brain Imaging Dataset Specification}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:bids-brain-imaging-dataset-specification}}
Recently, there has been growing interest to share datasets across labs and even on public repositories such as \sphinxhref{https://openneuro.org/}{openneuro}. In order to make this a successful enterprise, it is necessary to have some standards in how the data are named and organized. Historically, each lab has used their own idiosyncratic conventions, which can make it difficult for outsiders to analyze. In the past few years, there have been heroic efforts by the neuroimaging community to create a standardized file organization and naming practices. This specification is called \sphinxstylestrong{BIDS} for \sphinxhref{http://bids.neuroimaging.io/}{Brain Imaging Dataset Specification}.

As you can imagine, individuals have their own distinct method of organizing their files. Think about how you keep track of your files on your personal laptop (versus your friend). This may be okay in the personal realm, but in science, it’s best if anyone (especially  yourself 6 months from now!) can follow your work and know \sphinxstyleemphasis{which} files mean \sphinxstyleemphasis{what} by their titles.

Here’s an example of non\sphinxhyphen{}Bids versus BIDS dataset found in \sphinxhref{https://www.nature.com/articles/sdata201644}{this paper}:

\sphinxincludegraphics{{file_tree}.jpg}

Here are a few major differences between the two datasets:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
In BIDS, files are in nifti format (not dicoms).

\item {} 
In BIDS, scans are broken up into separate folders by type of scan(functional versus anatomical versus diffusion weighted) for each subject.

\item {} 
In BIDS, JSON files are included that contain descriptive information about the scans (e.g., acquisition parameters)

\end{enumerate}

Not only can using this specification be useful within labs to have a set way of structuring data, but it can also be useful when collaborating across labs, developing and utilizing software, and publishing data.

In addition, because this is a consistent format, it is possible to have a python package to make it easy to query a dataset. We recommend using \sphinxhref{https://github.com/bids-standard/pybids}{pybids}.

The dataset we will be working with has already been converted to the BIDS format (see download localizer tutorial).

You may need to install \DUrole{xref,myst}{pybids} to query the BIDS datasets using following command \sphinxcode{\sphinxupquote{!pip install pybids}}.


\subsubsection{The \sphinxstyleliteralintitle{\sphinxupquote{BIDSLayout}}}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:the-bidslayout}}
\sphinxhref{https://github.com/bids-standard/pybids}{Pybids} is a package to help query and navigate a neuroimaging dataset that is in the BIDs format. At the core of pybids is the \sphinxcode{\sphinxupquote{BIDSLayout}} object. A \sphinxcode{\sphinxupquote{BIDSLayout}} is a lightweight Python class that represents a BIDS project file tree and provides a variety of helpful methods for querying and manipulating BIDS files. While the BIDSLayout initializer has a large number of arguments you can use to control the way files are indexed and accessed, you will most commonly initialize a BIDSLayout by passing in the BIDS dataset root location as a single argument.

Notice we are setting \sphinxcode{\sphinxupquote{derivatives=True}}. This means the layout will also index the derivatives sub folder, which might contain preprocessed data, analyses, or other user generated files.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{bids} \PYG{k+kn}{import} \PYG{n}{BIDSLayout}\PYG{p}{,} \PYG{n}{BIDSValidator}
\PYG{k+kn}{import} \PYG{n+nn}{os}

\PYG{n}{data\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/Users/lukechang/Dropbox/Dartbrains/data/Localizer}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{layout} \PYG{o}{=} \PYG{n}{BIDSLayout}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{n}{derivatives}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{layout}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
BIDS Layout: ...pbox/Dartbrains/data/Localizer | Subjects: 94 | Sessions: 0 | Runs: 0
\end{sphinxVerbatim}

When we initialize a BIDSLayout, all of the files and metadata found under the specified root folder are indexed. This can take a few seconds (or, for very large datasets, a minute or two). Once initialization is complete, we can start querying the BIDSLayout in various ways. The main query method is \sphinxcode{\sphinxupquote{.get()}}. If we call .\sphinxcode{\sphinxupquote{get()}} with no additional arguments, we get back a list of all the BIDS files in our dataset.

Let’s return the first 10 files

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZlt{}BIDSJSONFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/dataset\PYGZus{}description.json\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/derivatives/fmriprep/.DS\PYGZus{}Store\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSJSONFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/derivatives/fmriprep/dataset\PYGZus{}description.json\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/derivatives/fmriprep/logs/CITATION.bib\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/derivatives/fmriprep/logs/CITATION.html\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/derivatives/fmriprep/logs/CITATION.md\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/derivatives/fmriprep/logs/CITATION.tex\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/derivatives/fmriprep/sub\PYGZhy{}S01.html\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/derivatives/fmriprep/sub\PYGZhy{}S01/.DS\PYGZus{}Store\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSJSONFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/derivatives/fmriprep/sub\PYGZhy{}S01/anat/sub\PYGZhy{}S01\PYGZus{}desc\PYGZhy{}brain\PYGZus{}mask.json\PYGZsq{}\PYGZgt{}]
\end{sphinxVerbatim}

As you can see, just a generic \sphinxcode{\sphinxupquote{.get()}} call gives us \sphinxstyleemphasis{all} of the files. We will definitely want to be a bit more specific. We can specify the type of data we would like to query. For example, suppose we want to return the first 10 subject ids.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{target}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subject}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{return\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}S01\PYGZsq{}, \PYGZsq{}S02\PYGZsq{}, \PYGZsq{}S03\PYGZsq{}, \PYGZsq{}S04\PYGZsq{}, \PYGZsq{}S05\PYGZsq{}, \PYGZsq{}S06\PYGZsq{}, \PYGZsq{}S07\PYGZsq{}, \PYGZsq{}S08\PYGZsq{}, \PYGZsq{}S09\PYGZsq{}, \PYGZsq{}S10\PYGZsq{}]
\end{sphinxVerbatim}

Or perhaps, we would like to get the file names for the raw bold functional nifti images for the first 10 subjects. We can filter files in the \sphinxcode{\sphinxupquote{raw}} or \sphinxcode{\sphinxupquote{derivatives}}, using \sphinxcode{\sphinxupquote{scope}} keyword.\sphinxcode{\sphinxupquote{scope=\textquotesingle{}raw\textquotesingle{}}}, to only query raw bold nifti files.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{target}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subject}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{raw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{return\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{file}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S01/func/sub\PYGZhy{}S01\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S02/func/sub\PYGZhy{}S02\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S03/func/sub\PYGZhy{}S03\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S04/func/sub\PYGZhy{}S04\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S05/func/sub\PYGZhy{}S05\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S06/func/sub\PYGZhy{}S06\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S07/func/sub\PYGZhy{}S07\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S08/func/sub\PYGZhy{}S08\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S09/func/sub\PYGZhy{}S09\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{},
 \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S10/func/sub\PYGZhy{}S10\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}]
\end{sphinxVerbatim}

When you call .get() on a BIDSLayout, the default returned values are objects of class BIDSFile. A BIDSFile is a lightweight container for individual files in a BIDS dataset.

Here are some of the attributes and methods available to us in a BIDSFile (note that some of these are only available for certain subclasses of BIDSFile; e.g., you can’t call get\_image() on a BIDSFile that doesn’t correspond to an image file!):
\begin{itemize}
\item {} 
.path: The full path of the associated file

\item {} 
.filename: The associated file’s filename (without directory)

\item {} 
.dirname: The directory containing the file

\item {} 
.get\_entities(): Returns information about entities associated with this BIDSFile (optionally including metadata)

\item {} 
.get\_image(): Returns the file contents as a nibabel image (only works for image files)

\item {} 
.get\_df(): Get file contents as a pandas DataFrame (only works for TSV files)

\item {} 
.get\_metadata(): Returns a dictionary of all metadata found in associated JSON files

\item {} 
.get\_associations(): Returns a list of all files associated with this one in some way

\end{itemize}

Let’s explore the first file in a little more detail.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f} \PYG{o}{=} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{f}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}BIDSJSONFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/dataset\PYGZus{}description.json\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

If we wanted to get the path of the file, we can use \sphinxcode{\sphinxupquote{.path}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f}\PYG{o}{.}\PYG{n}{path}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/dataset\PYGZus{}description.json\PYGZsq{}
\end{sphinxVerbatim}

Suppose we were interested in getting a list of tasks included in the dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get\PYGZus{}task}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}localizer\PYGZsq{}]
\end{sphinxVerbatim}

We can query all of the files associated with this task.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{localizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{raw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZlt{}BIDSImageFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S01/func/sub\PYGZhy{}S01\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSImageFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S02/func/sub\PYGZhy{}S02\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSImageFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S03/func/sub\PYGZhy{}S03\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSImageFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S04/func/sub\PYGZhy{}S04\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSImageFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S05/func/sub\PYGZhy{}S05\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSImageFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S06/func/sub\PYGZhy{}S06\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSImageFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S07/func/sub\PYGZhy{}S07\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSImageFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S08/func/sub\PYGZhy{}S08\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSImageFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S09/func/sub\PYGZhy{}S09\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}\PYGZgt{},
 \PYGZlt{}BIDSImageFile filename=\PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data/localizer/sub\PYGZhy{}S10/func/sub\PYGZhy{}S10\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}\PYGZgt{}]
\end{sphinxVerbatim}

Notice that there are nifti and event files. We can get the filename for the first particant’s functional run

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f} \PYG{o}{=} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{localizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{filename}
\PYG{n}{f}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}sub\PYGZhy{}S01\PYGZus{}task\PYGZhy{}localizer\PYGZus{}desc\PYGZhy{}carpetplot\PYGZus{}bold.svg\PYGZsq{}
\end{sphinxVerbatim}

If you want a summary of all the files in your BIDSLayout, but don’t want to have to iterate BIDSFile objects and extract their entities, you can get a nice bird’s\sphinxhyphen{}eye view of your dataset using the \sphinxcode{\sphinxupquote{to\_df()}} method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{layout}\PYG{o}{.}\PYG{n}{to\PYGZus{}df}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
entity                                               path datatype extension  \PYGZbs{}
0       /Users/lukechang/Dropbox/Dartbrains/Data/local...      NaN      json   
1       /Users/lukechang/Dropbox/Dartbrains/Data/local...      NaN      json   
2       /Users/lukechang/Dropbox/Dartbrains/Data/local...      NaN       tsv   
3       /Users/lukechang/Dropbox/Dartbrains/Data/local...      NaN       tsv   
4       /Users/lukechang/Dropbox/Dartbrains/Data/local...      NaN       tsv   
..                                                    ...      ...       ...   
284     /Users/lukechang/Dropbox/Dartbrains/Data/local...     anat    nii.gz   
285     /Users/lukechang/Dropbox/Dartbrains/Data/local...     func    nii.gz   
286     /Users/lukechang/Dropbox/Dartbrains/Data/local...     func       tsv   
287     /Users/lukechang/Dropbox/Dartbrains/Data/local...      NaN      json   
288     /Users/lukechang/Dropbox/Dartbrains/Data/local...      NaN       NaN   

entity subject        suffix       task  
0          NaN   description        NaN  
1          NaN  participants        NaN  
2          NaN  participants        NaN  
3          NaN   behavioural        NaN  
4          NaN       subject        NaN  
..         ...           ...        ...  
284        S94           T1w        NaN  
285        S94          bold  localizer  
286        S94        events  localizer  
287        NaN          bold  localizer  
288        NaN           NaN        NaN  

[289 rows x 6 columns]
\end{sphinxVerbatim}


\subsection{Loading Data with Nibabel}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:loading-data-with-nibabel}}
Neuroimaging data is often stored in the format of nifti files \sphinxcode{\sphinxupquote{.nii}} which can also be compressed using gzip \sphinxcode{\sphinxupquote{.nii.gz}}.  These files store both 3D and 4D data and also contain structured metadata in the image \sphinxstylestrong{header}.

There is an very nice tool to access nifti data stored on your file system in python called \sphinxhref{http://nipy.org/nibabel/}{nibabel}.  If you don’t already have nibabel installed on your computer it is easy via \sphinxcode{\sphinxupquote{pip}}. First, tell the jupyter cell that you would like to access the unix system outside of the notebook and then install nibabel using pip \sphinxcode{\sphinxupquote{!pip install nibabel}}. You only need to run this once (unless you would like to update the version).

nibabel objects can be initialized by simply pointing to a nifti file even if it is compressed through gzip.  First, we will import the nibabel module as \sphinxcode{\sphinxupquote{nib}} (short and sweet so that we don’t have to type so much when using the tool).  I’m also including a path to where the data file is located so that I don’t have to constantly type this.  It is easy to change this on your own computer.

We will be loading an anatomical image from subject S01 from the localizer \sphinxhref{http://brainomics.cea.fr/localizer/}{dataset}.  See this \sphinxhref{https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-8-91}{paper} for more information about this dataset.

We will use pybids to grab subject S01’s T1 image.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{nibabel} \PYG{k}{as} \PYG{n+nn}{nib}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{nib}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T1w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{return\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{file}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

If we want to get more help on how to work with the nibabel data object we can either consult the \sphinxhref{https://nipy.org/nibabel/tutorials.html\#tutorials}{documentation} or add a \sphinxcode{\sphinxupquote{?}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
data\PYG{o}{?}
\end{sphinxVerbatim}

The imaging data is stored in either a 3D or 4D numpy array. Just like numpy, it is easy to get the dimensions of the data using \sphinxcode{\sphinxupquote{shape}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(193, 229, 193)
\end{sphinxVerbatim}

Looks like there are 3 dimensions (x,y,z) that is the number of voxels in each dimension. If we know the voxel size, we could convert this into millimeters.

We can also directly access the data and plot a single slice using standard matplotlib functions.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{get\PYGZus{}fdata}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.image.AxesImage at 0x7fadc1706e50\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Neuroimaging_Data_31_1}.png}

Try slicing different dimensions (x,y,z) yourself to get a feel for how the data is represented in this anatomical image.

We can also access data from the image header. Let’s assign the header of an image to a variable and print it to view it’s contents.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{header} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{header}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{header}\PYG{p}{)}      
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}nibabel.nifti1.Nifti1Header\PYGZsq{}\PYGZgt{} object, endian=\PYGZsq{}\PYGZlt{}\PYGZsq{}
sizeof\PYGZus{}hdr      : 348
data\PYGZus{}type       : b\PYGZsq{}\PYGZsq{}
db\PYGZus{}name         : b\PYGZsq{}\PYGZsq{}
extents         : 0
session\PYGZus{}error   : 0
regular         : b\PYGZsq{}r\PYGZsq{}
dim\PYGZus{}info        : 0
dim             : [  3 193 229 193   1   1   1   1]
intent\PYGZus{}p1       : 0.0
intent\PYGZus{}p2       : 0.0
intent\PYGZus{}p3       : 0.0
intent\PYGZus{}code     : none
datatype        : float32
bitpix          : 32
slice\PYGZus{}start     : 0
pixdim          : [1. 1. 1. 1. 0. 0. 0. 0.]
vox\PYGZus{}offset      : 0.0
scl\PYGZus{}slope       : nan
scl\PYGZus{}inter       : nan
slice\PYGZus{}end       : 0
slice\PYGZus{}code      : unknown
xyzt\PYGZus{}units      : 2
cal\PYGZus{}max         : 0.0
cal\PYGZus{}min         : 0.0
slice\PYGZus{}duration  : 0.0
toffset         : 0.0
glmax           : 0
glmin           : 0
descrip         : b\PYGZsq{}xform matrices modified by FixHeaderApplyTransforms (niworkflows v1.1.12).\PYGZsq{}
aux\PYGZus{}file        : b\PYGZsq{}\PYGZsq{}
qform\PYGZus{}code      : mni
sform\PYGZus{}code      : mni
quatern\PYGZus{}b       : 0.0
quatern\PYGZus{}c       : 0.0
quatern\PYGZus{}d       : 0.0
qoffset\PYGZus{}x       : \PYGZhy{}96.0
qoffset\PYGZus{}y       : \PYGZhy{}132.0
qoffset\PYGZus{}z       : \PYGZhy{}78.0
srow\PYGZus{}x          : [  1.   0.   0. \PYGZhy{}96.]
srow\PYGZus{}y          : [   0.    1.    0. \PYGZhy{}132.]
srow\PYGZus{}z          : [  0.   0.   1. \PYGZhy{}78.]
intent\PYGZus{}name     : b\PYGZsq{}\PYGZsq{}
magic           : b\PYGZsq{}n+1\PYGZsq{}
\end{sphinxVerbatim}

Some of the important information in the header is information about the orientation of the image in space. This can be represented as the affine matrix, which can be used to transform images between different spaces.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{affine}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[   1.,    0.,    0.,  \PYGZhy{}96.],
       [   0.,    1.,    0., \PYGZhy{}132.],
       [   0.,    0.,    1.,  \PYGZhy{}78.],
       [   0.,    0.,    0.,    1.]])
\end{sphinxVerbatim}

We will dive deeper into affine transformations in the preprocessing tutorial.


\subsection{Plotting Data with Nilearn}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:plotting-data-with-nilearn}}
There are many useful tools from the \sphinxhref{https://nilearn.github.io/index.html}{nilearn} library to help manipulate and visualize neuroimaging data. See their \sphinxhref{https://nilearn.github.io/plotting/index.html\#different-plotting-functions}{documentation} for an example.

In this section, we will explore a few of their different plotting functions, which can work directly with nibabel instances.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{from} \PYG{n+nn}{nilearn}\PYG{n+nn}{.}\PYG{n+nn}{plotting} \PYG{k+kn}{import} \PYG{n}{view\PYGZus{}img}\PYG{p}{,} \PYG{n}{plot\PYGZus{}glass\PYGZus{}brain}\PYG{p}{,} \PYG{n}{plot\PYGZus{}anat}\PYG{p}{,} \PYG{n}{plot\PYGZus{}epi}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plot\PYGZus{}anat}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.displays.OrthoSlicer at 0x7fadd13c4c50\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Neuroimaging_Data_41_1}.png}

Nilearn plotting functions are very flexible and allow us to easily customize our plots

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plot\PYGZus{}anat}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{draw\PYGZus{}cross}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{display\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.displays.ZSlicer at 0x7fadc157fd50\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Neuroimaging_Data_43_1}.png}

try to get more information how to use the function with \sphinxcode{\sphinxupquote{?}} and try to add different commands to change the plot.

nilearn also has a neat interactive viewer called \sphinxcode{\sphinxupquote{view\_img}} for examining images directly in the notebook.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{view\PYGZus{}img}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.html\PYGZus{}stat\PYGZus{}map.StatMapView at 0x7f92000a1bd0\PYGZgt{}
\end{sphinxVerbatim}

The \sphinxcode{\sphinxupquote{view\_img}} function is particularly useful for overlaying statistical maps over an anatomical image so that we can interactively examine where the results are located.

As an example, let’s load a mask of the amygdala and try to find where it is located.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{amygdala\PYGZus{}mask} \PYG{o}{=} \PYG{n}{nib}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../..}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{masks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FSL\PYGZus{}BAmyg\PYGZus{}thr0.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{view\PYGZus{}img}\PYG{p}{(}\PYG{n}{amygdala\PYGZus{}mask}\PYG{p}{,} \PYG{n}{data}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/nilearn/image/resampling.py:513: UserWarning: Casting data from int32 to float32
  warnings.warn(\PYGZdq{}Casting data from \PYGZpc{}s to \PYGZpc{}s\PYGZdq{} \PYGZpc{} (data.dtype.name, aux))
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.html\PYGZus{}stat\PYGZus{}map.StatMapView at 0x7f9260d0f150\PYGZgt{}
\end{sphinxVerbatim}

We can also plot a glass brain which allows us to see through the brain from different slice orientations. In this example, we will plot the binary amygdala mask.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plot\PYGZus{}glass\PYGZus{}brain}\PYG{p}{(}\PYG{n}{amygdala\PYGZus{}mask}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.displays.OrthoProjector at 0x7f92415be7d0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Neuroimaging_Data_49_1}.png}


\subsection{Manipulating Data with Nltools}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:manipulating-data-with-nltools}}
Ok, we’ve now learned how to use nibabel to load imaging data and nilearn to plot it.

Next we are going to learn how to use the \sphinxcode{\sphinxupquote{nltools}} package that tries to make loading, plotting, and manipulating data easier. It uses many functions from nibabe, nilearn, and other python libraries. The bulk of the nltools toolbox is built around the \sphinxcode{\sphinxupquote{Brain\_Data()}} class. The concept behind the class is to have a similar feel to a pandas dataframe, which means that it should feel intuitive to manipulate the data.

The \sphinxcode{\sphinxupquote{Brain\_Data()}} class has several attributes that may be helpful to know about. First, it stores imaging data in \sphinxcode{\sphinxupquote{.data}} as a vectorized features by observations matrix. Each image is an observation and each voxel is a feature. Space is flattened using \sphinxcode{\sphinxupquote{nifti\_masker}} from nilearn. This object is also stored as an attribute in \sphinxcode{\sphinxupquote{.nifti\_masker}} to allow transformations from 2D to 3D/4D matrices. In addition, a brain\_mask is stored in \sphinxcode{\sphinxupquote{.mask}}. Finally, there are attributes to store either class labels for prediction/classification analyses in \sphinxcode{\sphinxupquote{.Y}} and design matrices in \sphinxcode{\sphinxupquote{.X}}. These are both expected to be pandas \sphinxcode{\sphinxupquote{DataFrames}}.

We will give a quick overview of basic Brain\_Data operations, but we encourage you to see our \sphinxhref{https://neurolearn.readthedocs.io/en/latest/index.html}{documentation} for more details.


\subsubsection{Brain\_Data basics}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:brain-data-basics}}
To get a feel for \sphinxcode{\sphinxupquote{Brain\_Data}}, let’s load an example anatomical overlay image that comes packaged with the toolbox.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Brain\PYGZus{}Data}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{get\PYGZus{}anatomical}

\PYG{n}{anat} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{get\PYGZus{}anatomical}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{anat}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
nltools.data.brain\PYGZus{}data.Brain\PYGZus{}Data(data=(238955,), Y=0, X=(0, 0), mask=MNI152\PYGZus{}T1\PYGZus{}2mm\PYGZus{}brain\PYGZus{}mask.nii.gz, output\PYGZus{}file=[])
\end{sphinxVerbatim}

To view the attributes of \sphinxcode{\sphinxupquote{Brain\_Data}} use the \sphinxcode{\sphinxupquote{vars()}} function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{vars}\PYG{p}{(}\PYG{n}{anat}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}\PYGZsq{}mask\PYGZsq{}: \PYGZlt{}nibabel.nifti1.Nifti1Image object at 0x7fad95829a50\PYGZgt{}, \PYGZsq{}nifti\PYGZus{}masker\PYGZsq{}: NiftiMasker(detrend=False, dtype=None, high\PYGZus{}pass=None, low\PYGZus{}pass=None,
            mask\PYGZus{}args=None,
            mask\PYGZus{}img=\PYGZlt{}nibabel.nifti1.Nifti1Image object at 0x7fad95829a50\PYGZgt{},
            mask\PYGZus{}strategy=\PYGZsq{}background\PYGZsq{}, memory=Memory(cachedir=None),
            memory\PYGZus{}level=1, reports=True, sample\PYGZus{}mask=None, sessions=None,
            smoothing\PYGZus{}fwhm=None, standardize=False, t\PYGZus{}r=None,
            target\PYGZus{}affine=None, target\PYGZus{}shape=None, verbose=0), \PYGZsq{}data\PYGZsq{}: array([1875., 2127., 2182., ..., 5170., 5180., 2836.], dtype=float32), \PYGZsq{}Y\PYGZsq{}: Empty DataFrame
Columns: []
Index: [], \PYGZsq{}X\PYGZsq{}: Empty DataFrame
Columns: []
Index: [], \PYGZsq{}file\PYGZus{}name\PYGZsq{}: []\PYGZcb{}
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{Brain\_Data}} has many methods to help manipulate, plot, and analyze imaging data. We can use the \sphinxcode{\sphinxupquote{dir()}} function to get a quick list of all of the available methods that can be used on this class.

To learn more about how to use these tools either use the \sphinxcode{\sphinxupquote{?}} function, or look up the function in the \sphinxhref{https://neurolearn.readthedocs.io/en/latest/api.html}{api documentation}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{dir}\PYG{p}{(}\PYG{n}{anat}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}X\PYGZsq{}, \PYGZsq{}Y\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}add\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}class\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}delattr\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}dict\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}dir\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}doc\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}eq\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}format\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}ge\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}getattribute\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}getitem\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}gt\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}hash\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}init\PYGZus{}subclass\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}iter\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}le\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}len\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}lt\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}module\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}mul\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}ne\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}new\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}radd\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}reduce\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}reduce\PYGZus{}ex\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}repr\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}rmul\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}rsub\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}setattr\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}setitem\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}sizeof\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}str\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}sub\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}subclasshook\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}\PYGZus{}\PYGZus{}weakref\PYGZus{}\PYGZus{}\PYGZsq{}, \PYGZsq{}aggregate\PYGZsq{}, \PYGZsq{}align\PYGZsq{}, \PYGZsq{}append\PYGZsq{}, \PYGZsq{}apply\PYGZus{}mask\PYGZsq{}, \PYGZsq{}astype\PYGZsq{}, \PYGZsq{}bootstrap\PYGZsq{}, \PYGZsq{}copy\PYGZsq{}, \PYGZsq{}data\PYGZsq{}, \PYGZsq{}decompose\PYGZsq{}, \PYGZsq{}detrend\PYGZsq{}, \PYGZsq{}distance\PYGZsq{}, \PYGZsq{}dtype\PYGZsq{}, \PYGZsq{}empty\PYGZsq{}, \PYGZsq{}extract\PYGZus{}roi\PYGZsq{}, \PYGZsq{}file\PYGZus{}name\PYGZsq{}, \PYGZsq{}filter\PYGZsq{}, \PYGZsq{}find\PYGZus{}spikes\PYGZsq{}, \PYGZsq{}groupby\PYGZsq{}, \PYGZsq{}icc\PYGZsq{}, \PYGZsq{}iplot\PYGZsq{}, \PYGZsq{}isempty\PYGZsq{}, \PYGZsq{}mask\PYGZsq{}, \PYGZsq{}mean\PYGZsq{}, \PYGZsq{}median\PYGZsq{}, \PYGZsq{}multivariate\PYGZus{}similarity\PYGZsq{}, \PYGZsq{}nifti\PYGZus{}masker\PYGZsq{}, \PYGZsq{}plot\PYGZsq{}, \PYGZsq{}predict\PYGZsq{}, \PYGZsq{}predict\PYGZus{}multi\PYGZsq{}, \PYGZsq{}r\PYGZus{}to\PYGZus{}z\PYGZsq{}, \PYGZsq{}randomise\PYGZsq{}, \PYGZsq{}regions\PYGZsq{}, \PYGZsq{}regress\PYGZsq{}, \PYGZsq{}scale\PYGZsq{}, \PYGZsq{}shape\PYGZsq{}, \PYGZsq{}similarity\PYGZsq{}, \PYGZsq{}smooth\PYGZsq{}, \PYGZsq{}standardize\PYGZsq{}, \PYGZsq{}std\PYGZsq{}, \PYGZsq{}sum\PYGZsq{}, \PYGZsq{}threshold\PYGZsq{}, \PYGZsq{}to\PYGZus{}nifti\PYGZsq{}, \PYGZsq{}transform\PYGZus{}pairwise\PYGZsq{}, \PYGZsq{}ttest\PYGZsq{}, \PYGZsq{}upload\PYGZus{}neurovault\PYGZsq{}, \PYGZsq{}write\PYGZsq{}]
\end{sphinxVerbatim}

Ok, now let’s load a single subject’s functional data from the localizer dataset. We will load one that has already been preprocessed with fmriprep and is stored in the derivatives folder.

Loading data can be a little bit slow especially if the data need to be resampled to the template, which is set at \(2mm^3\) by default. However, once it’s loaded into the workspace it should be relatively fast to work with it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{target}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subject}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{return\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{file}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

Here are a few quick basic data operations.

Find number of images in Brain\_Data() instance

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
128
\end{sphinxVerbatim}

Find the dimensions of the data (images x voxels)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(128, 238955)
\end{sphinxVerbatim}

We can use any type of indexing to slice the data such as integers, lists of integers, slices, or boolean vectors.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{index} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{bool}\PYG{p}{)}
\PYG{n}{index}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{22}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{k+kc}{True}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{n}{index}\PYG{p}{]}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(238955,)
(3, 238955)
(10, 238955)
(6, 238955)
\end{sphinxVerbatim}


\subsubsection{Simple Arithmetic Operations}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:simple-arithmetic-operations}}
Calculate the mean for every voxel over images

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
nltools.data.brain\PYGZus{}data.Brain\PYGZus{}Data(data=(238955,), Y=0, X=(0, 0), mask=MNI152\PYGZus{}T1\PYGZus{}2mm\PYGZus{}brain\PYGZus{}mask.nii.gz, output\PYGZus{}file=[])
\end{sphinxVerbatim}

Calculate the standard deviation for every voxel over images

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
nltools.data.brain\PYGZus{}data.Brain\PYGZus{}Data(data=(238955,), Y=0, X=(0, 0), mask=MNI152\PYGZus{}T1\PYGZus{}2mm\PYGZus{}brain\PYGZus{}mask.nii.gz, output\PYGZus{}file=[])
\end{sphinxVerbatim}

Methods can be chained.  Here we get the shape of the mean.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(238955,)
\end{sphinxVerbatim}

Brain\_Data instances can be added and subtracted

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{new} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}
\end{sphinxVerbatim}

Brain\_Data instances can be manipulated with basic arithmetic operations.

Here we add 10 to every voxel and scale by 2

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data2} \PYG{o}{=} \PYG{p}{(}\PYG{n}{data} \PYG{o}{+} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mi}{2}
\end{sphinxVerbatim}

Brain\_Data instances can be copied

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{new} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

Brain\_Data instances can be easily converted to nibabel instances, which store the data in a 3D/4D matrix.  This is useful for interfacing with other python toolboxes such as \sphinxhref{http://nilearn.github.io}{nilearn}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{to\PYGZus{}nifti}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nibabel.nifti1.Nifti1Image at 0x7fad966b7d10\PYGZgt{}
\end{sphinxVerbatim}

Brain\_Data instances can be concatenated using the append method

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{new} \PYG{o}{=} \PYG{n}{new}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

Lists of \sphinxcode{\sphinxupquote{Brain\_Data}} instances can also be concatenated by recasting as a \sphinxcode{\sphinxupquote{Brain\_Data}} object.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}list\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
nltools.data.brain\PYGZus{}data.Brain\PYGZus{}Data
\end{sphinxVerbatim}

Any Brain\_Data object can be written out to a nifti file.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tmp\PYGZus{}Data.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

Images within a Brain\_Data() instance are iterable.  Here we use a list comprehension to calculate the overall mean across all voxels within an image.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{n}{x}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{data}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[3631.2645411383587,
 3637.836965559738,
 3634.8721272399216,
 3629.901022394308,
 3625.903680905571,
 3629.8617967834693,
 3647.0626827990523,
 3656.532783431514,
 3652.9664477289007,
 3656.854690370624,
 3651.158568986219,
 3646.0627760781335,
 3647.096514090842,
 3650.3529496858014,
 3647.3099736730073,
 3647.541164716155,
 3653.0196194154546,
 3653.9954711802293,
 3648.761361342424,
 3643.4910051915754,
 3644.4916183059545,
 3643.806287863885,
 3632.513377052877,
 3634.0742188451927,
 3632.823087421774,
 3636.538413516169,
 3635.2725020511252,
 3641.7997040537466,
 3641.7916592727656,
 3634.091262543454,
 3643.245243803917,
 3658.5574913389905,
 3649.2724227942545,
 3641.1560643044045,
 3643.306476451434,
 3637.0167340942126,
 3637.5381102659767,
 3644.4683410479392,
 3639.8860350581976,
 3630.9967110874504,
 3623.7886672981303,
 3627.072535710563,
 3624.862147866512,
 3622.6118047366986,
 3634.9950195308434,
 3627.3577482514256,
 3628.0705544219018,
 3621.129485290891,
 3616.061808421585,
 3608.6081438384495,
 3619.78010303786,
 3625.2043623406903,
 3630.7597935028207,
 3628.074661527296,
 3630.138505445051,
 3624.1916798686207,
 3620.2774727712167,
 3619.0700248246812,
 3624.4436554897025,
 3625.009898536367,
 3622.018871417351,
 3629.2243599621006,
 3629.480077460646,
 3625.544210353266,
 3621.4695822776903,
 3617.3724419581886,
 3615.911084856664,
 3614.292718966583,
 3616.767158878271,
 3621.5679265347426,
 3617.094424744487,
 3609.954978219352,
 3612.64425239096,
 3629.0631390560125,
 3628.8013229265916,
 3621.5916670872352,
 3612.4806693745804,
 3613.664459683712,
 3621.6783653848643,
 3621.143953281178,
 3618.5757894664835,
 3610.795271247279,
 3613.4845495891113,
 3607.408626131088,
 3613.440184819298,
 3608.650315768319,
 3604.885826645298,
 3601.629542348782,
 3600.8264436489476,
 3600.6285381425578,
 3609.776906766102,
 3618.2023236145965,
 3615.7040390888783,
 3612.2955410039767,
 3606.200637253472,
 3623.39621843859,
 3627.132987563558,
 3611.296930448837,
 3594.7923511331287,
 3580.0196095574156,
 3579.4805425330965,
 3583.0559348949373,
 3592.5569828498697,
 3604.371337719925,
 3606.9999113264666,
 3620.9576329138504,
 3617.235063465483,
 3612.5929641090283,
 3605.798535577046,
 3597.116219752562,
 3588.5229574072764,
 3587.5091257654085,
 3598.2696343420794,
 3602.2806983812598,
 3601.700440752411,
 3610.3855186675264,
 3610.527882720576,
 3604.287286181301,
 3591.7958436327026,
 3591.129841385018,
 3598.8536387360964,
 3611.565702027962,
 3610.1719483427555,
 3618.0720138472166,
 3612.4640399870646,
 3598.581193499466,
 3594.3173923249437,
 3595.754862354543]
\end{sphinxVerbatim}

Though, we could also do this with the \sphinxcode{\sphinxupquote{mean}} method by setting \sphinxcode{\sphinxupquote{axis=1}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([3631.26454114, 3637.83696556, 3634.87212724, 3629.90102239,
       3625.90368091, 3629.86179678, 3647.0626828 , 3656.53278343,
       3652.96644773, 3656.85469037, 3651.15856899, 3646.06277608,
       3647.09651409, 3650.35294969, 3647.30997367, 3647.54116472,
       3653.01961942, 3653.99547118, 3648.76136134, 3643.49100519,
       3644.49161831, 3643.80628786, 3632.51337705, 3634.07421885,
       3632.82308742, 3636.53841352, 3635.27250205, 3641.79970405,
       3641.79165927, 3634.09126254, 3643.2452438 , 3658.55749134,
       3649.27242279, 3641.1560643 , 3643.30647645, 3637.01673409,
       3637.53811027, 3644.46834105, 3639.88603506, 3630.99671109,
       3623.7886673 , 3627.07253571, 3624.86214787, 3622.61180474,
       3634.99501953, 3627.35774825, 3628.07055442, 3621.12948529,
       3616.06180842, 3608.60814384, 3619.78010304, 3625.20436234,
       3630.7597935 , 3628.07466153, 3630.13850545, 3624.19167987,
       3620.27747277, 3619.07002482, 3624.44365549, 3625.00989854,
       3622.01887142, 3629.22435996, 3629.48007746, 3625.54421035,
       3621.46958228, 3617.37244196, 3615.91108486, 3614.29271897,
       3616.76715888, 3621.56792653, 3617.09442474, 3609.95497822,
       3612.64425239, 3629.06313906, 3628.80132293, 3621.59166709,
       3612.48066937, 3613.66445968, 3621.67836538, 3621.14395328,
       3618.57578947, 3610.79527125, 3613.48454959, 3607.40862613,
       3613.44018482, 3608.65031577, 3604.88582665, 3601.62954235,
       3600.82644365, 3600.62853814, 3609.77690677, 3618.20232361,
       3615.70403909, 3612.295541  , 3606.20063725, 3623.39621844,
       3627.13298756, 3611.29693045, 3594.79235113, 3580.01960956,
       3579.48054253, 3583.05593489, 3592.55698285, 3604.37133772,
       3606.99991133, 3620.95763291, 3617.23506347, 3612.59296411,
       3605.79853558, 3597.11621975, 3588.52295741, 3587.50912577,
       3598.26963434, 3602.28069838, 3601.70044075, 3610.38551867,
       3610.52788272, 3604.28728618, 3591.79584363, 3591.12984139,
       3598.85363874, 3611.56570203, 3610.17194834, 3618.07201385,
       3612.46403999, 3598.5811935 , 3594.31739232, 3595.75486235])
\end{sphinxVerbatim}

Let’s plot the mean to see how the global signal changes over time.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZlt{}matplotlib.lines.Line2D at 0x7fad966a6590\PYGZgt{}]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Neuroimaging_Data_90_1}.png}

Notice the slow linear drift over time, where the global signal intensity gradually decreases. We will learn how to remove this with a high pass filter in future tutorials.


\subsubsection{Plotting}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:plotting}}
There are multiple ways to plot your data.

For a very quick plot, you can return a montage of axial slices with the \sphinxcode{\sphinxupquote{.plot()}} method. As an example, we will plot the mean of each voxel over time.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Neuroimaging_Data_93_1}.png}

There is an interactive \sphinxcode{\sphinxupquote{.iplot()}} method based on nilearn \sphinxcode{\sphinxupquote{view\_img}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{iplot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatText(value=0.0, description=\PYGZsq{}Threshold\PYGZsq{}), HTML(value=\PYGZsq{}Image is 3D\PYGZsq{}, description=\PYGZsq{}Vo…
\end{sphinxVerbatim}

Brain\_Data() instances can be converted to a nibabel instance and plotted using any nilearn plot method such as glass brain.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plot\PYGZus{}glass\PYGZus{}brain}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{to\PYGZus{}nifti}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.displays.OrthoProjector at 0x7fad07c45a10\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Introduction_to_Neuroimaging_Data_97_1}.png}

Ok, that’s the basics. \sphinxcode{\sphinxupquote{Brain\_Data}} can do much more!

Check out some of our \sphinxhref{https://neurolearn.readthedocs.io/en/latest/auto\_examples/index.html}{tutorials} for more detailed examples.

We’ll be using this tool throughout the course.


\subsection{Exercises}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:exercises}}
For homework, let’s practice our skills in working with data.


\subsubsection{Exercise 1}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:exercise-1}}
A few subjects have already been preprocessed with fMRI prep.

Use pybids to figure out which subjects have been preprocessed.


\subsubsection{Exercise 2}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:exercise-2}}
One question we are often interested in is where in the brain do we have an adequate signal to noise ratio (SNR). There are many different metrics, here we will use temporal SNR, which the voxel mean over time divided by it’s standard deviation.
\begin{equation*}
\begin{split}\text{tSNR} = \frac{\text{mean}(\text{voxel}_{i})}{\text{std}(\text{voxel}_i)}\end{split}
\end{equation*}
In Exercise 2, calculate the SNR for S01 and plot this so we can figure which regions have high and low SNR.


\subsubsection{Exercise 3}
\label{\detokenize{content/Introduction_to_Neuroimaging_Data:exercise-3}}
We are often interested in identifying outliers in our data. In this exercise, find any image from ‘S01’ that exceeds a zscore of 2 and plot each one.


\section{Signal Processing Basics}
\label{\detokenize{content/Signal_Processing:signal-processing-basics}}\label{\detokenize{content/Signal_Processing::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

In this lab, we will cover the basics of convolution, sine waves, and fourier transforms. This lab is largely based on exercises from Mike X Cohen’s excellent book, \sphinxhref{https://www.amazon.com/Analyzing-Neural-Time-Data-Practice/dp/0262019876}{Analying Neural Data Analysis: Theory and Practice}. If you are interested in learning in more detail about the basics of EEG and time\sphinxhyphen{}series analyses I highly recommend his accessible introduction. I also encourage you to watch his accompanying freely available \sphinxhref{https://www.youtube.com/channel/UCUR\_LsXk7IYyueSnXcNextQ}{\sphinxstyleemphasis{lecturelets}} to learn more about each topic introduced in this notebook.


\subsection{Time Domain}
\label{\detokenize{content/Signal_Processing:time-domain}}
First we will work on signals in the time domain.  This requires measuring a signal at a constant interval over time.  The frequency with which we measure a signal is referred to as the sampling frequency.  The units of this are typically described in \(Hz\) \sphinxhyphen{} or the number of cycles per second. It is critical that the sampling frequency is consistent over the entire measurement of the time series.


\subsubsection{Dot Product}
\label{\detokenize{content/Signal_Processing:dot-product}}
To understand convolution, we first need to familiarize ourselves with the dot product.  The dot product is simply the sum of the elements of a vector weighted by the elements of another vector. This method is commonly used in signal processing, and also in statistics as a measure of similarity between two vectors. Finally, there is also a geometric inrepretation which is a mapping between vectors (i.e., the product of the magnitudes of the two vectors scaled by the cosine of the angle between them). For a more in depth overview of the dot product and its relation to convolution, you can watch this optional \sphinxhref{https://youtu.be/rea6M1oagmA}{video}.

\(dotproduct_{ab}=\sum\limits_{i=1}^n a_i b_i\)

Let’s create some vectors of random numbers and see how the dot product works.  First, the two vectos need to be of the same length.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{n}{a} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{n}{b}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scatterplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dot Product: }\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Dot Product: 434
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_3_1}.png}

what happens when we make the two variables more similar?  In the next example we add gaussian noise on top of one of the vectors.  What happens to the dot product?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{b} \PYG{o}{=} \PYG{n}{a} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{n}{b}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scatterplot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dot Product: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{n}{b}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Dot Product: 583.1197152049825
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_5_1}.png}


\subsubsection{Convolution}
\label{\detokenize{content/Signal_Processing:convolution}}
Convolution in the time domain is an extension of the dot product in which the dot product is computed iteratively over time.  One way to think about it is that one signal weights each time point of the other signal and then slides forward over time.  Let’s call the timeseries variable \sphinxstyleemphasis{signal} and the other vector the \sphinxstyleemphasis{kernel}. Importantly, for our purposes, the kernel will almost always be smaller than the signal, otherwise we would only have one scalar value afterwards.

To gain an intuition of how convolution works, let’s play with some data. First, let’s create a time series of spikes. Then let’s convolve this signal with a boxcar kernel.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{100}

\PYG{n}{signal} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}
\PYG{n}{signal}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0} \PYG{p}{,}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{n}{kernel} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{kernel}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{l+m+mi}{8}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Signal Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Signal }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kernel}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Kernel \PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_7_1}.png}

Notice how the kernel is only 10 samples long and the boxcar width is about 6 seconds, while the signal is 100 samples long with 5 single pulses.

Now let’s convolve the signal with the kernel by taking the dot product of the kernel with each time point of the signal. This can be illustrated by creating a matrix of the kernel shifted each time point of the signal.

We will illustrate using a heatmap, where the change in the color reflects the intensity, that this is simply moving the boxcar kernel, which is 6 seconds in duration forward in time for each sample.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{shifted\PYGZus{}kernel} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{o}{+}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{kernel}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{shifted\PYGZus{}kernel}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{n}{i}\PYG{p}{:}\PYG{n}{i}\PYG{o}{+}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{kernel}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{n}{kernel}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{shifted\PYGZus{}kernel}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Reds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time Shifted Kernels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Time Shifted Kernels\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_9_1}.png}

Now, let’s take the dot product of the signal with this matrix.

To refresh your memory from basic linear algebra. Matrix multiplication consists of taking the dot product of the signal vector with each row of this expanded kernel matrix.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{convolved\PYGZus{}signal} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{shifted\PYGZus{}kernel}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{convolved\PYGZus{}signal}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Signal convolved with boxcar kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Signal convolved with boxcar kernel\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_11_1}.png}

You can see that after convolution, each spike has now become the shape of the kernel. Spikes that were closer in time, compound if the boxes overlap.

Notice also how the shape of the final signal is the length of the combined signal and kernel minus one.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Signal Length: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Kernel Length: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{kernel}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Convolved Signal Length: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{convolved\PYGZus{}signal}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Signal Length: 100
Kernel Length: 10
Convolved Signal Length: 109
\end{sphinxVerbatim}

this process of iteratively taking the dot product of the kernel with each timepoint of the signal and summing all of the values can be performed by using the convolution function from numpy \sphinxcode{\sphinxupquote{np.convolve}}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{kernel}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Signal convolved with boxcar kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Signal convolved with boxcar kernel\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_15_1}.png}

What happens if the spikes have different intensities, reflected by different heights?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{signal} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}
\PYG{n}{signal}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Timeseries of spikes with varying intensities}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{kernel}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Signal convolved with boxcar kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 0, \PYGZsq{}Time\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_17_1}.png}

Now what happens if we switch out the boxcar kernel for something with a more interesting shape, say a hemodynamic response function?

Here we will use a double gamma hemodynamic function (HRF) developed by Gary Glover.

\sphinxstylestrong{Note}: If you haven’t install nltools yet run \sphinxcode{\sphinxupquote{!pip install nltools}}.  You may need to restart your jupyter kernel as well.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{external} \PYG{k+kn}{import} \PYG{n}{glover\PYGZus{}hrf}

\PYG{n}{tr} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{n}{hrf} \PYG{o}{=} \PYG{n}{glover\PYGZus{}hrf}\PYG{p}{(}\PYG{n}{tr}\PYG{p}{,} \PYG{n}{oversampling}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{hrf}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hemodynamic Response Function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear\PYGZus{}model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear\PYGZus{}model. Anything that cannot be imported from sklearn.linear\PYGZus{}model is now part of the private API.
  warnings.warn(message, FutureWarning)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Hemodynamic Response Function\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_19_2}.png}

For this example, we oversampled the function to make it more smooth.  In practice we will want to make sure that the kernel is the correct shape given our sampling resolution.  Be sure to se the oversampling to 1.  Notice how the function looks more jagged now?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{hrf} \PYG{o}{=} \PYG{n}{glover\PYGZus{}hrf}\PYG{p}{(}\PYG{n}{tr}\PYG{p}{,} \PYG{n}{oversampling}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{hrf}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hemodynamic Response Function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Hemodynamic Response Function\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_21_1}.png}

Now let’s try convolving our event pulses with this HRF kernel.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{signal} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}
\PYG{n}{signal}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{hrf}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Timeseries of spikes with varying intensities}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Signal convolved with boxcar kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Signal convolved with boxcar kernel\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_23_1}.png}

If you are interested in a more detailed overview of convolution in the time domain, I encourage you to watch this \sphinxhref{https://youtu.be/9Hk-RAIzOaw}{video} by Mike X Cohen. For more details about convolution and the HRF function, see this \sphinxhref{https://practical-neuroimaging.github.io/on\_convolution.html}{overview} using python examples.


\subsubsection{Oscillations}
\label{\detokenize{content/Signal_Processing:oscillations}}
Ok, now let’s move on to studying time\sphinxhyphen{}varying signals that have the shape of oscillating waves.

Let’s watch a short video by Mike X Cohen to get some more background on sine waves. Don’t worry too much about the matlab code as we will work through similar Python examples in this notebook.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{9RvZXZ46FRQ}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_26_0}.jpg}

Oscillations can be described mathematically as:

\(A\sin(2 \pi ft + \theta)\)

where \(f\) is the frequency or the speed of the oscillation described in the number of cycles per second \sphinxhyphen{} \(Hz\). Amplitude \(A\) refers to the height of the waves, which is half the distance of the peak to the trough. Finally, \(\theta\) describes the phase angle offset, which is in radians.

Here we will plot a simple sine wave.  Try playing with the different parameters (i.e., amplitude, frequency, \& theta) to gain an intuition of how they each impact the shape of the wave.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{numpy} \PYG{k+kn}{import} \PYG{n}{sin}\PYG{p}{,} \PYG{n}{pi}\PYG{p}{,} \PYG{n}{arange}

\PYG{n}{sampling\PYGZus{}freq} \PYG{o}{=} \PYG{l+m+mi}{500}
\PYG{n}{time} \PYG{o}{=} \PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{)}
\PYG{n}{amplitude} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{freq} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{theta} \PYG{o}{=} \PYG{l+m+mi}{0}

\PYG{n}{simulation} \PYG{o}{=} \PYG{n}{amplitude} \PYG{o}{*} \PYG{n}{sin}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{pi} \PYG{o}{*} \PYG{n}{freq} \PYG{o}{*} \PYG{n}{time} \PYG{o}{+} \PYG{n}{theta}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{time}\PYG{p}{,} \PYG{n}{simulation}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sine Wave}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Amplitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Amplitude\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_28_1}.png}

We can also see the impact of different parameters using interactive widgets. Here you can move the sliders to see the impact of varying the amplitude, frequency, and theta parameter on a sine wave. We also show the complex components of the sine wave in the right panel.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{ipywidgets} \PYG{k+kn}{import} \PYG{n}{interact}\PYG{p}{,} \PYG{n}{FloatSlider}
\PYG{k+kn}{from} \PYG{n+nn}{numpy} \PYG{k+kn}{import} \PYG{n}{sin}\PYG{p}{,} \PYG{n}{pi}\PYG{p}{,} \PYG{n}{arange}\PYG{p}{,} \PYG{n}{real}\PYG{p}{,} \PYG{n}{imag}

\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}oscillation}\PYG{p}{(}\PYG{n}{amplitude}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{frequency}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{theta}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{sampling\PYGZus{}frequency}\PYG{o}{=}\PYG{l+m+mi}{500}
    \PYG{n}{time} \PYG{o}{=} \PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}
    \PYG{n}{simulation} \PYG{o}{=} \PYG{n}{amplitude} \PYG{o}{*} \PYG{n}{sin}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{pi} \PYG{o}{*} \PYG{n}{frequency} \PYG{o}{*} \PYG{n}{time} \PYG{o}{+} \PYG{n}{theta}\PYG{p}{)}
    \PYG{n}{z} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{n}{j}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{pi} \PYG{o}{*} \PYG{n}{frequency} \PYG{o}{*} \PYG{n}{time} \PYG{o}{+} \PYG{n}{theta}\PYG{p}{)}\PYG{p}{)}

    \PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{gs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{GridSpec}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{left}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{right}\PYG{o}{=}\PYG{l+m+mf}{0.48}\PYG{p}{,} \PYG{n}{wspace}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}
    \PYG{n}{ax1} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{n}{gs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{time}\PYG{p}{,} \PYG{n}{simulation}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Amplitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    \PYG{n}{ax2} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{n}{gs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{polar}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{real}\PYG{p}{(}\PYG{n}{simulation}\PYG{p}{)}\PYG{p}{,} \PYG{n}{imag}\PYG{p}{(}\PYG{n}{simulation}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{interact}\PYG{p}{(}\PYG{n}{plot\PYGZus{}oscillation}\PYG{p}{,} \PYG{n}{amplitude}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{,}
         \PYG{n}{frequency}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{,} 
         \PYG{n}{theta}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatSlider(value=5.0, description=\PYGZsq{}amplitude\PYGZsq{}, max=10.0, step=0.5), FloatSlider(value=5…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}function \PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}.plot\PYGZus{}oscillation(amplitude=5, frequency=5, theta=1)\PYGZgt{}
\end{sphinxVerbatim}

Next we will generate a simulation combining multiple sine waves oscillating at different frequencies.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sampling\PYGZus{}freq} \PYG{o}{=} \PYG{l+m+mi}{500}

\PYG{n}{freq} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5} \PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{35}\PYG{p}{]}
\PYG{n}{amplitude} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{phases} \PYG{o}{=} \PYG{n}{pi}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{time} \PYG{o}{=} \PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{)} 

\PYG{n}{sine\PYGZus{}waves} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,}\PYG{n}{f} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{freq}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{sine\PYGZus{}waves}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{amplitude}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{*} \PYG{n}{sin}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{pi}\PYG{o}{*}\PYG{n}{f}\PYG{o}{*}\PYG{n}{time} \PYG{o}{+} \PYG{n}{phases}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sine\PYGZus{}waves} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{)}


\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,}\PYG{n}{x} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{freq}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sine waves oscillating at different frequencies}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Time}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}    
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_32_0}.png}

Let’s add all of those signals together to get a more complex signal.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sum of all of the sine waves}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Time}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_34_0}.png}

What is the effect of changing the sampling frequency on our ability to measure these oscillations?  Try dropping it to be very low (e.g., less than 70 hz.)  Notice that signals will alias when the sampling frequency is below the nyquist frequency of a signal. To observe the oscillations, we need to be sampling at least two times for each oscillation cycle. This will result in a jagged view of the data, but we can still theoretically observe the frequency. Practically, higher sampling rates allow us to better observe the underlying signals.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sampling\PYGZus{}freq} \PYG{o}{=} \PYG{l+m+mi}{60}

\PYG{n}{freq} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5} \PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{35}\PYG{p}{]}
\PYG{n}{amplitude} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{phases} \PYG{o}{=} \PYG{n}{pi}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{time} \PYG{o}{=} \PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{)} 

\PYG{n}{sine\PYGZus{}waves} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,}\PYG{n}{f} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{freq}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{sine\PYGZus{}waves}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{amplitude}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{*} \PYG{n}{sin}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{pi}\PYG{o}{*}\PYG{n}{f}\PYG{o}{*}\PYG{n}{time} \PYG{o}{+} \PYG{n}{phases}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sine\PYGZus{}waves} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{)}


\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,}\PYG{n}{x} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{freq}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sine waves oscillating at different frequencies}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Time}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}    


\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sum of all of the sine waves}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Time}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}    
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_36_0}.png}

\noindent\sphinxincludegraphics{{Signal_Processing_36_1}.png}

Notice the jagged lines for frequencies that are above the nyquist frequency? That’s because we don’t have enough samples to accurately see the oscillations.

Ok, let’s increase the sampling frequency to remove the aliasing. We can add a little bit of gaussian (white) noise on top of this signal to make it even more realistic.  Try varying the amount of noise by adjusting the scaling on the noise.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sampling\PYGZus{}freq} \PYG{o}{=} \PYG{l+m+mi}{500}

\PYG{n}{freq} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5} \PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{35}\PYG{p}{]}
\PYG{n}{amplitude} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{phases} \PYG{o}{=} \PYG{n}{pi}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{time} \PYG{o}{=} \PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{)} 

\PYG{n}{sine\PYGZus{}waves} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,}\PYG{n}{f} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{freq}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{sine\PYGZus{}waves}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{amplitude}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{*} \PYG{n}{sin}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{pi}\PYG{o}{*}\PYG{n}{f}\PYG{o}{*}\PYG{n}{time} \PYG{o}{+} \PYG{n}{phases}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sine\PYGZus{}waves} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{)}


\PYG{n}{noise} \PYG{o}{=} \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{signal} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{+} \PYG{n}{noise}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{signal}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sum of sine waves plus white noise}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 0, \PYGZsq{}Time\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_38_1}.png}


\subsection{Time \& Frequency Domains}
\label{\detokenize{content/Signal_Processing:time-frequency-domains}}
We have seen above how to represent signals in the time domain. However, these signals can also be represented in the frequency domain.

Let’s get started by watching a short video by Mike X Cohen to get an overview of how a signal can be represented in both of these different domains.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fYtVHhk3xJ0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_40_0}.jpg}


\subsection{Frequency Domain}
\label{\detokenize{content/Signal_Processing:frequency-domain}}
In the previous example, we generated a complex signal composed of multiple sine waves oscillating at different frequencies. Typically in data analysis, we only observe the signal and are trying to uncover the generative processes that gave rise to the signal.  In this section, we will introduce the frequency domain and how we can identify if there are any frequencies oscillating at a consistent frequency in our signal using the fourier transform. The fourier transform is essentially convolving different frequencies of sine waves with our data.

One important assumption to note is that the fourier transformations assume that your oscillatory signals are stationary, which means that the generative processes giving rise to the oscillations do not vary over time.

See this \sphinxhref{https://youtu.be/rea6M1oagmA}{video} for a more in depth discussion on stationarity.  In practice, this assumption is rarely true.  Often it can be useful to use other techniques such as wavelets to look at time x frequency representations. We will not be covering wavelets here, but see this series of \sphinxhref{https://youtu.be/7ahrcB5HL0k}{videos} for more information.


\subsubsection{Discrete Time Fourier Transform}
\label{\detokenize{content/Signal_Processing:discrete-time-fourier-transform}}
We will gain an intution of how the fourier transform works by building our own discrete time fourier transform.

Let’s watch this short video about the fourier transform by Mike X Cohen. Don’t worry too much about the details of the discussion on the matlab code as we will be exploring these concepts in python below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}htCsieA0\PYGZus{}U}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_43_0}.jpg}

The discrete Fourier transform of variable \(x\) at frequency \(f\) can be defined as:

\(X_f = \sum\limits_{k=0}^{n-1} x_k e^{-i2\pi f(k-1)n^{-1}}\)
where \(n\) refers to the number of data points in vector \(x\), and the capital letter \(X_f\) is the fourier coefficient of time series variable \(x\) at frequency \(f\).

Essentially, we create a bank of complex sine waves at different frequencies that are linearly spaced. The zero frequency component reflects the mean offset over the entire signal and will simply be zero in our example.


\paragraph{Complex Sine Waves}
\label{\detokenize{content/Signal_Processing:complex-sine-waves}}
You may have noticed that we are computing \sphinxstyleemphasis{complex} sine waves using the \sphinxcode{\sphinxupquote{np.exp}} function instead of the \sphinxcode{\sphinxupquote{np.sin}} function.
\begin{equation*}
\begin{split}\text{complex sine wave} = e^{i(2\pi ft + \theta)}\end{split}
\end{equation*}
We will not spend too much time on the details, but basically complex sine waves have three components: time, a real part of the sine wave, and the imaginary part of the sine wave, which are basically phase shifted by \(\frac{\pi}{2}\).
\sphinxcode{\sphinxupquote{1j}} is how we can specify a complex number in python. We can extract the real components using \sphinxcode{\sphinxupquote{np.real}} or the imaginary using \sphinxcode{\sphinxupquote{np.imag}}.

We can visualize complex sine waves in three dimensions. For more information, watch this \sphinxhref{https://youtu.be/iZCDOuzfsY0}{video}. If you need a refresher on complex numbers, you may want to watch this \sphinxhref{https://youtu.be/fNfXKiIIufY}{video}.

In this plot we show this complex signal in 3 dimensions and also project on two dimensional planes to show that the real and imaginary create a unit circle, and are phase offset by \(\frac{\pi}{2}\) with respect to time.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{mpl\PYGZus{}toolkits} \PYG{k+kn}{import} \PYG{n}{mplot3d}

\PYG{n}{frequency} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{z} \PYG{o}{=}  \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{n}{j}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{pi} \PYG{o}{*} \PYG{n}{frequency} \PYG{o}{*} \PYG{n}{time} \PYG{o}{+} \PYG{n}{theta}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{time}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{,} \PYG{n}{real}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}\PYG{p}{,} \PYG{n}{imag}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (sec)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real(z)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Imaginary(z)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Complex Sine Wave}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{view\PYGZus{}init}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{250}\PYG{p}{)}

\PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{real}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}\PYG{p}{,} \PYG{n}{imag}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real(z)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Imaginary(z)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Projecting on Real and Imaginary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}

\PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{time}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{,} \PYG{n}{real}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (sec)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real(z)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Projecting on Real and Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}

\PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{time}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{,} \PYG{n}{imag}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (sec)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Imaginary(z)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Projecting on Imaginary and Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_46_0}.png}


\paragraph{Create a filter bank}
\label{\detokenize{content/Signal_Processing:create-a-filter-bank}}
Ok, now let’s create a bank of n\sphinxhyphen{}1 linearly spaced complex sine waves and plot first 5 waves to see their frequencies.

Remember the first basis function is zero frequency component and reflects the mean offset over the entire signal.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{numpy} \PYG{k+kn}{import} \PYG{n}{exp}

\PYG{n}{time} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{)}

\PYG{n}{sine\PYGZus{}waves} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{sine\PYGZus{}waves}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{n}{j}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{pi}\PYG{o}{*}\PYG{n}{i}\PYG{o}{*}\PYG{n}{time}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sine\PYGZus{}waves} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{)}

\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bank of sine waves}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_48_0}.png}

We can visualize all of the sine waves simultaneously using a heatmap representation.  Each row is a different sine wave, and columns reflect time.  The intensity of the value is like if the sine wave was coming towards and away rather than up and down. Notice how it looks like that the second half of the sine waves appear to be a mirror image of the first half.  This is because the first half contain the \sphinxstyleemphasis{positive} frequencies, while the second half contains the \sphinxstyleemphasis{negative} frequencies.  Negative frequencies capture sine waves that travel in reverse order around the complex plane compared to that travel forward. This becomes more relevant with the hilbert transform, but for the purposes of this tutorial we will be ignoring the negative frequencies.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{real}\PYG{p}{(}\PYG{n}{sine\PYGZus{}waves}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 0, \PYGZsq{}Time\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_50_1}.png}


\paragraph{Estimate Fourier Coefficients}
\label{\detokenize{content/Signal_Processing:estimate-fourier-coefficients}}
Now let’s take the dot product of each of the sine wave basis set with our signal to get the fourier coefficients.

We can \sphinxstyleemphasis{scale} the coefficients to be more interpretable by dividing by the number of time points and multiplying by 2. Watch this \sphinxhref{https://youtu.be/Ee9btm3tros}{video} if you’re interested in a more detailed explanation. Basically, this only needs to be done if you want the amplitude to be in the same units as the original data. In practice, this scaling factor will not change your interpretation of the spectrum.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fourier} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{sine\PYGZus{}waves}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Visualizing Fourier Coefficients}
\label{\detokenize{content/Signal_Processing:visualizing-fourier-coefficients}}
Now that we have computed the fourier transform, we might want to examine the results.  The fourier transform provides a 3\sphinxhyphen{}D representation of the data including frquency, power, and phase. Typically, the phase information is ignored when plotting the results of a fourier analysis. The traditional way to view the information is plot the data as amplitude on the \sphinxstyleemphasis{y\sphinxhyphen{}axis} and frequency on the \sphinxstyleemphasis{x\sphinxhyphen{}axis}. We will extract amplitude by taking the absolute value of the fourier coefficients. Remember that we are only focusing on the positive frequencies (the 1st half of the sine wave basis functions).

Here the x axis simply reflects the index of the frequency.  The actual frequency is \(N/2 + 1\) as we are only able estimate frequencies that are half the sampling frequency, this is called the Nyquist frequency. Also, note that we are only plotting the first half of the frequencies. This is because we are only plotting the \sphinxstyleemphasis{positive} frequencies. We will ignore frequencies above the nyquist frequency (i.e., \(\frac{\text{fs}}{2}\)), which are called negative frequencies. Watch this \sphinxhref{https://youtu.be/Nupda1rm01Y}{video} if you’d like more information about why.

Watch this \sphinxhref{https://youtu.be/oh7WvhlkxnU}{video} to hear more about frequencies and zero padding.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{fourier}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ceil}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{fourier}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency (index)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Amplitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Power spectrum derived from discrete fourier transform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Power spectrum derived from discrete fourier transform\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_54_1}.png}

Notice that there are 5 different frequencies that have varying amplitudes. Recall that when we simulated this data we added 5 different sine waves with different frequencies and amplitudes.

\sphinxcode{\sphinxupquote{freq = {[}3, 10, 5 ,15, 35{]}}}
\sphinxcode{\sphinxupquote{amplitude = {[}5, 15, 10, 5, 7{]}}}

Let’s zoom in a bit more to see this more clearly and also add the correct frequency labels in \(Hz\). We will use the numpy \sphinxcode{\sphinxupquote{fftfreq}} function to help convert frequency indices to \(Hz\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{numpy}\PYG{n+nn}{.}\PYG{n+nn}{fft} \PYG{k+kn}{import} \PYG{n}{fftfreq}

\PYG{n}{freq} \PYG{o}{=} \PYG{n}{fftfreq}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{sampling\PYGZus{}freq}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{freq}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{80}\PYG{p}{]}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{fourier}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{80}\PYG{p}{]}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency (Hz)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Amplitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Power spectrum derived from discrete fourier transform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Power spectrum derived from discrete fourier transform\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_56_1}.png}

Ok, now that we’ve created our own discrete fourier transform, let’s learn a few more important details that are important to consider.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RHjqvcKVopg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_58_0}.jpg}


\subsubsection{Inverse Fourier Transform}
\label{\detokenize{content/Signal_Processing:inverse-fourier-transform}}
The fourier transform allows you to represent a time series in the frequency domain. This is a lossless operation, meaning that no information in the original signal is lost by the transform. This means that we can reconstruct the original signal by inverting the operation. Thus, we can create a time series with only the frequency domain information using the \sphinxstyleemphasis{inverse fourier transform}. Watch this \sphinxhref{https://youtu.be/HFacSL--vps}{video} if you would like a more in depth explanation.

\(x_k = \sum\limits_{k=0}^{n-1} X_k e^{i2\pi f(k-1)n^{-1}}\)

Notice that we are computing the dot product between the complex sine wave and the fourier coefficients \(X\) instead of the time series data \(x\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fourier}\PYG{p}{,} \PYG{n}{sine\PYGZus{}waves}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Reconstructed Time Series Signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Reconstructed Time Series Signal\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_60_1}.png}


\subsubsection{Fast Fourier Transform}
\label{\detokenize{content/Signal_Processing:fast-fourier-transform}}
The discrete time fourier transform is useful to understand the relationship between the time and frequency domains. However, in practice this method is rarely used as there are more faster and efficient methods to perform this computation. One popular algorithm is called the fast fourier transform (FFT). This function is also in numpy \sphinxcode{\sphinxupquote{np.fft.fft}}. Don’t forget to divide by the number of samples to keep the scaling.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{numpy}\PYG{n+nn}{.}\PYG{n+nn}{fft} \PYG{k+kn}{import} \PYG{n}{fft}\PYG{p}{,} \PYG{n}{ifft}\PYG{p}{,} \PYG{n}{fftfreq}

\PYG{n}{fourier\PYGZus{}fft} \PYG{o}{=} \PYG{n}{fft}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{80}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{fourier\PYGZus{}fft}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{80}\PYG{p}{]}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Amplitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency (Hz)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency domain representation of signal derived from fast fourier transform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Frequency domain representation of signal derived from fast fourier transform\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_62_1}.png}

We can also use the \sphinxcode{\sphinxupquote{ifft}} to perform an inverse fourier transform.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{ifft}\PYG{p}{(}\PYG{n}{fourier\PYGZus{}fft}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Reconstructed Time Series Signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Reconstructed Time Series Signal\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_64_1}.png}


\subsubsection{Convolution Theorem}
\label{\detokenize{content/Signal_Processing:convolution-theorem}}
Convolution in the time domain is the same multiplication in the frequency domain. This means that time domain convolution computations can be performed much more efficiently in the frequency domain via simple multiplication. (The opposite is also true that multiplication in the time domain is the same as convolution in the frequency domain. Watch this \sphinxhref{https://youtu.be/hj7j4Q8T3Ck}{video} for an overview of the convolution theorem and convolution in the frequency domain.

\sphinxincludegraphics{{ConvolutionTheorem}.png}


\subsection{Filters}
\label{\detokenize{content/Signal_Processing:filters}}
Filters can be classified as finite impulse response (FIR) or infinite impulse response (IIR). These terms describe how a filter responds to a single input impulse.  FIR filters have a response that ends at a disrete point in time, while IIR filters have a response that continues indefinitely.

Filters are constructed in the frequency domain and several properties that need to be considers.
\begin{itemize}
\item {} 
ripple in the pass\sphinxhyphen{}band

\item {} 
attenuation in the stop\sphinxhyphen{}band

\item {} 
steepness of roll\sphinxhyphen{}off

\item {} 
filter order (i.e., length for FIR filters)

\item {} 
time\sphinxhyphen{}domain ringing

\end{itemize}

In general, there is a frequency by time tradeoff.  The sharper something is in frequency, the broader it is in time, and vice versa.

Here we will use IIR butterworth filters as an example.


\subsubsection{High Pass}
\label{\detokenize{content/Signal_Processing:high-pass}}
High pass filters only allow high frequency signals to remain, effectively \sphinxstyleemphasis{removing} any low frequency information.

Here we will construct a high pass butterworth filter and plot it in frequency space.

\sphinxstylestrong{Note}: this example requires using scipy 1.2.1+.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{signal} \PYG{k+kn}{import} \PYG{n}{butter}\PYG{p}{,} \PYG{n}{filtfilt}\PYG{p}{,} \PYG{n}{freqz}

\PYG{n}{filter\PYGZus{}order} \PYG{o}{=} \PYG{l+m+mi}{3}
\PYG{n}{frequency\PYGZus{}cutoff} \PYG{o}{=} \PYG{l+m+mi}{25}
\PYG{n}{sampling\PYGZus{}frequency} \PYG{o}{=} \PYG{l+m+mi}{500}

\PYG{c+c1}{\PYGZsh{} Create the filter}
\PYG{n}{b}\PYG{p}{,} \PYG{n}{a} \PYG{o}{=} \PYG{n}{butter}\PYG{p}{(}\PYG{n}{filter\PYGZus{}order}\PYG{p}{,} \PYG{n}{frequency\PYGZus{}cutoff}\PYG{p}{,} \PYG{n}{btype}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{output}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ba}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fs}\PYG{o}{=}\PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{rad\PYGZus{}sample\PYGZus{}to\PYGZus{}hz}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{fs}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{x}\PYG{o}{*}\PYG{n}{fs}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}filter}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{fs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{w}\PYG{p}{,} \PYG{n}{h} \PYG{o}{=} \PYG{n}{freqz}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{worN}\PYG{o}{=}\PYG{l+m+mi}{512}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{whole}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{rad\PYGZus{}sample\PYGZus{}to\PYGZus{}hz}\PYG{p}{(}\PYG{n}{w}\PYG{p}{,} \PYG{n}{fs}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{h}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    
\PYG{n}{plot\PYGZus{}filter}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_68_0}.png}

Notice how the gain scales from {[}0,1{]}?  Filters can be multiplied by the FFT of a signal to apply the filter in the frequency domain. When the resulting signal is transformed back in the time domain using the inverse FFT, the new signal will be filtered. This can be much faster than applying filters in the time domain.

The filter\_order parameter adjusts the sharpness of the cutoff in the frequency domain.  Try playing with different values to see how it changes the filter plot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{filter\PYGZus{}order} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{n}{frequency\PYGZus{}cutoff} \PYG{o}{=} \PYG{l+m+mi}{25}
\PYG{n}{sampling\PYGZus{}frequency} \PYG{o}{=} \PYG{l+m+mi}{500}


\PYG{n}{b}\PYG{p}{,} \PYG{n}{a} \PYG{o}{=} \PYG{n}{butter}\PYG{p}{(}\PYG{n}{filter\PYGZus{}order}\PYG{p}{,} \PYG{n}{frequency\PYGZus{}cutoff}\PYG{p}{,} \PYG{n}{btype}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{output}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ba}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fs}\PYG{o}{=}\PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}    

\PYG{n}{plot\PYGZus{}filter}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_70_0}.png}

What does the filter look like in the temporal domain?  Let’s take the inverse FFT and plot it to see what it looks like as a kernel in the temporal domain.  Notice how changing the filter order adds more ripples in the time domain.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{signal} \PYG{k+kn}{import} \PYG{n}{sosfreqz}

\PYG{n}{filter\PYGZus{}order} \PYG{o}{=} \PYG{l+m+mi}{8}
\PYG{n}{sos} \PYG{o}{=} \PYG{n}{butter}\PYG{p}{(}\PYG{n}{filter\PYGZus{}order}\PYG{p}{,} \PYG{n}{frequency\PYGZus{}cutoff}\PYG{p}{,} \PYG{n}{btype}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{output}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sos}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fs}\PYG{o}{=}\PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}    
\PYG{n}{w\PYGZus{}sos}\PYG{p}{,} \PYG{n}{h\PYGZus{}sos} \PYG{o}{=} \PYG{n}{sosfreqz}\PYG{p}{(}\PYG{n}{sos}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{ifft}\PYG{p}{(}\PYG{n}{h\PYGZus{}sos}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Amplitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 0, \PYGZsq{}Time\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_72_1}.png}

Now let’s apply the filter to our data. We will be applying the filter to the signal in the time domain using the \sphinxcode{\sphinxupquote{filtfilt}} function. This is a good default option, even though there are several other functions to apply the filter. \sphinxcode{\sphinxupquote{filtfilt}} applies the filter forward and then in reverse ensuring that there is zero\sphinxhyphen{}phase distortion.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{filtered} \PYG{o}{=} \PYG{n}{filtfilt}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{signal}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{filtered}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Original}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Filtered}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x7f95c9a465d0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_74_1}.png}


\subsubsection{Low Pass}
\label{\detokenize{content/Signal_Processing:low-pass}}
Low pass filters only retain low frequency signals, which \sphinxstyleemphasis{removes} any high frequency information.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{signal} \PYG{k+kn}{import} \PYG{n}{butter}\PYG{p}{,} \PYG{n}{filtfilt}

\PYG{n}{filter\PYGZus{}order} \PYG{o}{=} \PYG{l+m+mi}{2} 
\PYG{n}{frequency\PYGZus{}cutoff} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{sampling\PYGZus{}frequency} \PYG{o}{=} \PYG{l+m+mi}{500}

\PYG{c+c1}{\PYGZsh{} Create the filter}
\PYG{n}{b}\PYG{p}{,} \PYG{n}{a} \PYG{o}{=} \PYG{n}{butter}\PYG{p}{(}\PYG{n}{filter\PYGZus{}order}\PYG{p}{,} \PYG{n}{frequency\PYGZus{}cutoff}\PYG{p}{,} \PYG{n}{btype}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{output}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ba}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fs}\PYG{o}{=}\PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Apply the filter}
\PYG{n}{filtered} \PYG{o}{=} \PYG{n}{filtfilt}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{signal}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{filtered}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Original}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Filtered}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x7f95b9019c90\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_76_1}.png}

What does the filter look like?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{filter\PYGZus{}order} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{frequency\PYGZus{}cutoff} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{sampling\PYGZus{}frequency} \PYG{o}{=} \PYG{l+m+mi}{500}

\PYG{c+c1}{\PYGZsh{} Create the filter}
\PYG{n}{b}\PYG{p}{,} \PYG{n}{a} \PYG{o}{=} \PYG{n}{butter}\PYG{p}{(}\PYG{n}{filter\PYGZus{}order}\PYG{p}{,} \PYG{n}{frequency\PYGZus{}cutoff}\PYG{p}{,} \PYG{n}{btype}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{output}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ba}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fs}\PYG{o}{=}\PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}

\PYG{n}{plot\PYGZus{}filter}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_78_0}.png}


\subsubsection{Bandpass}
\label{\detokenize{content/Signal_Processing:bandpass}}
Bandpass filters permit retaining only a specific frequency. Morlet wavelets are an example of a bandpass filter.  or example a Morlet wavelet is a gaussian with the peak frequency at the center of a bandpass filter.

Let’s try selecting removing specific frequencies

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{filter\PYGZus{}order} \PYG{o}{=} \PYG{l+m+mi}{2} 
\PYG{n}{lowcut} \PYG{o}{=} \PYG{l+m+mi}{7}
\PYG{n}{highcut} \PYG{o}{=} \PYG{l+m+mi}{13}

\PYG{c+c1}{\PYGZsh{} Create the filter}
\PYG{n}{b}\PYG{p}{,} \PYG{n}{a} \PYG{o}{=} \PYG{n}{butter}\PYG{p}{(}\PYG{n}{filter\PYGZus{}order}\PYG{p}{,} \PYG{p}{[}\PYG{n}{lowcut}\PYG{p}{,} \PYG{n}{highcut}\PYG{p}{]}\PYG{p}{,} \PYG{n}{btype}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bandpass}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{output}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ba}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fs}\PYG{o}{=}\PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Apply the filter}
\PYG{n}{filtered} \PYG{o}{=} \PYG{n}{filtfilt}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{signal}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{filtered}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Original}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Filtered}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x7f95d8e26d50\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_80_1}.png}


\subsubsection{Band\sphinxhyphen{}Stop}
\label{\detokenize{content/Signal_Processing:band-stop}}
Bandstop filters remove a specific frequency from the signal

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{filter\PYGZus{}order} \PYG{o}{=} \PYG{l+m+mi}{2} 
\PYG{n}{lowcut} \PYG{o}{=} \PYG{l+m+mi}{8}
\PYG{n}{highcut} \PYG{o}{=} \PYG{l+m+mi}{12}

\PYG{c+c1}{\PYGZsh{} Create the filter}
\PYG{n}{b}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{butter}\PYG{p}{(}\PYG{n}{filter\PYGZus{}order}\PYG{p}{,} \PYG{p}{[}\PYG{n}{lowcut}\PYG{p}{,} \PYG{n}{highcut}\PYG{p}{]}\PYG{p}{,} \PYG{n}{btype}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bandstop}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{output}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ba}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fs}\PYG{o}{=}\PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the filter}
\PYG{n}{plot\PYGZus{}filter}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}frequency}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Apply the filter}
\PYG{n}{filtered} \PYG{o}{=} \PYG{n}{filtfilt}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{signal}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{signal}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{filtered}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Original}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Filtered}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x7f96088bf390\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Signal_Processing_82_1}.png}

\noindent\sphinxincludegraphics{{Signal_Processing_82_2}.png}


\subsection{Exercises}
\label{\detokenize{content/Signal_Processing:exercises}}

\subsubsection{Exercise 1. Create a simulated time series with 7 different frequencies with noise}
\label{\detokenize{content/Signal_Processing:exercise-1-create-a-simulated-time-series-with-7-different-frequencies-with-noise}}

\subsubsection{Exercise 2. Show that you can identify each signal using a FFT}
\label{\detokenize{content/Signal_Processing:exercise-2-show-that-you-can-identify-each-signal-using-a-fft}}

\subsubsection{Exercise 3. Remove one frequency with a bandstop filter}
\label{\detokenize{content/Signal_Processing:exercise-3-remove-one-frequency-with-a-bandstop-filter}}

\subsubsection{Exercise 4. Reconstruct the signal with the frequency removed and compare it to the original}
\label{\detokenize{content/Signal_Processing:exercise-4-reconstruct-the-signal-with-the-frequency-removed-and-compare-it-to-the-original}}

\section{Preprocessing}
\label{\detokenize{content/Preprocessing:preprocessing}}\label{\detokenize{content/Preprocessing::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

Being able to study brain activity associated with cognitive processes in humans is an amazing achievement. However, as we have noted throughout this course, there is an extraordinary amount of noise and a very low levels of signal, which makes it difficult to make inferences about the function of the brain using this BOLD imaging. A critical step before we can perform any analyses is to do our best to remove as much of the noise as possible. The series of steps to remove noise comprise our \sphinxstyleemphasis{neuroimaging data \sphinxstylestrong{preprocessing} pipeline}.

\sphinxincludegraphics{{preprocessing}.png}

In this lab, we will go over the basics of preprocessing fMRI data using the \sphinxhref{https://fmriprep.readthedocs.io/en/stable/}{fmriprep} preprocessing pipeline. We will cover:
\begin{itemize}
\item {} 
Image transformations

\item {} 
Head motion correction

\item {} 
Spatial Normalization

\item {} 
Spatial Smoothing

\end{itemize}

There are other preprocessing steps that are also common, but not necessarily performed by all labs such as slice timing and distortion correction. We will not be discussing these in depth outside of the videos.

Let’s start with watching a short video by Martin Lindquist to get a general overview of the main steps of preprocessing and the basics of how to transform images and register them to other images.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Qc3rRaJWOc4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Preprocessing_1_0}.jpg}


\subsection{Image Transformations}
\label{\detokenize{content/Preprocessing:image-transformations}}
Ok, now let’s dive deeper into how we can transform images into different spaces using linear transformations.

Recall from our introduction to neuroimaging data lab, that neuroimaging data is typically stored in a nifti container, which contains a 3D or 4D matrix of the voxel intensities and also an affine matrix, which provides instructions for how to transform the matrix into another space.

Let’s create an interactive plot using ipywidgets so that we can get an intuition for how these affine matrices can be used to transform a 3D image.

We can move the sliders to play with applying rigid body  transforms to a 3D cube. A rigid body transformation has 6 parameters: translation in x,y, \& z, and rotation around each of these axes. The key thing to remember is that a rigid body transform doesn’t allow the image to be fundamentally changed. A full 12 parameter affine transformation adds an additional 3 parameters each for scaling and shearing, which can change the shape of the cube.

Try moving some of the sliders around. Note that the viewer is a little slow. Each time you move a slider it is applying an affine transformation to the matrix and re\sphinxhyphen{}plotting.

Translation moves the cube in x, y, and z dimensions.

We can also rotate the cube around the x, y, and z axes where the origin is the center point. Continuing to rotate around the point will definitely lead to the cube leaving the current field of view, but it will come back if you keep rotating it.

You’ll notice that every time we change the slider and apply a new affine transformation that the cube gets a little distorted with aliasing. Often we need to interpolate the image after applying a transformation to fill in the gaps after applying a transformation. It is important to keep in mind that every time we apply an affine transformation to our images, it is actually not a perfect representation of the original data. Additional steps like reslicing, interpolation, and spatial smoothing can help with this.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{from} \PYG{n+nn}{mpl\PYGZus{}toolkits} \PYG{k+kn}{import} \PYG{n}{mplot3d}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{nibabel}\PYG{n+nn}{.}\PYG{n+nn}{affines} \PYG{k+kn}{import} \PYG{n}{apply\PYGZus{}affine}\PYG{p}{,} \PYG{n}{from\PYGZus{}matvec}\PYG{p}{,} \PYG{n}{to\PYGZus{}matvec}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{ndimage} \PYG{k+kn}{import} \PYG{n}{affine\PYGZus{}transform}\PYG{p}{,} \PYG{n}{map\PYGZus{}coordinates}
\PYG{k+kn}{import} \PYG{n+nn}{nibabel} \PYG{k}{as} \PYG{n+nn}{nib}
\PYG{k+kn}{from} \PYG{n+nn}{ipywidgets} \PYG{k+kn}{import} \PYG{n}{interact}\PYG{p}{,} \PYG{n}{FloatSlider}

\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}rigid\PYGZus{}body\PYGZus{}transformation}\PYG{p}{(}\PYG{n}{trans\PYGZus{}x}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{trans\PYGZus{}y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{trans\PYGZus{}z}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{rot\PYGZus{}x}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{rot\PYGZus{}y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{rot\PYGZus{}z}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}This plot creates an interactive demo to illustrate the parameters of a rigid body transformation\PYGZsq{}\PYGZsq{}\PYGZsq{}}
    \PYG{n}{fov} \PYG{o}{=} \PYG{l+m+mi}{30}
    \PYG{n}{radius} \PYG{o}{=} \PYG{l+m+mi}{10}
    \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{indices}\PYG{p}{(}\PYG{p}{(}\PYG{n}{fov}\PYG{p}{,} \PYG{n}{fov}\PYG{p}{,} \PYG{n}{fov}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{cube} \PYG{o}{=} \PYG{p}{(}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZgt{}} \PYG{n}{fov}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{radius}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZlt{}} \PYG{n}{fov}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{radius}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZgt{}} \PYG{n}{fov}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{radius}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZlt{}} \PYG{n}{fov}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{radius}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{p}{(}\PYG{n}{z} \PYG{o}{\PYGZgt{}} \PYG{n}{fov}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{radius}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{z} \PYG{o}{\PYGZlt{}} \PYG{n}{fov}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{radius}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{p}{)}\PYG{p}{)}
    \PYG{n}{cube} \PYG{o}{=} \PYG{n}{cube}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}

    \PYG{n}{vec} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{trans\PYGZus{}x}\PYG{p}{,} \PYG{n}{trans\PYGZus{}y}\PYG{p}{,} \PYG{n}{trans\PYGZus{}z}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{n}{rot\PYGZus{}x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{radians}\PYG{p}{(}\PYG{n}{rot\PYGZus{}x}\PYG{p}{)}
    \PYG{n}{rot\PYGZus{}y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{radians}\PYG{p}{(}\PYG{n}{rot\PYGZus{}y}\PYG{p}{)}
    \PYG{n}{rot\PYGZus{}z} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{radians}\PYG{p}{(}\PYG{n}{rot\PYGZus{}z}\PYG{p}{)}
    \PYG{n}{rot\PYGZus{}axis1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                         \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{rot\PYGZus{}x}\PYG{p}{)}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{rot\PYGZus{}x}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}
                         \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{rot\PYGZus{}x}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{rot\PYGZus{}x}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{rot\PYGZus{}axis2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{rot\PYGZus{}y}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{rot\PYGZus{}y}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}
                         \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                         \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{rot\PYGZus{}y}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{rot\PYGZus{}y}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{rot\PYGZus{}axis3} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{rot\PYGZus{}z}\PYG{p}{)}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{rot\PYGZus{}z}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                         \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{rot\PYGZus{}z}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{rot\PYGZus{}z}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                         \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{rotation} \PYG{o}{=} \PYG{n}{rot\PYGZus{}axis1} \PYG{o}{@} \PYG{n}{rot\PYGZus{}axis2} \PYG{o}{@} \PYG{n}{rot\PYGZus{}axis3}
    
    \PYG{n}{affine} \PYG{o}{=} \PYG{n}{from\PYGZus{}matvec}\PYG{p}{(}\PYG{n}{rotation}\PYG{p}{,} \PYG{n}{vec}\PYG{p}{)}
    
    \PYG{n}{i\PYGZus{}coords}\PYG{p}{,} \PYG{n}{j\PYGZus{}coords}\PYG{p}{,} \PYG{n}{k\PYGZus{}coords} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{cube}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{cube}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{cube}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{indexing}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ij}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{coordinate\PYGZus{}grid} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i\PYGZus{}coords}\PYG{p}{,} \PYG{n}{j\PYGZus{}coords}\PYG{p}{,} \PYG{n}{k\PYGZus{}coords}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{coords\PYGZus{}last} \PYG{o}{=} \PYG{n}{coordinate\PYGZus{}grid}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{transformed} \PYG{o}{=} \PYG{n}{apply\PYGZus{}affine}\PYG{p}{(}\PYG{n}{affine}\PYG{p}{,} \PYG{n}{coords\PYGZus{}last}\PYG{p}{)}
    \PYG{n}{coords\PYGZus{}first} \PYG{o}{=} \PYG{n}{transformed}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

    \PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axes}\PYG{p}{(}\PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{voxels}\PYG{p}{(}\PYG{n}{map\PYGZus{}coordinates}\PYG{p}{(}\PYG{n}{cube}\PYG{p}{,} \PYG{n}{coords\PYGZus{}first}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n}{interact}\PYG{p}{(}\PYG{n}{plot\PYGZus{}rigid\PYGZus{}body\PYGZus{}transformation}\PYG{p}{,} 
         \PYG{n}{trans\PYGZus{}x}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
         \PYG{n}{trans\PYGZus{}y}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
         \PYG{n}{trans\PYGZus{}z}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
         \PYG{n}{rot\PYGZus{}x}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{360}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,}
         \PYG{n}{rot\PYGZus{}y}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{360}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,}
         \PYG{n}{rot\PYGZus{}z}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{360}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatSlider(value=0.0, description=\PYGZsq{}trans\PYGZus{}x\PYGZsq{}, max=10.0, min=\PYGZhy{}10.0, step=1.0), FloatSlide…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}function \PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}.plot\PYGZus{}rigid\PYGZus{}body\PYGZus{}affine\PYGZus{}transformation(trans\PYGZus{}x=0, trans\PYGZus{}y=0, trans\PYGZus{}z=0, rot\PYGZus{}x=0, rot\PYGZus{}y=0, rot\PYGZus{}z=0)\PYGZgt{}
\end{sphinxVerbatim}

\sphinxincludegraphics{{Rigid_Body}.gif}

Ok, so what’s going on behind the sliders?

Let’s borrow some of the material available in the nibabel \sphinxhref{https://nipy.org/nibabel/coordinate\_systems.html}{documentation} to understand how these transformations work.

The affine matrix is a way to transform images between spaces. In general, we have some voxel space coordinate \((i, j, k)\), and we want to figure out how to remap this into a reference space coordinate \((x, y, z)\).

It can be useful to think of this as a coordinate transform function \(f\) that accepts a voxel coordinate in the original space as an \sphinxstyleemphasis{input} and returns a coordinate in the \sphinxstyleemphasis{output} reference space:
\begin{equation*}
\begin{split}(x, y, z) = f(i, j, k)\end{split}
\end{equation*}
In theory \(f\) could be a complicated non\sphinxhyphen{}linear function, but in practice we typically assume that the relationship between \((i, j, k)\) and \((x, y, z)\) is linear (or \sphinxstyleemphasis{affine}), and can be encoded with linear affine transformations comprising translations, rotations, and zooms.

Scaling (zooming) in three dimensions can be represented by a diagonal 3 by 3
matrix.  Here’s how to zoom the first dimension by \(p\), the second by \(q\) and
the third by \(r\) units:
\begin{equation*}
\begin{split}
\begin{bmatrix}
x\\
y\\
z
\end{bmatrix} 
\quad
=
\quad
\begin{bmatrix}
p & i\\
q & j\\
r & k
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
p & 0 & 0 \\
0 & q & 0 \\
0 & 0 & r
\end{bmatrix}
\quad
\begin{bmatrix}
i\\
j\\
k
\end{bmatrix}
\end{split}
\end{equation*}
A rotation in three dimensions can be represented as a 3 by 3 \sphinxstyleemphasis{rotation matrix} \sphinxhref{https://en.wikipedia.org/wiki/Rotation\_matrix}{wikipedia rotation matrix}. For example, here is a rotation by \(\theta\) radians around the third array axis:
\begin{equation*}
\begin{split}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
\cos(\theta) &  -\sin(\theta) & 0 \\
\sin(\theta) & \cos(\theta) & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
\quad
\begin{bmatrix}
i \\
j \\
k
\end{bmatrix}
\end{split}
\end{equation*}
This is a rotation by \(\phi\) radians around the second array axis:
\begin{equation*}
\begin{split}
\begin{bmatrix}
x \\
y \\
z \\
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
\cos(\phi) & 0 & \sin(\phi) \\
0 & 1 & 0 \\
-\sin(\phi) & 0 & \cos(\phi) \\
\end{bmatrix}
\quad
\begin{bmatrix}
i \\
j \\
k 
\end{bmatrix}
\end{split}
\end{equation*}
A rotation of \(\gamma\) radians around the first array axis:
\begin{equation*}
\begin{split}
\begin{bmatrix}
x\\
y\\
z
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
1 & 0 & 0 \\
0 & \cos(\gamma) & -\sin(\gamma) \\
0 & \sin(\gamma) & \cos(\gamma) \\
\end{bmatrix}
\quad
\begin{bmatrix}
i \\
j \\
k
\end{bmatrix}
\end{split}
\end{equation*}
Zoom and rotation matrices can be combined by matrix multiplication.

Here’s a scaling of \(p, q, r\) units followed by a rotation of \(\theta\) radians
around the third axis followed by a rotation of \(\phi\) radians around the
second axis:
\begin{equation*}
\begin{split}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
\cos(\phi) & 0 & \sin(\phi) \\
0 & 1 & 0 \\
-\sin(\phi) & 0 & \cos(\phi) \\
\end{bmatrix}
\quad
\begin{bmatrix}
\cos(\theta) &  -\sin(\theta) & 0 \\
\sin(\theta) & \cos(\theta) & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
\quad
\begin{bmatrix}
p & 0 & 0 \\
0 & q & 0 \\
0 & 0 & r \\
\end{bmatrix}
\quad
\begin{bmatrix}
i\\
j\\
k\\
\end{bmatrix}
\end{split}
\end{equation*}
This can also be written:
\begin{equation*}
\begin{split}
M
\quad
=
\quad
\begin{bmatrix}
\cos(\phi) & 0 & \sin(\phi) \\
0 & 1 & 0 \\
-\sin(\phi) & 0 & \cos(\phi) \\
\end{bmatrix}
\quad
\begin{bmatrix}
\cos(\theta) &  -\sin(\theta) & 0 \\
\sin(\theta) & \cos(\theta) & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
\quad
\begin{bmatrix}
p & 0 & 0 \\
0 & q & 0 \\
0 & 0 & r \\
\end{bmatrix}
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
\begin{bmatrix}
x \\
y \\
z 
\end{bmatrix}
\quad
=
\quad
M
\quad
\begin{bmatrix}
i \\
j \\
k
\end{bmatrix}
\end{split}
\end{equation*}
This might be obvious because the matrix multiplication is the result of
applying each transformation in turn on the coordinates output from the
previous transformation. Combining the transformations into a single matrix
\(M\) works because matrix multiplication is associative \textendash{} \(ABCD = (ABC)D\).

A translation in three dimensions can be represented as a length 3 vector to
be added to the length 3 coordinate.  For example, a translation of \(a\) units
on the first axis, \(b\) on the second and \(c\) on the third might be written
as:
\begin{equation*}
\begin{split}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
i \\
j \\
k
\end{bmatrix}
\quad
+
\quad
\begin{bmatrix}
a \\
b \\
c 
\end{bmatrix}
\end{split}
\end{equation*}
We can write our function \(f\) as a combination of matrix multiplication by some 3 by 3 rotation / zoom matrix \(M\) followed by addition of a 3 by 1 translation vector \((a, b, c)\)
\begin{equation*}
\begin{split}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
\quad
=
\quad
M
\quad
\begin{bmatrix}
i \\
j \\
k
\end{bmatrix}
\quad
+
\quad
\begin{bmatrix}
a \\
b \\
c
\end{bmatrix}
\end{split}
\end{equation*}
We could record the parameters necessary for \(f\) as the 3 by 3 matrix, \(M\)
and the 3 by 1 vector \((a, b, c)\).

In fact, the 4 by 4 image \sphinxstyleemphasis{affine array} includes this exact information. If \(m_{i,j}\) is the value in row \(i\) column \(j\) of matrix \(M\), then the image affine matrix \(A\) is:
\begin{equation*}
\begin{split}
A
\quad
=
\quad
\begin{bmatrix}
m_{1,1} & m_{1,2} & m_{1,3} & a \\
m_{2,1} & m_{2,2} & m_{2,3} & b \\
m_{3,1} & m_{3,2} & m_{3,3} & c \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
\end{split}
\end{equation*}
Why the extra row of \([0, 0, 0, 1]\)?  We need this row because we have rephrased the combination of rotations / zooms and translations as a transformation in \sphinxstyleemphasis{homogenous coordinates} (see \sphinxhref{https://en.wikipedia.org/wiki/Homogeneous\_coordinates}{wikipedia homogenous
coordinates}). This is a trick that allows us to put the translation part into the same matrix as the rotations / zooms, so that both translations and rotations / zooms can be applied by matrix multiplication. In order to make this work, we have to add an extra 1 to our input and output coordinate vectors:
\begin{equation*}
\begin{split}
\begin{bmatrix}
x \\
y \\
z \\
1
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
m_{1,1} & m_{1,2} & m_{1,3} & a \\
m_{2,1} & m_{2,2} & m_{2,3} & b \\
m_{3,1} & m_{3,2} & m_{3,3} & c \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
\quad
\begin{bmatrix}
i \\
j \\
k \\
1
\end{bmatrix}
\end{split}
\end{equation*}
This results in the same transformation as applying \(M\) and \((a, b, c)\) separately. One advantage of encoding transformations this way is that we can combine two sets of rotations, zooms, translations by matrix multiplication of the two corresponding affine matrices.

In practice, although it is common to combine 3D transformations using 4 x 4 affine matrices, we usually \sphinxstyleemphasis{apply} the transformations by breaking up the affine matrix into its component \(M\) matrix and \((a, b, c)\) vector and doing:
\begin{equation*}
\begin{split}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
\quad
=
\quad
M
\quad
\begin{bmatrix}
i \\
j \\
k
\end{bmatrix}
\quad
+
\quad
\begin{bmatrix}
a \\
b \\
c
\end{bmatrix}
\end{split}
\end{equation*}
As long as the last row of the 4 by 4 is \([0, 0, 0, 1]\), applying the transformations in this way is mathematically the same as using the full 4 by 4 form, without the inconvenience of adding the extra 1 to our input and output vectors.

You can think of the image affine as a combination of a series of transformations to go from voxel coordinates to mm coordinates in terms of the magnet isocenter. Here is the EPI affine broken down into a series of transformations, with the results shown on the localizer image:

\noindent\sphinxincludegraphics{{illustrating_affine}.png}

Applying different affine transformations allows us to rotate, reflect, scale, and shear the image.


\subsection{Cost Functions}
\label{\detokenize{content/Preprocessing:cost-functions}}
Now that we have learned how affine transformations can be applied to transform images into different spaces, how can we use this to register one brain image to another image?

The key is to identify a way to quantify how aligned the two images are to each other. Our visual systems are very good at identifying when two images are aligned, however, we need to create an alignment measure. These measures are often called \sphinxstyleemphasis{cost functions}.

There are many different types of cost functions depending on the types of images that are being aligned. For example, a common cost function is called minimizing the sum of the squared differences and is similar to how regression lines are fit to minimize deviations from the observed data. This measure works best if the images are of the same type and have roughly equivalent signal intensities.

Let’s create another interactive plot and find the optimal X \& Y translation parameters that minimize the difference between a two\sphinxhyphen{}dimensional target image to a reference image.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}affine\PYGZus{}cost}\PYG{p}{(}\PYG{n}{trans\PYGZus{}x}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{trans\PYGZus{}y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}This function creates an interactive demo to highlight how a cost function works in image registration.\PYGZsq{}\PYGZsq{}\PYGZsq{}}
    \PYG{n}{fov} \PYG{o}{=} \PYG{l+m+mi}{30}
    \PYG{n}{radius} \PYG{o}{=} \PYG{l+m+mi}{15}
    \PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{indices}\PYG{p}{(}\PYG{p}{(}\PYG{n}{fov}\PYG{p}{,} \PYG{n}{fov}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{square1} \PYG{o}{=} \PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZlt{}} \PYG{n}{radius}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZlt{}} \PYG{n}{radius}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{square2} \PYG{o}{=} \PYG{p}{(}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZgt{}} \PYG{n}{fov}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{radius}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZlt{}} \PYG{n}{fov}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{radius}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZgt{}} \PYG{n}{fov}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{radius}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZlt{}} \PYG{n}{fov}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{radius}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{square1} \PYG{o}{=} \PYG{n}{square1}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{)}
    \PYG{n}{square2} \PYG{o}{=} \PYG{n}{square2}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{)}

    \PYG{n}{vec} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{trans\PYGZus{}y}\PYG{p}{,} \PYG{n}{trans\PYGZus{}x}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{n}{affine} \PYG{o}{=} \PYG{n}{from\PYGZus{}matvec}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{eye}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vec}\PYG{p}{)}
    
    \PYG{n}{i\PYGZus{}coords}\PYG{p}{,} \PYG{n}{j\PYGZus{}coords} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{square1}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{square1}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{indexing}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ij}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{coordinate\PYGZus{}grid} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i\PYGZus{}coords}\PYG{p}{,} \PYG{n}{j\PYGZus{}coords}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{coords\PYGZus{}last} \PYG{o}{=} \PYG{n}{coordinate\PYGZus{}grid}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{transformed} \PYG{o}{=} \PYG{n}{apply\PYGZus{}affine}\PYG{p}{(}\PYG{n}{affine}\PYG{p}{,} \PYG{n}{coords\PYGZus{}last}\PYG{p}{)}
    \PYG{n}{coords\PYGZus{}first} \PYG{o}{=} \PYG{n}{transformed}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
    
    \PYG{n}{transformed\PYGZus{}square} \PYG{o}{=} \PYG{n}{map\PYGZus{}coordinates}\PYG{p}{(}\PYG{n}{square1}\PYG{p}{,} \PYG{n}{coords\PYGZus{}first}\PYG{p}{)}
    \PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{transformed\PYGZus{}square}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Target Image}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{square2}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Reference Image}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    
    \PYG{n}{point\PYGZus{}x} \PYG{o}{=} \PYG{n}{deepcopy}\PYG{p}{(}\PYG{n}{trans\PYGZus{}x}\PYG{p}{)}
    \PYG{n}{point\PYGZus{}y} \PYG{o}{=} \PYG{n}{deepcopy}\PYG{p}{(}\PYG{n}{trans\PYGZus{}y}\PYG{p}{)}
    \PYG{n}{sse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{n}{transformed\PYGZus{}square} \PYG{o}{\PYGZhy{}} \PYG{n}{square2}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{sse}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylim}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{350}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SSE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cost Function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Parameters: (}\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{trans\PYGZus{}x}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{,}\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{trans\PYGZus{}y}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
    
\PYG{n}{interact}\PYG{p}{(}\PYG{n}{plot\PYGZus{}affine\PYGZus{}cost}\PYG{p}{,} 
         \PYG{n}{trans\PYGZus{}x}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
         \PYG{n}{trans\PYGZus{}y}\PYG{o}{=}\PYG{n}{FloatSlider}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{min}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n+nb}{max}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatSlider(value=0.0, description=\PYGZsq{}trans\PYGZus{}x\PYGZsq{}, max=0.0, min=\PYGZhy{}30.0, step=1.0), FloatSlider…
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}function \PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}.plot\PYGZus{}affine\PYGZus{}cost(trans\PYGZus{}x=0, trans\PYGZus{}y=0)\PYGZgt{}
\end{sphinxVerbatim}

\sphinxincludegraphics{{Cost_Function}.gif}

You probably had to move the sliders around back and forth until you were able to reduce the sum of squared error to zero. This cost function increases exponentially the further you are away from your target. The process of minimizing (or sometimes maximizing) cost functions to identify the best fitting parameters is called \sphinxstyleemphasis{optimization} and is a concept that is core to fitting models to data across many different disciplines.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
Cost Function
&\sphinxstyletheadfamily 
Use Case
&\sphinxstyletheadfamily 
Example
\\
\hline
Sum of Squared Error
&
Images of same modality and scaling
&
Two T2* images
\\
\hline
Normalized correlation
&
Images of same modality
&
two T1 images
\\
\hline
Correlation ratio
&
Any modality
&
T1 and FLAIR
\\
\hline
Mutual information or normalized mutual information
&
Any modality
&
T1 and CT
\\
\hline
Boundary Based Registration
&
Images with some contrast across boundaries of interest
&
EPI and T1
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{Realignment}
\label{\detokenize{content/Preprocessing:realignment}}
Now let’s put everything we learned together to understand how we can correct for head motion in functional images that occurred during a scanning session. It is extremely important to make sure that a specific voxel has the same 3D coordinate across all time points to be able to model neural processes. This of course is made difficult by the fact that participants move during a scanning session and also in between runs.

Realignment is the preprocessing step in which a rigid body transformation is applied to each volume to align them to a common space. One typically needs to choose a reference volume, which might be the first, middle, or last volume, or the mean of all volumes.

Let’s look at an example of the translation and rotation parameters after running realignment on our first subject.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{bids} \PYG{k+kn}{import} \PYG{n}{BIDSLayout}\PYG{p}{,} \PYG{n}{BIDSValidator}
\PYG{k+kn}{import} \PYG{n+nn}{os}

\PYG{n}{data\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../data/localizer}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{layout} \PYG{o}{=} \PYG{n}{BIDSLayout}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{n}{derivatives}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{data}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{=}\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Translation (mm)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (TR)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Translation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}

\PYG{n}{data}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{=}\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Rotation (radian)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (TR)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Rotation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Rotation\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Preprocessing_11_1}.png}

Don’t forget that even though we can approximately put each volume into a similar position with realignment that head motion always distorts the magnetic field and can lead to nonlinear changes in signal intensity that will not be addressed by this procedure. In the resting\sphinxhyphen{}state literature, where many analyses are based on functional connectivity, head motion can lead to spurious correlations. Some researchers choose to exclude any subject that moved more than certain amount. Other’s choose to remove the impact of these time points in their data through removing the volumes via \sphinxstyleemphasis{scrubbing} or modeling out the volume with a dummy code in the first level general linear models.


\subsection{Spatial Normalization}
\label{\detokenize{content/Preprocessing:spatial-normalization}}
There are several other preprocessing steps that involve image registration. The main one is called \sphinxstyleemphasis{spatial normalization}, in which each subject’s brain data is warped into a common stereotactic space. Talaraich is an older space, that has been subsumed by various standards developed by the Montreal Neurological Institute.

There are a variety of algorithms to warp subject data into stereotactic space. Linear 12 parameter affine transformation have been increasingly been replaced by more complicated nonlinear normalizations that have hundreds to thousands of parameters.

One nonlinear algorithm that has performed very well across comparison studies is \sphinxstyleemphasis{diffeomorphic registration}, which can also be inverted so that subject space can be transformed into stereotactic space and back to subject space. This is the core of the \sphinxhref{http://stnava.github.io/ANTs/}{ANTs} algorithm that is implemented in fmriprep. See this \sphinxhref{https://elef.soic.indiana.edu/documentation/0.15.0.dev/examples\_built/syn\_registration\_2d/}{overview} for more details.

Let’s watch another short video by Martin Lindquist and Tor Wager to learn more about the core preprocessing steps.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{qamRGWSC\PYGZhy{}6g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Preprocessing_14_0}.jpg}

There are many different steps involved in the spatial normalization process and these details vary widely across various imaging software packages. We will briefly discuss some of the steps involved in the anatomical preprocessing pipeline implemented by fMRIprep and will be showing example figures from the output generated by the pipeline.

First, brains are extracted from the skull and surrounding dura mater. You can check and see how well the algorithm performed by examining the red outline.

\sphinxincludegraphics{{T1_normalization}.png}

Next, the anatomical images are segmented into different tissue types, these tissue maps are used for various types of analyses, including providing a grey matter mask to reduce the computational time in estimating statistics. In addition, they provide masks to aid in extracting average activity in CSF, or white matter, which might be used as covariates in the statistical analyses to account for physiological noise.
\sphinxincludegraphics{{T1_segmentation}.png}


\subsubsection{Spatial normalization of the anatomical T1w reference}
\label{\detokenize{content/Preprocessing:spatial-normalization-of-the-anatomical-t1w-reference}}
fmriprep uses the \sphinxhref{http://stnava.github.io/ANTs/}{ANTs} to perform nonlinear spatial normaliziation. It is easy to check to see how well the algorithm performed by viewing the results of aligning the T1w reference to the stereotactic reference space. Hover on the panels with the mouse pointer to transition between both spaces. We are using the MNI152NLin2009cAsym template.
\sphinxincludegraphics{{sub-S01_space-MNI152NLin2009cAsym_T1w}.svg}


\subsubsection{Alignment of functional and anatomical MRI data}
\label{\detokenize{content/Preprocessing:alignment-of-functional-and-anatomical-mri-data}}
Next, we can evaluate the quality of alignment of the functional data to the anatomical T1 image. FSL \sphinxcode{\sphinxupquote{flirt}} was used to generate transformations from EPI\sphinxhyphen{}space to T1w\sphinxhyphen{}space \sphinxhyphen{} The white matter mask calculated with FSL \sphinxcode{\sphinxupquote{fast}} (brain tissue segmentation) was used for BBR. Note that Nearest Neighbor interpolation is used in the reportlets in order to highlight potential spin\sphinxhyphen{}history and other artifacts, whereas final images are resampled using Lanczos interpolation. Notice these images are much blurrier and show some distortion compared to the T1s.
\sphinxincludegraphics{{sub-S01_task-localizer_desc-flirtbbr_bold}.svg}


\subsection{Spatial Smoothing}
\label{\detokenize{content/Preprocessing:spatial-smoothing}}
The last step we will cover in the preprocessing pipeline is \sphinxstyleemphasis{spatial smoothing}. This step involves applying a filter to the image, which removes high frequency spatial information. This step is identical to convolving a kernel to a 1\sphinxhyphen{}D signal that we covered in the {\hyperref[\detokenize{content/Signal_Processing::doc}]{\sphinxcrossref{\DUrole{doc}{Signal Processing Basics}}}} lab, but the kernel here is a 3\sphinxhyphen{}D Gaussian kernel. The amount of smoothing is determined by specifying the width of the distribution (i.e., the standard deviation) using the Full Width at Half Maximum (FWHM) parameter.

Why we would want to decrease our image resolution with spatial smoothing after we tried very hard to increase our resolution at the data acquisition stage? This is because this step may help increase the signal to noise ratio by reducing the impact of partial volume effects, residual anatomical differences following normalization, and other aliasing from applying spatial transformation.

Here is what a 3D gaussian kernel looks like.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}gaussian}\PYG{p}{(}\PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{surface}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{viridis}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}Generates a 3D matplotlib plot of a Gaussian distribution\PYGZsq{}\PYGZsq{}\PYGZsq{}}
    \PYG{n}{mean}\PYG{o}{=}\PYG{l+m+mi}{0}
    \PYG{n}{domain}\PYG{o}{=}\PYG{l+m+mi}{10}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{domain} \PYG{o}{+} \PYG{n}{mean}\PYG{p}{,} \PYG{n}{domain} \PYG{o}{+} \PYG{n}{mean}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{/}\PYG{l+m+mi}{10}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{domain} \PYG{o}{+} \PYG{n}{mean}\PYG{p}{,} \PYG{n}{domain} \PYG{o}{+} \PYG{n}{mean}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{/}\PYG{l+m+mi}{10}\PYG{p}{)}
    \PYG{n}{x}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}
    \PYG{n}{r} \PYG{o}{=} \PYG{p}{(}\PYG{n}{x} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{y} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{sigma} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{z} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{/} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{*} \PYG{n}{sigma} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{4}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{r}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{r}\PYG{p}{)}

    \PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}

    \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axes}\PYG{p}{(}\PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{kind}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wire}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
        \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot\PYGZus{}wireframe}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{n}{linewidth}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
    \PYG{k}{elif} \PYG{n}{kind}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{surface}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
        \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot\PYGZus{}surface}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{z}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{n}{linewidth}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n+nb+bp}{NotImplemented}
    
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{plot\PYGZus{}gaussian}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{surface}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Preprocessing_17_0}.png}


\subsection{fmriprep}
\label{\detokenize{content/Preprocessing:fmriprep}}
Throughout this lab and course, you have frequently heard about \sphinxhref{https://fmriprep.readthedocs.io/en/stable/}{fmriprep}, which is a functional magnetic resonance imaging (fMRI) data preprocessing pipeline that was developed by a team at the \sphinxhref{http://reproducibility.stanford.edu/}{Center for Reproducible Research} led by Russ Poldrack and Chris Gorgolewski. Fmriprep was designed to provide an easily accessible, state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art interface that is robust to variations in scan acquisition protocols, requires minimal user input, and provides easily interpretable and comprehensive error and output reporting. Fmriprep performs basic processing steps (coregistration, normalization, unwarping, noise component extraction, segmentation, skullstripping etc.) providing outputs that are ready for data analysis.

fmriprep was built on top of \sphinxhref{https://nipype.readthedocs.io/en/latest/}{nipype}, which is a tool to build preprocessing pipelines in python using graphs. This provides a completely flexible way to create custom pipelines using any type of software while also facilitating easy parallelization of steps across the pipeline on high performance computing platforms. Nipype is completely flexible, but has a fairly steep learning curve and is best for researchers who have strong opinions about how they want to preprocess their data, or are working with nonstandard data that might require adjusting the preprocessing steps or parameters. In practice, most researchers typically use similar preprocessing steps and do not need to tweak the pipelines very often. In addition, many researchers do not fully understand how each preprocessing step will impact their results and would prefer if somebody else picked suitable defaults based on current best practices in the literature. The fmriprep pipeline uses a combination of tools from well\sphinxhyphen{}known software packages, including FSL\_, ANTs\_, FreeSurfer\_ and AFNI\_. This pipeline was designed to provide the best software implementation for each state of preprocessing, and is quickly being updated as methods evolve and bugs are discovered by a growing user base.

This tool allows you to easily do the following:
\begin{itemize}
\item {} 
Take fMRI data from raw to fully preprocessed form.

\item {} 
Implement tools from different software packages.

\item {} 
Achieve optimal data processing quality by using the best tools available.

\item {} 
Generate preprocessing quality reports, with which the user can easily identify outliers.

\item {} 
Receive verbose output concerning the stage of preprocessing for each subject, including meaningful errors.

\item {} 
Automate and parallelize processing steps, which provides a significant speed\sphinxhyphen{}up from typical linear, manual processing.

\item {} 
More information and documentation can be found at https://fmriprep.readthedocs.io/

\end{itemize}

\sphinxincludegraphics{{fmriprep}.png}


\subsubsection{Running fmriprep}
\label{\detokenize{content/Preprocessing:running-fmriprep}}
Running fmriprep is a (mostly) trivial process of running a single line in the command line specifying a few choices and locations for the output data. One of the annoying things about older neuroimaging software that was developed by academics is that the packages were developed using many different development environments and on different operating systems (e.g., unix, windows, mac). It can be a nightmare getting some of these packages to install on more modern computing systems. As fmriprep uses many different packages, they have made it much easier to circumvent the time\sphinxhyphen{}consuming process of installing many different packages by releasing a \sphinxhref{https://fmriprep.readthedocs.io/en/stable/docker.html}{docker container} that contains everything you need to run the pipeline.

Unfortunately, our AWS cloud instances running our jupyter server are not equipped with enough computational resources to run fmriprep at this time. However, if you’re interested in running this on your local computer, here is the code you could use to run it in a jupyter notebook, or even better in the command line on a high performance computing environment.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
import os
base\PYGZus{}dir = \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/Data\PYGZsq{}
data\PYGZus{}path = os.path.join(base\PYGZus{}dir, \PYGZsq{}localizer\PYGZsq{})
output\PYGZus{}path = os.path.join(base\PYGZus{}dir, \PYGZsq{}preproc\PYGZsq{})
work\PYGZus{}path = os.path.join(base\PYGZus{}dir, \PYGZsq{}work\PYGZsq{})

sub = \PYGZsq{}S01\PYGZsq{}
subs = [f\PYGZsq{}S\PYGZob{}x:0\PYGZgt{}2d\PYGZcb{}\PYGZsq{} for x in range(10)]
for sub in subs:
    !fmriprep\PYGZhy{}docker \PYGZob{}data\PYGZus{}path\PYGZcb{} \PYGZob{}output\PYGZus{}path\PYGZcb{} participant \PYGZhy{}\PYGZhy{}participant\PYGZus{}label sub\PYGZhy{}\PYGZob{}sub\PYGZcb{} \PYGZhy{}\PYGZhy{}write\PYGZhy{}graph \PYGZhy{}\PYGZhy{}fs\PYGZhy{}no\PYGZhy{}reconall \PYGZhy{}\PYGZhy{}notrack \PYGZhy{}\PYGZhy{}fs\PYGZhy{}license\PYGZhy{}file \PYGZti{}/Dropbox/Dartbrains/License/license.txt \PYGZhy{}\PYGZhy{}work\PYGZhy{}dir \PYGZob{}work\PYGZus{}path\PYGZcb{}
\end{sphinxVerbatim}


\subsubsection{Quick primer on High Performance Computing}
\label{\detokenize{content/Preprocessing:quick-primer-on-high-performance-computing}}
We could run fmriprep on our computer, but this could take a long time if we have a lot of participants. Because we have a limited amount of computational resources on our laptops (e.g., cpus, and memory), we would have to run each participant sequentially. For example, if we had 50 participants, it would take 50 times longer to run all participants than a single one.

Imagine if you had 50 computers and ran each participant separate at the same time in parallel across all of the computers. This would allow us to run 50 participants in the same amount of time as a single participant. This is the basic idea behind high performance computing, which contains a cluster of many computers that have been installed in racks. Below is a picture of what Dartmouth’s \sphinxhref{https://rc.dartmouth.edu/index.php/discovery-overview/}{Discovery cluster} looks like:

\sphinxincludegraphics{{hpc}.png}

A cluster is simply a collection of nodes. A node can be thought of as an individual computer. Each node contains processors, which encompass multiple cores. Discovery contains 3000+ cores, which is certainly a lot more than your laptop!

In order to submit a job, you can create a Portable Batch System (PBS) script that sets up the parameters (e.g., how much time you want your script to run, specifying directory to run, etc) and submits your job to a queue.

\sphinxstylestrong{NOTE}: For this class, we will only be using the jupyterhub server, but if you end up working in a lab in the future, you will need to request access to the \sphinxstyleemphasis{discovery} system using this \sphinxhref{https://rcweb.dartmouth.edu/accounts/}{link}.


\subsubsection{fmriprep output}
\label{\detokenize{content/Preprocessing:fmriprep-output}}
You can see a summary of the operations fmriprep performed by examining the .html files in the \sphinxcode{\sphinxupquote{derivatives/fmriprep}} folder within the \sphinxcode{\sphinxupquote{localizer}} data directory.

We will load the first subject’s output file. Spend some time looking at the outputs and feel free to examine other subjects as well. Currently, the first 10 subjects should be available on the jupyterhub.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{HTML}

\PYG{n}{HTML}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub\PYGZhy{}S01.html}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.HTML object\PYGZgt{}
\end{sphinxVerbatim}


\subsection{Limitations of fmriprep}
\label{\detokenize{content/Preprocessing:limitations-of-fmriprep}}
In general, we recommend using this pipeline if you want a sensible default. Considerable thought has gone into selecting reasonable default parameters and selecting preprocessing steps based on best practices in the field (as determined by the developers). This is not necessarily the case for any of the default settings in any of the more conventional software packages (e.g., spm, fsl, afni, etc).

However, there is an important tradeoff in using this tool. On the one hand, it’s nice in that it is incredibly straightforward to use (one line of code!), has excellent documentation, and is actively being developed to fix bugs and improve the overall functionality. There is also a growing user base to ask questions.  \sphinxhref{https://neurostars.org/}{Neurostars} is an excellent forum to post questions and learn from others. On the other hand, fmriprep, is unfortunately in its current state not easily customizable. If you disagree with the developers about the order or specific preprocessing steps, it is very difficult to modify. Future versions will hopefully be more modular and easier to make custom pipelines.  If you need this type of customizability we strongly recommend using nipype over fmriprep.

In practice, it’s alway a little bit finicky to get everything set up on a particular system. Sometimes you might run into issues with a specific missing file like the \sphinxhref{https://fmriprep.readthedocs.io/en/stable/usage.html\#the-freesurfer-license}{freesurfer license} even if you’re not using it. You might also run into issues with the format of the data that might have some conflicts with the \sphinxhref{https://github.com/bids-standard/bids-validator}{bids\sphinxhyphen{}validator}. In our experience, there is always some frustrations getting this to work, but it’s very nice once it’s done.


\subsection{Exercises}
\label{\detokenize{content/Preprocessing:exercises}}

\subsubsection{Exercise 1. Inspect HTML output of other participants.}
\label{\detokenize{content/Preprocessing:exercise-1-inspect-html-output-of-other-participants}}
For this exercise, you will need to navigate to the derivatives folder containing the fmriprep preprocessed data \sphinxcode{\sphinxupquote{../data/data/localizer/derivatives/fmriprep}} and inspect the html output of other subjects (ie., not ‘S01’). Did the preprocessing steps works? are there any issues with the data that we should be concerned about?


\section{Introduction to the General Linear Model}
\label{\detokenize{content/GLM:introduction-to-the-general-linear-model}}\label{\detokenize{content/GLM::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

This tutorial provides an introduction for how the general linear model (GLM) can be used to make inferences about brain responses in a single subject. We will explore the statistics in the context of a simple hypothetical experiment using simulated data.

In this lab we will cover:
\begin{itemize}
\item {} 
How to use a GLM to test psychological hypotheses.

\item {} 
Simulating brain data

\item {} 
Estimating GLM using ordinary least squares

\item {} 
Calculating Standard Errors

\item {} 
Contrast Basics

\end{itemize}

Let’s start by watching two short videos introducing the general linear model by Tor Wager and how this can be applied to fMRI.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GDkLQuV4he4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_1_0}.jpg}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OyLKMb9FNhg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_2_0}.jpg}


\subsection{Are you ready for this?}
\label{\detokenize{content/GLM:are-you-ready-for-this}}
This lab assumes that you have some basic background knowledge in statistics from an introductory course. If you are already feeling overwhelmed from Tor Wager’s videos and think you might need to slow down and refresh some basic concepts and lingo, I highly encourage you to watch Jeannette Mumford’s crash course in statistics. These are certainly not required, but she is a wonderful teacher and watching her videos will provide an additional explanation of the core concepts needed to understand the GLM. You could watch these in one sitting, or go back and forth with working through the notebooks. There is so much to know in statistics and people can often feel lost because the concepts are certainly not intuitive. For example, even though advanced statistics have been an important part of my own work, I still find it helpful to periodically revisit core concepts. In general, I find that learning neuroimaging is an iterative process. In the beginning, it is important to get a broad understanding of the key steps and how neuroimaging can be used to make inferences, but as you progress in your training you will have plenty of opportunities to zoom into specific steps to learn more about particular details and nuances that you may not have fully appreciated the first time around.
\begin{itemize}
\item {} 
\sphinxhref{https://youtu.be/apt8uAgtgdY}{Basic statistics terminology} This video gently introduces some of the key concepts that provide the foundation for statistics.

\item {} 
\sphinxhref{https://www.youtube.com/watch?v=yLgPpmXVVbs}{Simple Linear Regression} This video explains how a regression works using a single variable.

\item {} 
\sphinxhref{https://www.youtube.com/watch?v=fkZj8QoYjq8}{Matrix Algebra Basics} This video provides the background linear algebra needed for understanding the GLM.

\item {} 
\sphinxhref{https://www.youtube.com/watch?v=qdOG7YMolmA}{Multiple Linear Regression} This video explains how multiple regression works using linear algebra.

\item {} 
\sphinxhref{https://www.youtube.com/watch?v=ULeg3DH3g9w}{Hypothesis Testing} This video covers the basics of hypothesis testing.

\item {} 
\sphinxhref{https://www.youtube.com/watch?v=yLgPpmXVVbs\&t=631s}{Contrasts in Linear Models} This video provides an overview of how to test hypotheses using contrasts in the context of the GLM.

\item {} 
\sphinxhref{https://www.youtube.com/watch?v=uClfe4pLrCo}{Intepreting Regression Parameters} This video covers how to interpret the results from a regression analysis.

\item {} 
\sphinxhref{https://www.youtube.com/watch?v=K4S576j90N8}{Mean Centering Regressors} This video covers a more subtle detail of why you might consider mean centering your continuour regression variables.

\end{itemize}

Ok, let’s get started. First, we will need to import all of the modules used in this tutorial.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{regress}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{external} \PYG{k+kn}{import} \PYG{n}{glover\PYGZus{}hrf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear\PYGZus{}model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear\PYGZus{}model. Anything that cannot be imported from sklearn.linear\PYGZus{}model is now part of the private API.
  warnings.warn(message, FutureWarning)
\end{sphinxVerbatim}


\subsection{Simulate a voxel time course}
\label{\detokenize{content/GLM:simulate-a-voxel-time-course}}
To generate an intuition for how we use the GLM to make inferences in fMRI data analysis, we will simulate a time series for a single voxel. A simulation means that we will be generating synthetic data that will resemble real data. However, because we know the ground truth of the signal, we can evaluate how well we can recover the true signal using a general linear model. Throughout this course, we frequently rely on simulations to gain an intuition for how a particular preprocessing step or statistic works. This is important because it reinforces the assumptions behind the operation (which are rarely met in real data), and also provides a method to learn how to answer your own questions by generating your own simulations.

Imagine that we are interested in identifying which region of the brain is involved in processing faces. To explore this question, we could show participants a bunch of different types of faces. Each presentation of a face will be a \sphinxstyleemphasis{trial}. Let’s simulate what a design might look like with 5 face trials.

First, we will need to specify the number of volumes in the time series. Then we need to specify the timepoint, in which a face is presented.

\sphinxincludegraphics{{faces}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}tr} \PYG{o}{=} \PYG{l+m+mi}{200}
\PYG{n}{n\PYGZus{}trial} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{face} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
\PYG{n}{face}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{n\PYGZus{}tr}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{o}{/}\PYG{n}{n\PYGZus{}trial}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}Plot a timeseries}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    Args:}
\PYG{l+s+sd}{        data: (np.ndarray) signal varying over time, where each column is a different signal.}
\PYG{l+s+sd}{        labels: (list) labels which need to correspond to the number of columns.}
\PYG{l+s+sd}{        linewidth: (int) thickness of line}
\PYG{l+s+sd}{    \PYGZsq{}\PYGZsq{}\PYGZsq{}}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{n}{linewidth}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{labels} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{)} \PYG{o}{!=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}
            \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Need to have the same number of labels as columns in data.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    
\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{face}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_7_0}.png}

We now have 5 events where a face is shown for 2 seconds (i.e., one TR). If we scanned someone with this design, we might expect to see any region involved in processing faces increase in activation around the time of the face presentation. How would we know which of these regions, if any, \sphinxstyleemphasis{selectively} process faces? Many of the regions we would observe are likely involved in processing \sphinxstyleemphasis{any} visual stimulus, and not specifically faces.

To rule out this potential confound, we would need at least one other condition that would serve as a visual control. Something that might have similar properties to a face, but isn’t a face.

One possibility is to create a visual stimulus that has all of the same visual properties in terms of luminance and color, but no longer resembles a face. Here is an example of the same faces that have been Fourier transformed, phase\sphinxhyphen{}scrambled, and inverse Fourier transformed. These pictures have essentially identical low level visual properties, but are clearly not faces.

\sphinxincludegraphics{{phase_scrambled}.png}

However, one might argue that faces are a type of object, and regions that are involved in higher visual processing such as object recognition might not be selective to processing faces. To rule out this possibility, we would need to add an additional visual control such as objects.

\sphinxincludegraphics{{objects}.png}

Both of these conditions could serve as a different type of visual control. To keep things simple, let’s start with pictures of objects as it controls for low level visual features, but also more complex object processing.

To demonstrate that a region is processing faces and not simply lower level visual properties or objects more generally, we can search for regions that are selectively more activated in response to viewing faces relative to objects. This is called a \sphinxstyleemphasis{contrast} and is the basic principle of the subtraction method for controlling for potential experimental confounds. Because BOLD fMRI is a relative and not absolute measure of brain activity, the subtraction method is a key aspect of experimental design.

Figures are from Huettel, Song, \& McCarthy (2008)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}tr} \PYG{o}{=} \PYG{l+m+mi}{200}
\PYG{n}{n\PYGZus{}trial} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{face} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
\PYG{n}{face}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{n\PYGZus{}tr}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{o}{/}\PYG{n}{n\PYGZus{}trial}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{obj} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
\PYG{n}{obj}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{n\PYGZus{}tr}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{o}{/}\PYG{n}{n\PYGZus{}trial}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{voxel} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{face}\PYG{p}{,}\PYG{n}{obj}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}

\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{voxel}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Face}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Object}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_9_0}.png}

Let’s imagine that in a voxel processing face specific information we might expect to see a larger activation in response to faces. Maybe two times bigger?

In our simulation, these two values are parameters we are specifying to generate the data. Specifically they refer to the amplitude of the response to Faces and Houses within a particular region of the brain.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}tr} \PYG{o}{=} \PYG{l+m+mi}{200}
\PYG{n}{n\PYGZus{}trial} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{face\PYGZus{}intensity} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{n}{object\PYGZus{}intensity} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{n}{face} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
\PYG{n}{face}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{n\PYGZus{}tr}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{o}{/}\PYG{n}{n\PYGZus{}trial}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{n}{face\PYGZus{}intensity}
\PYG{n}{obj} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
\PYG{n}{obj}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{n\PYGZus{}tr}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{o}{/}\PYG{n}{n\PYGZus{}trial}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{n}{object\PYGZus{}intensity}
\PYG{n}{voxel} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{face}\PYG{p}{,}\PYG{n}{obj}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}

\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{voxel}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Face}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Object}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_11_0}.png}

Ok, now we have two conditions that are alternating over time.

We know that the brain has a delayed hemodynamic response to events that has a particular shape, so we will need to convolve these events with an appropriate HRF function. Here, we will use the double\sphinxhyphen{}gamma HRF function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tr} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{n}{hrf} \PYG{o}{=} \PYG{n}{glover\PYGZus{}hrf}\PYG{p}{(}\PYG{n}{tr}\PYG{p}{,} \PYG{n}{oversampling}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{hrf}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 0, \PYGZsq{}Time\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_13_1}.png}

We will use \sphinxcode{\sphinxupquote{np.convolve}} from numpy to perform the convolution.  The length of the convolved data will be the length of the time series plus the length of the kernel minus 1. To make sure everything is the same length, we will chop off the extra time off the convolved time series using \sphinxcode{\sphinxupquote{mode=\textquotesingle{}same\textquotesingle{}}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{face\PYGZus{}conv} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{face}\PYG{p}{,} \PYG{n}{hrf}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{obj\PYGZus{}conv} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{obj}\PYG{p}{,} \PYG{n}{hrf}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{voxel\PYGZus{}conv} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{face\PYGZus{}conv}\PYG{p}{,} \PYG{n}{obj\PYGZus{}conv}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}

\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{voxel\PYGZus{}conv}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Face}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Object}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_15_0}.png}

While this might reflect the expected HRF response to a single event, real data is much noiser. It is easy to add different types of noise. For example, there might be a low frequency drift, autocorrelation, or possibly some aliased physiological artifacts.

For now, let’s start with something simple, like independent white noise drawn from a random Gaussian distribution
\begin{equation*}
\begin{split}\epsilon \sim \mathcal{N}(\mu,\,\sigma^{2})\end{split}
\end{equation*}
where \(\mu = 0\) and \(\sigma = 0.15\)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sigma} \PYG{o}{=} \PYG{l+m+mf}{0.15}
\PYG{n}{epsilon} \PYG{o}{=} \PYG{n}{sigma}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{voxel\PYGZus{}conv\PYGZus{}noise} \PYG{o}{=} \PYG{n}{voxel\PYGZus{}conv} \PYG{o}{+} \PYG{n}{epsilon}

\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{voxel\PYGZus{}conv\PYGZus{}noise}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Face}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Object}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_17_0}.png}

Now this is looking much more like real BOLD activity.

Remember, the goal of this exercise is to generate simulated activity from a voxel. If we were to extract signal from a specific voxel we wouldn’t know which condition was which, so let’s combine these two signals into a single simulated voxel timeseries by adding the two vectors together with the \sphinxcode{\sphinxupquote{.sum()}} method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Y} \PYG{o}{=} \PYG{n}{voxel\PYGZus{}conv\PYGZus{}noise}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_19_0}.png}


\subsection{Construct Design Matrix}
\label{\detokenize{content/GLM:construct-design-matrix}}
Now that we have our simulated voxel timeseries, let’s try and see if we can recover the original signal using a general linear model in the form of:
\begin{equation*}
\begin{split}Y = X\beta + \epsilon\end{split}
\end{equation*}
where \(Y\) is our observed voxel time series. \(X\) is our model or design matrix, and is where we will specify a predicted response to each condition. \(\beta\) is a vector of values that we will estimate to scale our model. \(\epsilon\) is independent gaussian noise. This model is linear because we can decompose \(Y\) into a set of features or independent variables that are scaled by an estimated \(\beta\) parameter and summed together. The \(\epsilon\) parameter is not usually known and can also be estimated.

You may be wondering how our model is distinct from our simulated data. Remember when we simulated the data we specified 3 parameters \sphinxhyphen{} face amplitude, object amplitude, and \(\epsilon\), we could have also added a mean, but for now, let’s just assume that it is zero. When we fit our model to the simulated data, we should in theory be able to almost perfectly recover these three parameters.

Now let’s build a design matrix \(X\) using an intercept, and a regressor indicating the onset of each condition, convolved with the hemodynamic response function (HRF).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}tr} \PYG{o}{=} \PYG{l+m+mi}{200}
\PYG{n}{n\PYGZus{}trial} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{face} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
\PYG{n}{face}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{n\PYGZus{}tr}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{o}{/}\PYG{n}{n\PYGZus{}trial}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{obj} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
\PYG{n}{obj}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{n\PYGZus{}tr}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{o}{/}\PYG{n}{n\PYGZus{}trial}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{intercept} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{intercept}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{face}\PYG{p}{,} \PYG{n}{hrf}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{obj}\PYG{p}{,} \PYG{n}{hrf}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}

\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_22_0}.png}

We can write our model out so that it is very clear what we are doing.
\begin{equation*}
\begin{split}Voxel = \beta_0 \cdot Intercept + \beta_1 \cdot Faces + \beta_2 \cdot Objects + \epsilon\end{split}
\end{equation*}
We can also make a plot and rotate the timeseries, to better reflect the equation.

It should be clear how each of these components relate to the regression equation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f}\PYG{p}{,} \PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sharey}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intercept}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Faces}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Objects}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{gca}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{invert\PYGZus{}yaxis}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_24_0}.png}


\subsection{Estimate GLM}
\label{\detokenize{content/GLM:estimate-glm}}
Now that have created our simulated voxel timeseries \(Y\) and our design matrix \(X\), we need to fit our model to the data by estimating the three \(\beta\) parameters.

There are several ways to estimate the parameters for our general linear model. The Ordinary Least Squares (OLS) estimator finds the \(\hat\beta\) hyperplane that minimizes the error between the observed \(Y\) and predicted \(\hat Y\).

This can be formulated using linear algebra as:
\begin{equation*}
\begin{split}\hat{\beta} = (X^T X)^{-1}X^TY\end{split}
\end{equation*}
There is also maximum likelihood estimator, which should produce an almost identical result to the ordinary least squares estimator when the error terms are normally distributed.
\begin{equation*}
\begin{split}L(\beta, \sigma^2 | Y, X) = \displaystyle \prod_{i=1}^{n}\frac{1}{\sqrt(2\pi\sigma^2)} \cdot e^{-\frac{(Y_i - \beta X_i)^2}{2\sigma^2}}\end{split}
\end{equation*}
where
\begin{equation*}
\begin{split}\mathcal{N}(0,\sigma^{2})\end{split}
\end{equation*}
For this class, we will primarily be focusing on the Ordinary Least Squares Estimator. In fact, just to demonstrate that the math is actually relatively straightforward, we will write our own function for the estimator using the linear algebra formulation. In practice, we typically will use a premade function, which is usually slightly more computationally efficient and will also calculate standard errors, etc.

For a more in depth overview of GLM estimation, watch this \sphinxhref{https://www.youtube.com/watch?v=Ab-5AbJ8gAs}{video} by Tor Wager and Martin Lindquist.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{ols\PYGZus{}estimator}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{pinv}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{)}

\PYG{n}{beta} \PYG{o}{=} \PYG{n}{ols\PYGZus{}estimator}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intercept}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Faces}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Objects}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{beta}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Regressor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Beta Value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{beta Faces \PYGZhy{} beta Objects: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
beta Faces \PYGZhy{} beta Objects: 0.95
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_26_1}.png}

We can see that our model is working pretty well. We did not add a mean to the simulated timeseries, so our estimator correctly figures out that the intercept parameter should be zero. The model also correctly figured out that the scaling parameter for the faces regressor was 2, and 1 for the objects regressor, with the difference between them equal to approximately 1.

Another way to evaluate how well our model is working is to plot our predicted \(\hat Y\) on top of our simulated \(Y\).

We can quantify the degree to which our model is accurately predicting the observed data by calculating the residual.
\begin{equation*}
\begin{split}residual = Y - \hat Y\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{predicted\PYGZus{}y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{beta}\PYG{p}{)}

\PYG{n}{predicted\PYGZus{}ts} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}y}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}

\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{predicted\PYGZus{}ts}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Simulated Voxel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted Voxel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{residual} \PYG{o}{=} \PYG{n}{Y} \PYG{o}{\PYGZhy{}} \PYG{n}{predicted\PYGZus{}y}

\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{residual}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residual}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Residual\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_28_1}.png}

\noindent\sphinxincludegraphics{{GLM_28_2}.png}


\subsubsection{Standard Errors}
\label{\detokenize{content/GLM:standard-errors}}
As you can see, we are doing a reasonable job recovering the original signals.

You may recall that we specified 3 parameters in our simulation
\begin{itemize}
\item {} 
a \(\beta\) weight for faces

\item {} 
a \(\beta\) weight for objects

\item {} 
an \(\epsilon\) noise parameter.

\end{itemize}

The \sphinxstyleemphasis{standard error of the estimate} refers to the standard deviation of the residual.

Formally, this can be described as:
\begin{equation*}
\begin{split}\hat \sigma = \sqrt{\frac{\displaystyle \sum_i^n(\hat Y_i - Y_i)^2}{n-k}}\end{split}
\end{equation*}
where \(n\) is the number of observations and \(k\) is the total number of regressors.

This number is essentially an estimate of the overall amount of error in the model or \(\epsilon\). This error is assumed to be independent and normally distributed. The smaller the residual variance \(\hat\sigma\) the better the fit of the model.

As you can see, the parameter is close, but slightly higher than the one we simulated.  This might be because we have relatively little data in our simulation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{standard\PYGZus{}error\PYGZus{}of\PYGZus{}estimate} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{residual}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Standard Error of the Estimate: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{standard\PYGZus{}error\PYGZus{}of\PYGZus{}estimate}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{residual}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Residual Error}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prediction Error}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Standard Error of the Estimate: 0.2
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 0, \PYGZsq{}Prediction Error\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_30_2}.png}


\subsubsection{Explained Variance}
\label{\detokenize{content/GLM:explained-variance}}
Sometimes we want a single metric to quantify overall how well our model was able to explain variance in the data. There are many metrics that can provide a quantitative measure of \sphinxstyleemphasis{goodness of fit}.

Here we will calculate \(R^2\) using the following formula:
\begin{equation*}
\begin{split}R^2 = 1 - \frac{\displaystyle \sum_i^n(\hat y_i - y_i)^2}{\displaystyle \sum_i^n(y_i - \bar y)^2}\end{split}
\end{equation*}
where \(y_i\) is the measured value of the voxel at timepoint \(i\), \(\hat y_i\) is the predicted value for time point \(i\), and \(\bar y\) is the mean of the measured voxel timeseries.

\(R^2\) will lie on the interval between \([0,1]\) and can be interpreted as percentage of the total variance in \(Y\) explained by the model, \(X\), where 1 is 100\% and 0 is none.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{r\PYGZus{}square}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}y}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{SS\PYGZus{}total} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{n}{Y} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{SS\PYGZus{}residual} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{n}{Y} \PYG{o}{\PYGZhy{}} \PYG{n}{predicted\PYGZus{}y}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{SS\PYGZus{}residual}\PYG{o}{/}\PYG{n}{SS\PYGZus{}total}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{R\PYGZca{}2: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{r\PYGZus{}square}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}y}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
R\PYGZca{}2: 0.6
\end{sphinxVerbatim}


\subsubsection{Standard Error of \protect\(\beta\protect\) estimates}
\label{\detokenize{content/GLM:standard-error-of-beta-estimates}}
We can also estimate the uncertainty of regression coefficients. The uncertainty of the beta parameters is quantified as a standard error around each specific estimate.
\begin{equation*}
\begin{split}\sigma = \sqrt{diag((X^TX)^{-1})} \cdot \hat \sigma\end{split}
\end{equation*}
This is essentially a confidence interval around the \(\beta_j\) estimate. One standard error, \(1*\hat \sigma\) is approximately equivalent to a 68\% confidence interval, while \(2*\hat\sigma\) is approximately a 95\% confidence interval.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{std\PYGZus{}error} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{pinv}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)} \PYG{o}{*} \PYG{n}{standard\PYGZus{}error\PYGZus{}of\PYGZus{}estimate}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intercept}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Faces}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Objects}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{beta}\PYG{p}{,} \PYG{n}{yerr} \PYG{o}{=} \PYG{n}{standard\PYGZus{}error\PYGZus{}of\PYGZus{}estimate}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Regressor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Beta Value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Beta Value\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_35_1}.png}


\subsubsection{Statistical Significance}
\label{\detokenize{content/GLM:statistical-significance}}
We could also perform a hypothesis test to evaluate if any of the regressors are statistically different from zero.

This exercise is simply meant to provide parallels to common statistical jargon. In practice, this is actually rarely done in neuroimaging analysis as we are typically more interested in making statistical inferences across the population rather than within a single participant.

The formula for calculating a t\sphinxhyphen{}statistic is very simple:
\begin{equation*}
\begin{split}t = \frac{\hat \beta_j}{\hat \sigma_j}\end{split}
\end{equation*}
where \(\beta_j\) refers to the estimated parameter for a regressor \(j\), and \(\sigma_j\) refers to the standard error of regressor \(j\).

\(t\) values that are more than 2 standard errors away from zero are called \sphinxstyleemphasis{statistically significant}, which basically just means we are more confident that the estimate is stable and not just an artifact of small sample size. In general, we don’t recommend reading too much into significance for individual \(\beta\) estimates in single subject fMRI analysis.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{t} \PYG{o}{=} \PYG{n}{beta}\PYG{o}{/}\PYG{n}{std\PYGZus{}error}
\PYG{n}{t}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZhy{}0.65716867, 15.83929013,  8.23580115])
\end{sphinxVerbatim}

Just like in intro statistics, we could find the p\sphinxhyphen{}value that corresponds to a particular t\sphinxhyphen{}statistic using the t\sphinxhyphen{}distribution. We will load the t distribution from \sphinxcode{\sphinxupquote{scipy.stats}} and calculate the corresponding p\sphinxhyphen{}values using the survival function or  \(1- cdf\), which requires specifying the degrees of freedom (df), which is \(n-1\). We multiply these values by 2 to calculated a two\sphinxhyphen{}tailed test.

You can see that the intercept \(\beta\) is not significant, but the face and object regressors are well below \sphinxcode{\sphinxupquote{p \textless{} 0.05}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy} \PYG{k+kn}{import} \PYG{n}{stats}

\PYG{n}{p} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{t}\PYG{o}{.}\PYG{n}{sf}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{t}\PYG{p}{)}\PYG{p}{,} \PYG{n}{n\PYGZus{}tr}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{2}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intercept}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Face}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Object}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{p}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p\PYGZhy{}values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Regressor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hlines}\PYG{p}{(}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{linestyles}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dashed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[5.11831798e\PYGZhy{}01 4.26134329e\PYGZhy{}37 2.33906074e\PYGZhy{}14]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.collections.LineCollection at 0x7fe020f135d0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_39_2}.png}


\subsubsection{Contrasts}
\label{\detokenize{content/GLM:contrasts}}
Contrasts are a very important concept in fMRI data analysis as they provide the statistical inference underlying the subtraction method of making inferences.

Let’s watch a short video by Tor Wager on contrasts. We will also spend much more time on contrasts in the group analysis tutorial. We also recommend watching Jeannette Mumford’s \sphinxhref{https://www.youtube.com/watch?v=yLgPpmXVVbs}{overview} of contrasts for a more statistical perspective.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{7MibM1ATai4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_41_0}.jpg}

Contrasts describe a linear combination of variables in a regression model whose coefficients add up to zero. This allows us to flexibly compare different experimental conditions.

For example, suppose we just wanted to know the magnitude of an effect for a single condition, such as the brain response to faces. We would create a contrast code that isolates the effect size (i.e., \(\beta\) estimate for the face regressor)

If our GLM, was:
\begin{equation*}
\begin{split}Y = \beta_0 \cdot Intercept + \beta_1 \cdot Faces + \beta_2 \cdot Objects\end{split}
\end{equation*}
then, the corresponding contrast code or vector for faces would be:

{[}0, 1, 0{]}

The contrast code for the object condition would be:

{[}0, 0, 1{]}

and importantly the contrast \sphinxstyleemphasis{between} the face and object condition would be:

{[}0, 1, \sphinxhyphen{}1{]}

More simply, we are calculating the magnitude of the effect of the difference between viewing faces and objects in a single voxel.

To make this a little bit more clear, we will show a graphical representation of the design matrix to make it obvious what we are contrasting.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}

\PYG{n}{c1} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{c2} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{c3} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{p}{\PYGZob{}}\PYG{p}{(}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{c}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{beta}\PYG{p}{,} \PYG{n}{c}\PYG{p}{)}\PYG{p}{)} \PYG{k}{for} \PYG{n}{c} \PYG{o+ow}{in} \PYG{p}{[}\PYG{n}{c1}\PYG{p}{,} \PYG{n}{c2}\PYG{p}{,} \PYG{n}{c3}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}(\PYGZsq{}[0, 0, 1]\PYGZsq{}, 1.0721521304389894),
 (\PYGZsq{}[0, 1, \PYGZhy{}1]\PYGZsq{}, 0.8735019759362619),
 (\PYGZsq{}[0, 1, 0]\PYGZsq{}, 1.9456541063752513)\PYGZcb{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_43_1}.png}


\subsubsection{Efficiency}
\label{\detokenize{content/GLM:efficiency}}
We can estimate the efficiency, or the quality of an estimator for a specific experimental design or a hypothesis testing procedure. Efficiency is related to power, or the ability to detect an effect should one exist. However, unlike power, we can estimate efficiency from our design matrix and do not actually need to know the standard error for the model (unlike with power calculations). Specifically, efficiency is defined as the inverse of the sum of the estimator variances. For a more detailed explanation and general experimental design recommendations see this \sphinxhref{http://imaging.mrc-cbu.cam.ac.uk/imaging/DesignEfficiency}{overview} by Rik Henson, or this \sphinxhref{https://theclevermachine.wordpress.com/tag/fmri-design-efficiency/}{blog post} on efficiency in experimental designs.
\begin{equation*}
\begin{split}e(c\hat\beta) = \frac{1}{c(X^TX)^{-1}c^T}\end{split}
\end{equation*}
Reducing collinearity or covariance between regressors can increase design efficiency

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{contrast\PYGZus{}efficiency}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{contrast}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{c} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{contrast}\PYG{p}{)}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{c}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{pinv}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{c}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{c1} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{c2} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{c3} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{p}{[}\PYG{n}{contrast\PYGZus{}efficiency}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{p}{[}\PYG{n}{c1}\PYG{p}{,} \PYG{n}{c2}\PYG{p}{,} \PYG{n}{c3}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[2.44032677542221, 2.5429401030547045, 1.3578767952519295]
\end{sphinxVerbatim}


\subsubsection{Varying the Inter\sphinxhyphen{}Trial Interval with Jittering}
\label{\detokenize{content/GLM:varying-the-inter-trial-interval-with-jittering}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Set Simulation Parameters}
\PYG{n}{n\PYGZus{}tr} \PYG{o}{=} \PYG{l+m+mi}{200}
\PYG{n}{n\PYGZus{}trial} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{face\PYGZus{}intensity} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{n}{object\PYGZus{}intensity} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{sigma} \PYG{o}{=} \PYG{l+m+mf}{0.15}

\PYG{c+c1}{\PYGZsh{} Build Simulation}
\PYG{n}{face\PYGZus{}trials} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{180}\PYG{p}{,} \PYG{n}{n\PYGZus{}trial}\PYG{p}{)}
\PYG{n}{obj\PYGZus{}trials} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{180}\PYG{p}{,} \PYG{n}{n\PYGZus{}trial}\PYG{p}{)}
\PYG{n}{face} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
\PYG{n}{face}\PYG{p}{[}\PYG{n}{face\PYGZus{}trials}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{obj} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
\PYG{n}{obj}\PYG{p}{[}\PYG{n}{obj\PYGZus{}trials}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{voxel\PYGZus{}conv} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{face}\PYG{o}{*}\PYG{n}{face\PYGZus{}intensity}\PYG{p}{,} \PYG{n}{hrf}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{obj}\PYG{o}{*}\PYG{n}{object\PYGZus{}intensity}\PYG{p}{,} \PYG{n}{hrf}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}
\PYG{n}{epsilon} \PYG{o}{=} \PYG{n}{sigma}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{voxel\PYGZus{}conv\PYGZus{}noise} \PYG{o}{=} \PYG{n}{voxel\PYGZus{}conv} \PYG{o}{+} \PYG{n}{epsilon}

\PYG{c+c1}{\PYGZsh{} Build Model}
\PYG{n}{Y} \PYG{o}{=} \PYG{n}{voxel\PYGZus{}conv\PYGZus{}noise}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{face}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{face}\PYG{p}{,} \PYG{n}{hrf}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{obj}\PYG{p}{,} \PYG{n}{hrf}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}

\PYG{c+c1}{\PYGZsh{} Estimate Model}
\PYG{n}{beta} \PYG{o}{=} \PYG{n}{ols\PYGZus{}estimator}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{)}
\PYG{n}{predicted\PYGZus{}y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{beta}\PYG{p}{)}
\PYG{n}{predicted\PYGZus{}sigma} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{residual}\PYG{p}{)}
\PYG{n}{predicted\PYGZus{}ts} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}y}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}
\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{predicted\PYGZus{}ts}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Simulated Voxel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted Voxel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Estimate Contrast Efficiency}
\PYG{p}{[}\PYG{n}{contrast\PYGZus{}efficiency}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{p}{[}\PYG{n}{c1}\PYG{p}{,} \PYG{n}{c2}\PYG{p}{,} \PYG{n}{c3}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[2.5597167760964403, 2.066820546582558, 1.1233445684652128]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_47_1}.png}


\subsubsection{Autocorrelation}
\label{\detokenize{content/GLM:autocorrelation}}
The BOLD signal has some intrinsic autocorrelation that varies with the length of the TR. Different software packages have provided varying solutions to this problem. For example, SPM implements an AR(1) model, which means that it trys to account for the fact that the signal is consistently correlated (i.e., autoregressive) with one lag. In practice, these will rarely change the beta estimates, but rather will adjust our standard errors around the estimates. As we will discuss soon, most group level analyses ignore these subject level, or first\sphinxhyphen{}level errors anyway. It is debatable if this is actually a good practice, but it reduces the importance of accounting for autocorrelation when looking at group level statistics in standard experimental design.

Another important thing to note is that there is some evidence that the AR(1) model can actually increase false positives, especiall in shorter TRs.  See this \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1053811912003825}{paper} by Anders Ekland and colleagues for more details. Also, this is a helpful \sphinxhref{https://mandymejia.com/2016/11/06/how-to-efficiently-prewhiten-fmri-timeseries-the-right-way/}{blog post} discussing prewhitening.

For the scope of this course we will largely be ignoring this issue, but I will plan to add some examples and simulations in the future.  For now, I encourage you to watch this video on \sphinxhref{https://www.youtube.com/watch?v=Mb9LDzvhecY\&list=PLfXA4opIOVrGHncHRxI3Qa5GeCSudwmxM\&index=24}{AR models} if you are interested in learning more about this topic.


\subsection{Exercises}
\label{\detokenize{content/GLM:exercises}}
For our homework exercises, let’s use the simulation to answer questions we might have about experimental design.


\subsubsection{Exercise 1. What happens when we vary the signal amplitude?}
\label{\detokenize{content/GLM:exercise-1-what-happens-when-we-vary-the-signal-amplitude}}
Some signals will be very strong and others weaker. How does the model fit change when the signal amplitudes are stronger and weaker?

In this exercise, make a plot showing how the \(r^2\) changes over 3 different levels of signal intensity.


\subsubsection{Exercise 2. What happens when we vary the noise?}
\label{\detokenize{content/GLM:exercise-2-what-happens-when-we-vary-the-noise}}
How does the amount of noise in the data impact our model fits?

In this exercise, make a plot showing the \(r^2\) for 3 different levels of simulated noise.


\subsubsection{Exercise 3. How many trials do we need?}
\label{\detokenize{content/GLM:exercise-3-how-many-trials-do-we-need}}
A common question in experimental design is determining the optimal number of trials.

In this exercise, try evaluating how 3 different numbers of trials might impact the contrast efficiency.


\subsubsection{Exercise 4. What is the impact of the stimulus duration?}
\label{\detokenize{content/GLM:exercise-4-what-is-the-impact-of-the-stimulus-duration}}
What if one condition simply results in processes that systematically take longer than the other condition?

In this exercise, let’s try to answer this question by creating a simulation where the signal intensity between the two condition is identical, but one simply has a longer duration (i.e., the duration has more TRs than the other condition).

Make a plot showing what happens to the \(\beta\) estimates.


\section{Modeling Single Subject Data}
\label{\detokenize{content/GLM_Single_Subject_Model:modeling-single-subject-data}}\label{\detokenize{content/GLM_Single_Subject_Model::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

Now that we have learned the basics of the GLM using simulations, it’s time to apply this to working with real data. The first step in fMRI data analysis is to build a model for each subject to predict the activation in a single voxel over the entire scanning session. To do this, we need to build a design matrix for our general linear model. We expect distinct brain regions to be involved in processing specific aspects of our task. This means that we will construct separate regressors that model different brain processes.

In this tutorial, we will learn how to build and estimate a single subject first\sphinxhyphen{}level model and will cover the following topics:
\begin{itemize}
\item {} 
Building a design matrix

\item {} 
Modeling noise in the GLM with nuisance variables

\item {} 
Estimating GLM

\item {} 
Performing basic contrasts

\end{itemize}


\subsection{Dataset}
\label{\detokenize{content/GLM_Single_Subject_Model:dataset}}
We will continue to work with the Pinel Localizer dataset from our preprocessing examples.

The Pinel Localizer task was designed to probe several different types of basic cognitive processes, such as visual perception, finger tapping, language, and math. Several of the tasks are cued by reading text on the screen (i.e., visual modality) and also by hearing auditory instructions (i.e., auditory modality). The trials are randomized across conditions and have been optimized to maximize efficiency for a rapid event related design. There are 100 trials in total over a 5\sphinxhyphen{}minute scanning session. Read the original \sphinxhref{https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-8-91}{paper} for more specific details about the task and the \sphinxhref{https://doi.org/10.1016/j.neuroimage.2015.09.052}{dataset paper}.

This dataset is well suited for these tutorials as it is (a) publicly available to anyone in the world, (b) relatively small (only about 5min), and (c) provides many options to create different types of contrasts.

There are a total of 94 subjects available, but we will primarily only be working with a smaller subset of 10\sphinxhyphen{}20 participants. See our tutorial on how to download the data if you are not taking the Psych60 version of the class.


\subsection{Building a Design Matrix}
\label{\detokenize{content/GLM_Single_Subject_Model:building-a-design-matrix}}
First, we will learn the basics of how to build a design matrix for our GLM.

Let’s load all of the python modules we will need to complete this tutorial.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{nibabel} \PYG{k}{as} \PYG{n+nn}{nib}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{file\PYGZus{}reader} \PYG{k+kn}{import} \PYG{n}{onsets\PYGZus{}to\PYGZus{}dm}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{regress}\PYG{p}{,} \PYG{n}{zscore}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{,} \PYG{n}{Design\PYGZus{}Matrix}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{find\PYGZus{}spikes} 
\PYG{k+kn}{from} \PYG{n+nn}{nilearn}\PYG{n+nn}{.}\PYG{n+nn}{plotting} \PYG{k+kn}{import} \PYG{n}{view\PYGZus{}img}\PYG{p}{,} \PYG{n}{glass\PYGZus{}brain}\PYG{p}{,} \PYG{n}{plot\PYGZus{}stat\PYGZus{}map}
\PYG{k+kn}{from} \PYG{n+nn}{bids} \PYG{k+kn}{import} \PYG{n}{BIDSLayout}\PYG{p}{,} \PYG{n}{BIDSValidator}

\PYG{n}{data\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../data/localizer}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{layout} \PYG{o}{=} \PYG{n}{BIDSLayout}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{n}{derivatives}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear\PYGZus{}model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear\PYGZus{}model. Anything that cannot be imported from sklearn.linear\PYGZus{}model is now part of the private API.
  warnings.warn(message, FutureWarning)
\end{sphinxVerbatim}

To build the design matrix, we will be using the Design\_Matrix class from the nltools toolbox.  First, we use pandas to load the text file that contains the onset and duration for each condition of the task. Rows reflect measurements in time sampled at 1/tr cycles per second. Columns reflect distinct conditions. Conditions are either on or off. We then cast this Pandas DataFrame as a Design\_Matrix object. Be sure to specify the sampling frequency, which is \(\frac{1}{tr}\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{load\PYGZus{}bids\PYGZus{}events}\PYG{p}{(}\PYG{n}{layout}\PYG{p}{,} \PYG{n}{subject}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}Create a design\PYGZus{}matrix instance from BIDS event file\PYGZsq{}\PYGZsq{}\PYGZsq{}}
    
    \PYG{n}{tr} \PYG{o}{=} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get\PYGZus{}tr}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}tr} \PYG{o}{=} \PYG{n}{nib}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{subject}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{raw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}

    \PYG{n}{onsets} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{subject}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{events}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{onsets}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Onset}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Duration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Stim}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{k}{return} \PYG{n}{onsets\PYGZus{}to\PYGZus{}dm}\PYG{p}{(}\PYG{n}{onsets}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{,} \PYG{n}{run\PYGZus{}length}\PYG{o}{=}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}

\PYG{n}{dm} \PYG{o}{=} \PYG{n}{load\PYGZus{}bids\PYGZus{}events}\PYG{p}{(}\PYG{n}{layout}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/nltools\PYGZhy{}0.3.18\PYGZhy{}py3.7.egg/nltools/file\PYGZus{}reader.py:141: UserWarning: Computed onsets for video\PYGZus{}right\PYGZus{}hand are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!
  f\PYGZdq{}Computed onsets for \PYGZob{}data.Stim.unique()[i]\PYGZcb{} are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!\PYGZdq{}
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/nltools\PYGZhy{}0.3.18\PYGZhy{}py3.7.egg/nltools/file\PYGZus{}reader.py:141: UserWarning: Computed onsets for video\PYGZus{}left\PYGZus{}hand are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!
  f\PYGZdq{}Computed onsets for \PYGZob{}data.Stim.unique()[i]\PYGZcb{} are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!\PYGZdq{}
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/nltools\PYGZhy{}0.3.18\PYGZhy{}py3.7.egg/nltools/file\PYGZus{}reader.py:141: UserWarning: Computed onsets for audio\PYGZus{}computation are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!
  f\PYGZdq{}Computed onsets for \PYGZob{}data.Stim.unique()[i]\PYGZcb{} are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!\PYGZdq{}
\end{sphinxVerbatim}

The Design\_Matrix class is built on top of Pandas DataFrames and retains most of that functionality. There are additional methods to help with building design matrices. Be sure to check out this \sphinxhref{https://neurolearn.readthedocs.io/en/latest/auto\_examples/01\_DataOperations/plot\_design\_matrix.html\#sphx-glr-auto-examples-01-dataoperations-plot-design-matrix-py}{tutorial} for more information about how to use this tool.

We can check out details about the data using the \sphinxcode{\sphinxupquote{.info()}} method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dm}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}nltools.data.design\PYGZus{}matrix.Design\PYGZus{}Matrix\PYGZsq{}\PYGZgt{}
RangeIndex: 128 entries, 0 to 127
Data columns (total 10 columns):
 \PYGZsh{}   Column                   Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   video\PYGZus{}computation        128 non\PYGZhy{}null    float64
 1   horizontal\PYGZus{}checkerboard  128 non\PYGZhy{}null    float64
 2   audio\PYGZus{}right\PYGZus{}hand         128 non\PYGZhy{}null    float64
 3   audio\PYGZus{}sentence           128 non\PYGZhy{}null    float64
 4   video\PYGZus{}right\PYGZus{}hand         128 non\PYGZhy{}null    float64
 5   audio\PYGZus{}left\PYGZus{}hand          128 non\PYGZhy{}null    float64
 6   video\PYGZus{}left\PYGZus{}hand          128 non\PYGZhy{}null    float64
 7   vertical\PYGZus{}checkerboard    128 non\PYGZhy{}null    float64
 8   audio\PYGZus{}computation        128 non\PYGZhy{}null    float64
 9   video\PYGZus{}sentence           128 non\PYGZhy{}null    float64
dtypes: float64(10)
memory usage: 10.1 KB
\end{sphinxVerbatim}

We can also view the raw design matrix as a dataframe just like pd.Dataframe.  We use the \sphinxcode{\sphinxupquote{.head()}} method to just post the first few rows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dm}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   video\PYGZus{}computation  horizontal\PYGZus{}checkerboard  audio\PYGZus{}right\PYGZus{}hand  \PYGZbs{}
0                1.0                      0.0               0.0   
1                1.0                      0.0               0.0   
2                0.0                      0.0               0.0   
3                0.0                      1.0               0.0   
4                0.0                      1.0               1.0   

   audio\PYGZus{}sentence  video\PYGZus{}right\PYGZus{}hand  audio\PYGZus{}left\PYGZus{}hand  video\PYGZus{}left\PYGZus{}hand  \PYGZbs{}
0             0.0               0.0              0.0              0.0   
1             0.0               0.0              0.0              0.0   
2             0.0               0.0              0.0              0.0   
3             0.0               0.0              0.0              0.0   
4             0.0               0.0              0.0              0.0   

   vertical\PYGZus{}checkerboard  audio\PYGZus{}computation  video\PYGZus{}sentence  
0                    0.0                0.0             0.0  
1                    0.0                0.0             0.0  
2                    0.0                0.0             0.0  
3                    0.0                0.0             0.0  
4                    0.0                0.0             0.0  
\end{sphinxVerbatim}

We can plot each regressor’s time course using the \sphinxcode{\sphinxupquote{.plot()}} method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{dm}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{=}\PYG{n}{a}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7fb700ed4110\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_10_1}.png}

This plot can be useful sometimes, but here there are too many regressors, which makes it difficult to see what is going on.

Often,  \sphinxcode{\sphinxupquote{.heatmap()}} method provides a more useful visual representation of the design matrix.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dm}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_12_0}.png}


\subsubsection{HRF Convolution}
\label{\detokenize{content/GLM_Single_Subject_Model:hrf-convolution}}
Recall what we learned about convolution in our signal processing tutorial. We can now convolve all of the onset regressors with an HRF function using the \sphinxcode{\sphinxupquote{.convolve()}} method. By default it will convolve all regressors with the standard double gamma HRF function, though you can specify custom ones and also specific regressors to convolve. Check out the docstrings for more information by adding a \sphinxcode{\sphinxupquote{?}} after the function name. If you are interested in learning more about different ways to model the HRF using temporal basis functions, watch this \sphinxhref{https://www.youtube.com/watch?v=YfeMIcDWwko\&t=9s}{video}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dm\PYGZus{}conv} \PYG{o}{=} \PYG{n}{dm}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{dm\PYGZus{}conv}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_14_0}.png}

You can see that each of the regressors is now  bit blurrier and now has the shape of an HRF function. We can plot a single regoressor to see this more clearly using the \sphinxcode{\sphinxupquote{.plot()}} method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{dm\PYGZus{}conv}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal\PYGZus{}checkerboard\PYGZus{}c0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{=}\PYG{n}{a}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7fb69091c990\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_16_1}.png}

Maybe we want to plot both of the checkerboard regressors.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{dm\PYGZus{}conv}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal\PYGZus{}checkerboard\PYGZus{}c0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{vertical\PYGZus{}checkerboard\PYGZus{}c0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{=}\PYG{n}{a}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7fb690964c90\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_18_1}.png}


\subsubsection{Multicollinearity}
\label{\detokenize{content/GLM_Single_Subject_Model:multicollinearity}}
In statistics, collinearity or multicollinearity is when one regressor can be strongly linearly predicted from the others. While this does not actually impact the model’s ability to predict data as a whole, it will impact our ability to accurately attribute variance to a single regressor. Recall that in multiple regression, we are estimating the independent variance from each regressor from \sphinxcode{\sphinxupquote{X}} on \sphinxcode{\sphinxupquote{Y}}. If there is substantial overlap between the regressors, then the estimator can not attribute the correct amount of variance each regressor accounts for \sphinxcode{\sphinxupquote{Y}} and the coefficients can become unstable. A more intuitive depiction of this problem can be seen in the venn diagram. The dark orange area in the center at the confluence of all 3 circles reflects the shared variance between \sphinxcode{\sphinxupquote{X1}} and \sphinxcode{\sphinxupquote{X2}} on \sphinxcode{\sphinxupquote{Y}}. If this area becomes bigger, the unique variances become smaller and individually reflect less of the total variance on \sphinxcode{\sphinxupquote{Y}}.

\sphinxincludegraphics{{MultipleRegression}.png}

One way to evaluate multicollinearity is to examine the pairwise correlations between each regressor. We plot the correlation matrix as a heatmap.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{dm\PYGZus{}conv}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RdBu\PYGZus{}r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7fb700e335d0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_20_1}.png}


\paragraph{Variance Inflation Factor}
\label{\detokenize{content/GLM_Single_Subject_Model:variance-inflation-factor}}
Pairwise correlations will let you know if any regressor is correlated with another regressor. However, we are even more concerned about being able to explain any regressor as a linear combination of the other regressors. For example, \sphinxstyleemphasis{can one regressor be explained by three or more of the remaining regressors?} The variance inflation factor (VIF) is a metric that can help us detect multicollinearity. Specifically, it is simply the ratio of variance in a model with multiple terms, divided by the variance of a model with only a single term. This ratio reduces to the following formula:
\begin{equation*}
\begin{split}VIF_j=\frac{1}{1-R_i^2}\end{split}
\end{equation*}
Where \(R_j^2\) is the \(R^2\) value obtained by regressing the \(jth\) predictor on the remaining predictors. This means that each regressor \(j\) will have it’s own variance inflation factor.

How should we interpret the VIF values?

A VIF of 1 indicates that there is no correlation among the \(jth\) predictor and the remaining variables. Values greater than 4 should be investigated further, while VIFs exceeding 10 indicate significant multicollinearity and will likely require intervention.

Here we will use the \sphinxcode{\sphinxupquote{.vif()}} method to calculate the variance inflation factor for our design matrix.

See this \sphinxhref{https://newonlinecourses.science.psu.edu/stat501/node/347/}{overview} for more details on VIFs.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{dm\PYGZus{}conv}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{,} \PYG{n}{dm\PYGZus{}conv}\PYG{o}{.}\PYG{n}{vif}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(}\PYG{n}{rotation}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variance Inflation Factor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Variance Inflation Factor\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_22_1}.png}


\paragraph{Orthogonalization}
\label{\detokenize{content/GLM_Single_Subject_Model:orthogonalization}}
There are many ways to deal with collinearity. In practice, don’t worry about collinearity between your covariates. The more pernicious issues are collinearity in your experimental design.

It is commonly thought that using a procedure called orthogonalization should be used to address issues of multicollinearity. In linear algebra, orthogonalization is the process of prioritizing shared variance between regressors to a single regressor. Recall that the standard GLM already accounts for shared variance by removing it from individual regressors. Orthogonalization allows a user to assign that variance to a specific regressor. However, the process of performing this procedure can introduce artifact into the model and often changes the interpretation of the beta weights in unanticipated ways.

\sphinxincludegraphics{{Orthogonalization}.png}

In general, we do not recommend using orthogonalization in most use cases, with the exception of centering regressor variables. We encourage the interested reader to review this very useful \sphinxhref{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126255}{overview} of collinearity and orthogonalization by Jeanette Mumford and colleagues.


\subsection{Nuisance Variables}
\label{\detokenize{content/GLM_Single_Subject_Model:nuisance-variables}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DEtwsFdFwYc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_25_0}.jpg}


\subsubsection{Filtering}
\label{\detokenize{content/GLM_Single_Subject_Model:filtering}}
Recall from our signal processing tutorial, that there are often other types of artifacts in our signal that might take the form of slow or fast oscillations. It is common to apply a high pass filter to the data to remove low frequency artifacts. Often this can also be addressed by simply using a few polynomials to model these types of trends. If we were to directly filter the brain data using something like a butterworth filter as we did in our signal processing tutorial, we would also need to apply it to our design matrix to make sure that we don’t have any low frequency drift in experimental design. One easy way to simultaneously perform both of these procedures is to simply build a filter into the design matrix. We will be using a discrete cosine transform (DCT), which is a basis set of cosine regressors of varying frequencies up to a filter cutoff of a specified number of seconds. Many software use 100s or 128s as a default cutoff, but we encourage caution that the filter cutoff isn’t too short for your specific experimental design. Longer trials will require longer filter cutoffs. See this \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1053811900906098}{paper} for a more technical treatment of using the DCT as a high pass filter in fMRI data analysis. In addition, here is a more detailed discussion about \sphinxhref{http://mindhive.mit.edu/node/116}{filtering}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt} \PYG{o}{=} \PYG{n}{dm\PYGZus{}conv}\PYG{o}{.}\PYG{n}{add\PYGZus{}dct\PYGZus{}basis}\PYG{p}{(}\PYG{n}{duration}\PYG{o}{=}\PYG{l+m+mi}{128}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7fb69095e390\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_28_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt} \PYG{o}{=} \PYG{n}{dm\PYGZus{}conv}\PYG{o}{.}\PYG{n}{add\PYGZus{}dct\PYGZus{}basis}\PYG{p}{(}\PYG{n}{duration}\PYG{o}{=}\PYG{l+m+mi}{128}\PYG{p}{)}
\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_29_0}.png}


\subsubsection{Intercepts}
\label{\detokenize{content/GLM_Single_Subject_Model:intercepts}}
We almost always want to include an intercept in our model. This will usually reflect the baseline, or the average voxel response during the times that are not being modeled as a regressor. It is important to note that you must have some sparsity to your model, meaning that you can’t model every point in time, as this will make your model rank deficient and unestimable.

If you are concatenating runs and modeling them all together, it is recommended to include a separate intercept for each run, but not for the entire model. This means that the average response within a voxel might differ across runs. You can add an intercept by simply creating a new column of ones (e.g., \sphinxcode{\sphinxupquote{dm{[}\textquotesingle{}Intercept{]} = 1}}). Here we provide an example using the \sphinxcode{\sphinxupquote{.add\_poly()}} method, which adds an intercept by default.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt\PYGZus{}poly} \PYG{o}{=} \PYG{n}{dm\PYGZus{}conv\PYGZus{}filt}\PYG{o}{.}\PYG{n}{add\PYGZus{}poly}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt\PYGZus{}poly}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_31_0}.png}


\subsubsection{Linear Trends}
\label{\detokenize{content/GLM_Single_Subject_Model:linear-trends}}
We also often want to remove any slow drifts in our data.  This might include a linear trend and a quadratic trend. We can also do this with the \sphinxcode{\sphinxupquote{.add\_poly()}} method and adding all trends up to an order of 2 (e.g., quadratic). We typically use this approach rather than applying a high pass filter when working with naturalistic viewing data.

Notice that these do not appear to be very different from the high pass filter basis set. It’s actually okay if there is collinearity in our covariate regressors. Collinearity is only a problem when it correlates with the task regressors as it means that we will not be able to uniquely model the variance. The DCT can occasionally run into edge artifacts, which can be addressed by the linear trend.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt\PYGZus{}poly} \PYG{o}{=} \PYG{n}{dm\PYGZus{}conv\PYGZus{}filt}\PYG{o}{.}\PYG{n}{add\PYGZus{}poly}\PYG{p}{(}\PYG{n}{order}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{include\PYGZus{}lower}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt\PYGZus{}poly}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_33_0}.png}


\subsubsection{Noise Covariates}
\label{\detokenize{content/GLM_Single_Subject_Model:noise-covariates}}
Another important thing to consider is removing variance associated with head motion. Remember the preprocessed data has already realigned each TR in space, but head motion itself can nonlinearly distort the magnetic field. There are several common strategies for trying to remove artifacts associated with head motion. One is using a data driven denoising algorithm like ICA and combining it with a classifer such as FSL’s \sphinxhref{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FIX}{FIX} module. Another approach is to include the amount of correction that needed to be applied to align each TR. For example, if someone moved a lot in a single TR, there will be a strong change in their realignment parameters. It is common to include the 6 parameters as covariates in your regression model. However, as we already noted, often motion can have a nonlinear relationship with signal intensity, so it is often good to include other transformations of these signals to capture nonlinear signal changes resulting from head motion. We typically center the six realigment parameters (or zscore) and then additionally add a quadratic version, a derivative, and the square of the derivatives, which becomes 24 additional regressors.

In addition, it is common to model out big changes using a regressor with a single value indicating the timepoint of the movement. This will be zeros along time, with a single value of one at the time point of interest. This effectively removes any variance associated with this single time point. It is important to model each “spike” as a separate regressor as there might be distinct spatial patterns associated with different types of head motions. We strongly recommond against using a single continuous frame displacement metric as is often recommended by the fMRIprep team. This assumes (1) that there is a \sphinxstyleemphasis{linear} relationship between displacement and voxel activity, and (2) that there is a \sphinxstyleemphasis{single} spatial generator or pattern associated with frame displacement. As we saw in the ICA noise lab, there might be many different types of head motion artifacts. This procedure of including spikes as nuisance regressors is mathematically equivalent to censoring your data and removing the bad TRs. We think it is important to do this in the context of the GLM as it will also reduce the impact if it happens to covary with your task.

First, let’s load preprocessed data from one participant.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sub} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{sub}\PYG{p}{,} \PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{localizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{return\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{file}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

Now let’s inspect the realignment parameters for this participant. These pertain to how much each volume had to be moved in the (X,Y,Z) planes and rotations around each axis. We are standardizing the data so that rotations and translations are on the same scale.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covariates} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{mc} \PYG{o}{=} \PYG{n}{covariates}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{zscore}\PYG{p}{(}\PYG{n}{mc}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZlt{}matplotlib.lines.Line2D at 0x7fb68b61c190\PYGZgt{},
 \PYGZlt{}matplotlib.lines.Line2D at 0x7fb68b61cd10\PYGZgt{},
 \PYGZlt{}matplotlib.lines.Line2D at 0x7fb68b61c390\PYGZgt{},
 \PYGZlt{}matplotlib.lines.Line2D at 0x7fb68b61c110\PYGZgt{},
 \PYGZlt{}matplotlib.lines.Line2D at 0x7fb68b61cc50\PYGZgt{},
 \PYGZlt{}matplotlib.lines.Line2D at 0x7fb68b61c290\PYGZgt{}]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_37_1}.png}

Now, let’s build the 24 covariates related to head motion. We include the 6 realignment parameters that have been standardized. In addition, we add their quadratic, their derivative, and the square of their derivative.

We can create a quick visualization to see what the overall pattern is across the different regressors.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}motion\PYGZus{}covariates}\PYG{p}{(}\PYG{n}{mc}\PYG{p}{,} \PYG{n}{tr}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z\PYGZus{}mc} \PYG{o}{=} \PYG{n}{zscore}\PYG{p}{(}\PYG{n}{mc}\PYG{p}{)}
    \PYG{n}{all\PYGZus{}mc} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{z\PYGZus{}mc}\PYG{p}{,} \PYG{n}{z\PYGZus{}mc}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{z\PYGZus{}mc}\PYG{o}{.}\PYG{n}{diff}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{z\PYGZus{}mc}\PYG{o}{.}\PYG{n}{diff}\PYG{p}{(}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{all\PYGZus{}mc}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{Design\PYGZus{}Matrix}\PYG{p}{(}\PYG{n}{all\PYGZus{}mc}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{)}

\PYG{n}{tr} \PYG{o}{=} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get\PYGZus{}tr}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{mc\PYGZus{}cov} \PYG{o}{=} \PYG{n}{make\PYGZus{}motion\PYGZus{}covariates}\PYG{p}{(}\PYG{n}{mc}\PYG{p}{,} \PYG{n}{tr}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{mc\PYGZus{}cov}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7fb68bb6d210\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_39_1}.png}

Now let’s try to find some spikes in the data. This is performed by finding TRs that exceed a global mean threshold and also that exceed an overall average intensity change by a threshold.  We are using an arbitrary cutoff of 3 standard deviations as a threshold.

First, let’s plot the average signal intensity across all voxels over time.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{data}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Intensity\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_41_1}.png}

Notice there is a clear slow drift in the signal that we will need to remove with our high pass filter.

Now, let’s see if there are any spikes in the data that exceed our threshold. What happens if we use a different threshold?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{spikes} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{find\PYGZus{}spikes}\PYG{p}{(}\PYG{n}{global\PYGZus{}spike\PYGZus{}cutoff}\PYG{o}{=}\PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{n}{diff\PYGZus{}spike\PYGZus{}cutoff}\PYG{o}{=}\PYG{l+m+mf}{2.5}\PYG{p}{)}

\PYG{n}{f}\PYG{p}{,} \PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{spikes} \PYG{o}{=} \PYG{n}{Design\PYGZus{}Matrix}\PYG{p}{(}\PYG{n}{spikes}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{)}
\PYG{n}{spikes}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{ax} \PYG{o}{=} \PYG{n}{a}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7fb7010e7950\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_43_1}.png}

For this subject, our spike identification procedure only found a single spike. Let’s add all of these covariate to our design matrix.

In this example, we will append each of these additional matrices to our main design matrix.

\sphinxstylestrong{Note}: \sphinxcode{\sphinxupquote{.append()}} requires that all matrices are a design\_matrix with the same sampling frequency.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt\PYGZus{}poly\PYGZus{}cov} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt\PYGZus{}poly}\PYG{p}{,} \PYG{n}{mc\PYGZus{}cov}\PYG{p}{,} \PYG{n}{spikes}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{dm\PYGZus{}conv\PYGZus{}filt\PYGZus{}poly\PYGZus{}cov}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RdBu\PYGZus{}r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_45_0}.png}


\subsection{Smoothing}
\label{\detokenize{content/GLM_Single_Subject_Model:smoothing}}
To increase the signal to noise ratio and clean up the data, it is common to apply spatial smoothing to the image.

Here we will convolve the image with a 3\sphinxhyphen{}D gaussian kernel, with a 6mm full width half maximum (FWHM) using the \sphinxcode{\sphinxupquote{.smooth()}} method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fwhm}\PYG{o}{=}\PYG{l+m+mi}{6}
\PYG{n}{smoothed} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{smooth}\PYG{p}{(}\PYG{n}{fwhm}\PYG{o}{=}\PYG{n}{fwhm}\PYG{p}{)}
\end{sphinxVerbatim}

Let’s take a look and see how this changes the image.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_50_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_51_1}.png}


\subsection{Estimate GLM for all voxels}
\label{\detokenize{content/GLM_Single_Subject_Model:estimate-glm-for-all-voxels}}
Now we are ready to estimate the regression model for all voxels.

We will assign the design\_matrix object to the \sphinxcode{\sphinxupquote{.X}} attribute of our \sphinxcode{\sphinxupquote{Brain\_Data}} instance.

Then we simply need to run the \sphinxcode{\sphinxupquote{.regress()}} method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{X} \PYG{o}{=} \PYG{n}{dm\PYGZus{}conv\PYGZus{}filt\PYGZus{}poly\PYGZus{}cov}
\PYG{n}{stats} \PYG{o}{=} \PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{regress}\PYG{p}{(}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dict\PYGZus{}keys([\PYGZsq{}beta\PYGZsq{}, \PYGZsq{}t\PYGZsq{}, \PYGZsq{}p\PYGZsq{}, \PYGZsq{}sigma\PYGZsq{}, \PYGZsq{}residual\PYGZsq{}])
\end{sphinxVerbatim}

Ok, it’s done! Let’s take a look at the results.

The stats variable is a dictionary with the main results from the regression: a brain image with all of the betas for each voxel, a correspondign image of t\sphinxhyphen{}values, p\sphinxhyphen{}values, standard error of the estimate, and residuals.

Remember we have run the same regression model separately on each voxel of the brain.

Let’s take a look at one of the regressors. The names of each of them are in the column names of the design matrix, which is in the \sphinxcode{\sphinxupquote{data.X}} field.  We can print them to see the names. Let’s plot the first one, which is a horizontal checkerboard.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{X}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Index([\PYGZsq{}video\PYGZus{}computation\PYGZus{}c0\PYGZsq{}, \PYGZsq{}horizontal\PYGZus{}checkerboard\PYGZus{}c0\PYGZsq{},
       \PYGZsq{}audio\PYGZus{}right\PYGZus{}hand\PYGZus{}c0\PYGZsq{}, \PYGZsq{}audio\PYGZus{}sentence\PYGZus{}c0\PYGZsq{}, \PYGZsq{}video\PYGZus{}right\PYGZus{}hand\PYGZus{}c0\PYGZsq{},
       \PYGZsq{}audio\PYGZus{}left\PYGZus{}hand\PYGZus{}c0\PYGZsq{}, \PYGZsq{}video\PYGZus{}left\PYGZus{}hand\PYGZus{}c0\PYGZsq{}, \PYGZsq{}vertical\PYGZus{}checkerboard\PYGZus{}c0\PYGZsq{},
       \PYGZsq{}audio\PYGZus{}computation\PYGZus{}c0\PYGZsq{}, \PYGZsq{}video\PYGZus{}sentence\PYGZus{}c0\PYGZsq{}, \PYGZsq{}cosine\PYGZus{}1\PYGZsq{}, \PYGZsq{}cosine\PYGZus{}2\PYGZsq{},
       \PYGZsq{}cosine\PYGZus{}3\PYGZsq{}, \PYGZsq{}cosine\PYGZus{}4\PYGZsq{}, \PYGZsq{}poly\PYGZus{}0\PYGZsq{}, \PYGZsq{}poly\PYGZus{}1\PYGZsq{}, \PYGZsq{}poly\PYGZus{}2\PYGZsq{}, \PYGZsq{}poly\PYGZus{}3\PYGZsq{},
       \PYGZsq{}trans\PYGZus{}x\PYGZsq{}, \PYGZsq{}trans\PYGZus{}y\PYGZsq{}, \PYGZsq{}trans\PYGZus{}z\PYGZsq{}, \PYGZsq{}rot\PYGZus{}x\PYGZsq{}, \PYGZsq{}rot\PYGZus{}y\PYGZsq{}, \PYGZsq{}rot\PYGZus{}z\PYGZsq{}, \PYGZsq{}trans\PYGZus{}x\PYGZsq{},
       \PYGZsq{}trans\PYGZus{}y\PYGZsq{}, \PYGZsq{}trans\PYGZus{}z\PYGZsq{}, \PYGZsq{}rot\PYGZus{}x\PYGZsq{}, \PYGZsq{}rot\PYGZus{}y\PYGZsq{}, \PYGZsq{}rot\PYGZus{}z\PYGZsq{}, \PYGZsq{}trans\PYGZus{}x\PYGZsq{}, \PYGZsq{}trans\PYGZus{}y\PYGZsq{},
       \PYGZsq{}trans\PYGZus{}z\PYGZsq{}, \PYGZsq{}rot\PYGZus{}x\PYGZsq{}, \PYGZsq{}rot\PYGZus{}y\PYGZsq{}, \PYGZsq{}rot\PYGZus{}z\PYGZsq{}, \PYGZsq{}trans\PYGZus{}x\PYGZsq{}, \PYGZsq{}trans\PYGZus{}y\PYGZsq{}, \PYGZsq{}trans\PYGZus{}z\PYGZsq{},
       \PYGZsq{}rot\PYGZus{}x\PYGZsq{}, \PYGZsq{}rot\PYGZus{}y\PYGZsq{}, \PYGZsq{}rot\PYGZus{}z\PYGZsq{}, \PYGZsq{}diff\PYGZus{}spike1\PYGZsq{}, \PYGZsq{}diff\PYGZus{}spike2\PYGZsq{}, \PYGZsq{}diff\PYGZus{}spike3\PYGZsq{}],
      dtype=\PYGZsq{}object\PYGZsq{})
\end{sphinxVerbatim}

Brain\_Data instances have their own plotting methods. We will be using \sphinxcode{\sphinxupquote{.iplot()}} here, which can allow us to interactively look at all of the values.

If you would like to see the top values, we can quickly apply a threshold. Try using \sphinxcode{\sphinxupquote{95}}\% threshold, and be sure to click the \sphinxcode{\sphinxupquote{percentile\_threshold}} option.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{iplot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatText(value=0.0, description=\PYGZsq{}Threshold\PYGZsq{}), HTML(value=\PYGZsq{}Image is 3D\PYGZsq{}, description=\PYGZsq{}Vo…
\end{sphinxVerbatim}


\subsubsection{Save Image}
\label{\detokenize{content/GLM_Single_Subject_Model:save-image}}
We will frequently want to save different brain images we are working with to a nifti file. This is useful for saving intermediate work, or sharing our results with others. This is easy with the \sphinxcode{\sphinxupquote{.write()}} method. Be sure to specify a path and file name for the file.

\sphinxstylestrong{Note}: You can only write to folders where you have permission. Try changing the path to your own directory.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sub}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZus{}betas\PYGZus{}denoised\PYGZus{}smoothed}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{fwhm}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZus{}preprocessed\PYGZus{}fMRI\PYGZus{}bold.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Contrasts}
\label{\detokenize{content/GLM_Single_Subject_Model:contrasts}}
Now that we have estimated our model, we will likely want to create contrasts to examine brain activation to different conditions.

This procedure is identical to those introduced in our GLM tutorial.

Let’s watch another video by Tor Wager to better understand contrasts at the first\sphinxhyphen{}level model stage.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{7MibM1ATai4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{GLM_Single_Subject_Model_61_0}.jpg}

Now, let’s try making a simple contrast where we average only the regressors pertaining to motor. This is essentially summing all of the motor regressors. To take the mean we need to divide by the number of regressors.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{X}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}

\PYG{n}{c1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{c1}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{4}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{c1}\PYG{p}{)}

\PYG{n}{motor} \PYG{o}{=} \PYG{n}{stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{*} \PYG{n}{c1}

\PYG{n}{motor}\PYG{o}{.}\PYG{n}{iplot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Index([\PYGZsq{}video\PYGZus{}computation\PYGZus{}c0\PYGZsq{}, \PYGZsq{}horizontal\PYGZus{}checkerboard\PYGZus{}c0\PYGZsq{},
       \PYGZsq{}audio\PYGZus{}right\PYGZus{}hand\PYGZus{}c0\PYGZsq{}, \PYGZsq{}audio\PYGZus{}sentence\PYGZus{}c0\PYGZsq{}, \PYGZsq{}video\PYGZus{}right\PYGZus{}hand\PYGZus{}c0\PYGZsq{},
       \PYGZsq{}audio\PYGZus{}left\PYGZus{}hand\PYGZus{}c0\PYGZsq{}, \PYGZsq{}video\PYGZus{}left\PYGZus{}hand\PYGZus{}c0\PYGZsq{}, \PYGZsq{}vertical\PYGZus{}checkerboard\PYGZus{}c0\PYGZsq{},
       \PYGZsq{}audio\PYGZus{}computation\PYGZus{}c0\PYGZsq{}, \PYGZsq{}video\PYGZus{}sentence\PYGZus{}c0\PYGZsq{}, \PYGZsq{}cosine\PYGZus{}1\PYGZsq{}, \PYGZsq{}cosine\PYGZus{}2\PYGZsq{},
       \PYGZsq{}cosine\PYGZus{}3\PYGZsq{}, \PYGZsq{}cosine\PYGZus{}4\PYGZsq{}, \PYGZsq{}poly\PYGZus{}0\PYGZsq{}, \PYGZsq{}poly\PYGZus{}1\PYGZsq{}, \PYGZsq{}poly\PYGZus{}2\PYGZsq{}, \PYGZsq{}poly\PYGZus{}3\PYGZsq{},
       \PYGZsq{}trans\PYGZus{}x\PYGZsq{}, \PYGZsq{}trans\PYGZus{}y\PYGZsq{}, \PYGZsq{}trans\PYGZus{}z\PYGZsq{}, \PYGZsq{}rot\PYGZus{}x\PYGZsq{}, \PYGZsq{}rot\PYGZus{}y\PYGZsq{}, \PYGZsq{}rot\PYGZus{}z\PYGZsq{}, \PYGZsq{}trans\PYGZus{}x\PYGZsq{},
       \PYGZsq{}trans\PYGZus{}y\PYGZsq{}, \PYGZsq{}trans\PYGZus{}z\PYGZsq{}, \PYGZsq{}rot\PYGZus{}x\PYGZsq{}, \PYGZsq{}rot\PYGZus{}y\PYGZsq{}, \PYGZsq{}rot\PYGZus{}z\PYGZsq{}, \PYGZsq{}trans\PYGZus{}x\PYGZsq{}, \PYGZsq{}trans\PYGZus{}y\PYGZsq{},
       \PYGZsq{}trans\PYGZus{}z\PYGZsq{}, \PYGZsq{}rot\PYGZus{}x\PYGZsq{}, \PYGZsq{}rot\PYGZus{}y\PYGZsq{}, \PYGZsq{}rot\PYGZus{}z\PYGZsq{}, \PYGZsq{}trans\PYGZus{}x\PYGZsq{}, \PYGZsq{}trans\PYGZus{}y\PYGZsq{}, \PYGZsq{}trans\PYGZus{}z\PYGZsq{},
       \PYGZsq{}rot\PYGZus{}x\PYGZsq{}, \PYGZsq{}rot\PYGZus{}y\PYGZsq{}, \PYGZsq{}rot\PYGZus{}z\PYGZsq{}, \PYGZsq{}diff\PYGZus{}spike1\PYGZsq{}, \PYGZsq{}diff\PYGZus{}spike2\PYGZsq{}, \PYGZsq{}diff\PYGZus{}spike3\PYGZsq{}],
      dtype=\PYGZsq{}object\PYGZsq{})
[0.   0.   0.25 0.   0.25 0.25 0.25 0.   0.   0.   0.   0.   0.   0.
 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
 0.   0.   0.  ]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatText(value=0.0, description=\PYGZsq{}Threshold\PYGZsq{}), HTML(value=\PYGZsq{}Image is 3D\PYGZsq{}, description=\PYGZsq{}Vo…
\end{sphinxVerbatim}

Ok, now we can clearly see regions specifically involved in motor processing.

Now let’s see which regions are more active when making motor movements with our right hand compared to our left hand.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{c\PYGZus{}rvl} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{c\PYGZus{}rvl}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{]}

\PYG{n}{motor\PYGZus{}rvl} \PYG{o}{=} \PYG{n}{stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{*} \PYG{n}{c\PYGZus{}rvl}

\PYG{n}{motor\PYGZus{}rvl}\PYG{o}{.}\PYG{n}{iplot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatText(value=0.0, description=\PYGZsq{}Threshold\PYGZsq{}), HTML(value=\PYGZsq{}Image is 3D\PYGZsq{}, description=\PYGZsq{}Vo…
\end{sphinxVerbatim}

What do you see?


\subsection{Exercises}
\label{\detokenize{content/GLM_Single_Subject_Model:exercises}}
For homework, let’s get a better handle on how to play with our data and test different hypotheses.


\subsubsection{1. Which regions are more involved with visual compared to auditory sensory processing?}
\label{\detokenize{content/GLM_Single_Subject_Model:which-regions-are-more-involved-with-visual-compared-to-auditory-sensory-processing}}\begin{itemize}
\item {} 
Create a contrast to test this hypothesis

\item {} 
plot the results

\item {} 
write the file to your output folder.

\end{itemize}


\subsubsection{2. Which regions are more involved in processing numbers compared to words?}
\label{\detokenize{content/GLM_Single_Subject_Model:which-regions-are-more-involved-in-processing-numbers-compared-to-words}}\begin{itemize}
\item {} 
Create a contrast to test this hypothesis

\item {} 
plot the results

\item {} 
write the file to your output folder.

\end{itemize}


\subsubsection{3. Which regions are more involved with motor compared to cognitive processes (e.g., language and math)?}
\label{\detokenize{content/GLM_Single_Subject_Model:which-regions-are-more-involved-with-motor-compared-to-cognitive-processes-e-g-language-and-math}}\begin{itemize}
\item {} 
Create a contrast to test this hypothesis

\item {} 
plot the results

\item {} 
write the file to your output folder.

\end{itemize}


\subsubsection{4. How are your results impacted by different smoothing kernels?}
\label{\detokenize{content/GLM_Single_Subject_Model:how-are-your-results-impacted-by-different-smoothing-kernels}}\begin{itemize}
\item {} 
Pick two different sized smoothing kernels and create two new brain images with each smoothing kernel

\item {} 
Pick any contrast of interest to you and evaluate the impact of smoothing on the contrast.

\item {} 
plot the results

\item {} 
write the file to your output folder.

\end{itemize}


\section{Group Analysis}
\label{\detokenize{content/Group_Analysis:group-analysis}}\label{\detokenize{content/Group_Analysis::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

In fMRI analysis, we are primarily interested in making inferences about how the brain processes information that is fundamentally similar across all brains even for people that did not directly participate in our study. This requires making inferences about the magnitude of the population level brain response based on measurements from a few randomly sampled participants who were scanned during our experiment.

In this tutorial, we will cover how we go from modeling brain responses in each voxel for a single participant to making inferences about the group. We will cover the following topics:
\begin{itemize}
\item {} 
Mixed Effects Models

\item {} 
How to use the summary statistic approach to make inferences at second level

\item {} 
How to perform many types of inferences at second level with different types of design matrics

\end{itemize}

Let’s start by watching an overview of group statistics by Tor Wager.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}cOYPifDWk}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_1_0}.jpg}


\subsection{Hierarchical Data Structure}
\label{\detokenize{content/Group_Analysis:hierarchical-data-structure}}
We can think of the data as being organized into a hierarchical structure. For each brain, we are measuring BOLD activity in hundreds of thousands of cubic voxels sampled at about 0.5Hz (i.e., TR=2s). Our experimental task will have many different trials for each condition (seconds), and these trials may be spread across multiple scanning runs (minutes), or entire scanning sessions (hours). We are ultimately interested in modeling all of these different scales of data to make an inference about the function of a particular region of the brain across the group of participants we sampled, which we would hope will generalize to the broader population.

\sphinxincludegraphics{{HierarchicalStructure}.png}

In the past few notebooks, we have explored how to preprocess the data to reduce noise and enhance our signal and also how we can estimate responses in each voxel to specific conditions within a single participant based on convolving our experimental design with a canonical hemodynamic response function (HRF). Here we will discuss how we combine these brain responses estimated at the first\sphinxhyphen{}level in a second\sphinxhyphen{}level model to make inferences about the group.


\subsection{Modeling Mixed Effects}
\label{\detokenize{content/Group_Analysis:modeling-mixed-effects}}
Let’s dive deeper into how we can model both random and fixed effects using multi\sphinxhyphen{}level models by watching another video by Tor Wager.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}abMLQSjMSI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_4_0}.jpg}

Most of the statistics we have discussed to this point have assumed that the data we are trying to model are drawn from an identical distribution and that they are independent of each other. For example, each group of participants that complete each version of our experiment are assumed to be random sample of the larger population. However, if there was some type of systematic bias in our sampling strategy, our group level statistics would not necessarily reflect a random draw from the population\sphinxhyphen{}level Gaussian distribution. However, as should already be clear from the graphical depiction of the hierarchical structure of our data above, our data are not always independent. For example, we briefly discussed this in the GLM notebook, but voxel responses within the same participant are not necessarily independent as there appears to be a small amount of autocorrelation in the BOLD response. This requires whitening the data to meet the independence assumption. What is clear from the hierarchy is that all of the data measured from one participant are likely to be more similar to each other than another participant. In fact, it is almost always the case that the variance \sphinxstyleemphasis{within} a subject \(\sigma_{within}^2\) is almost always smaller than the variance \sphinxstyleemphasis{across} participants \(\sigma_{between}^2\). If we combined all of the data from all participants and treated them as if they were independent, we would likely have an inflated view of the group effect (this was historically referred to as a “fixed effects group analysis”).

This problem has been elegantly solved in statistics in a class of models called \sphinxstyleemphasis{mixed effects models}. Mixed effects models are an extension of regression that allows data to be structured into groups and coefficients to vary by groups. They are referred to differently in different scientific domains, for example they may be referred to as multilevel, hierarchical, or panel models. The reason that this framework has been found to be useful in many different fields, is that it is particularly well suited for modeling clustered data, such as students in a classroom and also longitudinal or repeated data, such as within\sphinxhyphen{}subject designs.

The term “mixed” comes from the fact that these models are composed of both \sphinxstyleemphasis{fixed} and \sphinxstyleemphasis{random} effects. Fixed effects refer to parameters describing the amount of variance that a feature explains of an outcome variable. Fixed factors are often explicitly manipulated in an experiment and can be categorical (e.g., gender) or continuous (e.g., age). We assume that the magnitude of these effects are \sphinxstyleemphasis{fixed} in the population, but that the observed signal strength will vary across sessions and subjects. This variation can be decomposed into different sources of variance, such as:
\sphinxhyphen{} Measurement or Irreducible Error
\sphinxhyphen{} Response magnitude that varies randomly across subjects.
\sphinxhyphen{} Response magnitude that varies randomly across different elicitations (e.g., trials or sessions).

Modeling these different sources of variance allows us to have a better idea of how generalizable our estimates might be to another participant or trial.

As an example, imagine if we were interested if there were any gender differences between the length of how males and females cut their hair. We might sample a given individual several times over the course of a couple of years to get an accurate measurement of how long they keep their hair. These samples are akin to trials and will give us a way to represent the overall tendency of the length an individual keeps their hair in the form of a distribution. Narrow distributions mean that there is little variability in the length of the hair at each measurement, while wider distributions indicate more variation in the hair length across time. Of course, we are most interested not in the length of how an individual cuts their hair, but rather how many individuals from the same group cut their hair. This requires measuring multiple participants, who will all vary randomly around some population level hair length parameter. We are interested in modeling the true \sphinxstyleemphasis{fixed effect} of what the population parameter is for hair length, and specifically, whether this differs across gender. The variation in measurements within an individual and across individuals will reflect some degree of randomness that we need to account for in order to estimate a parameter that will generalize beyond the participants we measured their hair, but to new participants.

\sphinxincludegraphics{{MixedEffects}.png}
from Poldrack, Mumford, \& Nichols (2011)

In statistics, it is useful to distinguish between the \sphinxstyleemphasis{model} used to describe the data, the \sphinxstyleemphasis{method} of parameter estimation, and the \sphinxstyleemphasis{algorithm} used to obtain them.

Let’s now watch a video by Martin Lindquist to learn more about the way these models are estimated.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}yaHTygR9b8}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_6_0}.jpg}


\subsubsection{First Level \sphinxhyphen{} Single Subject Model}
\label{\detokenize{content/Group_Analysis:first-level-single-subject-model}}
In fMRI data analysis, we often break analyses into multiple stages. First, we are interested in estimating the parameter (or distribution) of signal in a given region resulting from our experimental manipulation, while simultaneously attempting to control for as much noise and artifacts as possible. This will give us a a single number for each participant of the average length they keep their hair.

At the first level model, for each participant we can define our model as:

\(Y_i = X_i\beta + \epsilon_i\), where \(i\) is an observation for a single participant and \(\epsilon_i \sim \mathcal{N}(0, \sigma_i^2)\)

Because participants are independent, it is possible to estimate each participant separately.

To provide a concrete illustration of the different sources of variability in a signal, let’s make a quick simulation a hypothetical voxel timeseries.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{regress}\PYG{p}{,} \PYG{n}{zscore}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{,} \PYG{n}{Design\PYGZus{}Matrix}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{regress} 
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{external} \PYG{k+kn}{import} \PYG{n}{glover\PYGZus{}hrf}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{ttest\PYGZus{}1samp}

\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{axes}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{a}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{n}{linewidth}\PYG{p}{)}
    \PYG{n}{a}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intensity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    \PYG{n}{a}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{labels} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
    \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{axes}\PYG{p}{:}
        \PYG{n}{a}\PYG{o}{.}\PYG{n}{axes}\PYG{o}{.}\PYG{n}{get\PYGZus{}xaxis}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{set\PYGZus{}visible}\PYG{p}{(}\PYG{k+kc}{False}\PYG{p}{)}
        \PYG{n}{a}\PYG{o}{.}\PYG{n}{axes}\PYG{o}{.}\PYG{n}{get\PYGZus{}yaxis}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{set\PYGZus{}visible}\PYG{p}{(}\PYG{k+kc}{False}\PYG{p}{)}
        
\PYG{k}{def} \PYG{n+nf}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{n}{n\PYGZus{}trial}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{amplitude}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{tr}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
    \PYG{n}{y}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{n\PYGZus{}tr}\PYG{p}{,} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{o}{/}\PYG{n}{n\PYGZus{}trial}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{n}{amplitude}

    \PYG{n}{hrf} \PYG{o}{=} \PYG{n}{glover\PYGZus{}hrf}\PYG{p}{(}\PYG{n}{tr}\PYG{p}{,} \PYG{n}{oversampling}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{hrf}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{epsilon} \PYG{o}{=} \PYG{n}{sigma}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{y} \PYG{o}{+} \PYG{n}{epsilon}
    \PYG{k}{return} \PYG{n}{y}

\PYG{n}{sim1} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{sim2} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{sim1}\PYG{p}{,}\PYG{n}{sim2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Noisy Signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_8_0}.png}

Notice that the noise appears to be independent over each TR.


\subsubsection{Second level summary of between group variance}
\label{\detokenize{content/Group_Analysis:second-level-summary-of-between-group-variance}}
In the second level model, we are interested in relating the subject specific parameters contained in \(\beta\) to the population parameters \(\beta_g\).  We assume that the first level parameters are randomly sampled from a population of possible regression parameters.

\(\beta = X_g\beta_g + \eta\)

\(\eta \sim \mathcal{N}(0,\,\sigma_g^{2})\)

Now let’s add noise onto the beta parameter to see what happens.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{beta} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{3}
\PYG{n}{sim1} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{sim2} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}
\PYG{n}{sim3} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{amplitude}\PYG{o}{=}\PYG{n}{beta}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{sim1}\PYG{p}{,}\PYG{n}{sim2}\PYG{p}{,}\PYG{n}{sim3}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Noisy Signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Noisy Beta + Noisy Signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_11_0}.png}

Try running the above code several times. Can you see how the beta parameter impacts the amplitude of each trial, while the noise appears to be random and uncorrelated with the signal?

Let’s try simulating three subjects with a beta drawn from a normal distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sim1} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{amplitude}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}
\PYG{n}{sim2} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{amplitude}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}
\PYG{n}{sim3} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{amplitude}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{sim1}\PYG{p}{,} \PYG{n}{sim2}\PYG{p}{,} \PYG{n}{sim3}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Subject 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Subject 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Subject 3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_13_0}.png}

To make an inference if there is a reliable difference within or across groups, we need to model the distribution of the parameters resulting from the first level model using a second level model. For example, if we were solely interested in estimating the average length men keep their hair, we would need to measure hair lengths from lots of different men and the average would be our best guess for any new male sampled from the same population. In our example, we are explicitly interested in the pairwise difference between males and females in hair length. Does the mean hair length for one sex significantly different from the hair length of the other group that is larger than the variations in hair length we observe within each group?


\subsubsection{Mixed Effects Model}
\label{\detokenize{content/Group_Analysis:mixed-effects-model}}
In neuroimaging data analysis, there are two main approaches to implementing these different models. Some software packages attempt to use a computationally efficient approximation and use what is called a two stage summary statistic approach. First level models are estimated separately for every participant and then the betas from each participant’s model is combined in a second level model. This is the strategy implemented in SPM and is computationally efficient. However, another approach simultaneously estimates the first and second level models at the same time and often use algorithms that iterate back and forth from the single to the group. The main advantage of this approach over the two\sphinxhyphen{}stage approach is that the uncertainty in the parameter estimates at the first\sphinxhyphen{}level can be appropriately weighted at the group level. For example, if we had a bad participant with very noisy data, we might not want to weight their estimate when we aggregate everyone’s data across the group. The disadvantage of this approach is that the estimation procedure is considerably more computationally expensive. This is the approach implemented in FSL, BrainVoyager, and AFNI. In practice, the advantage of the true random effects simultaneous parameter estimation only probably benefits getting more reliable estimates when the sample size is small. In the limit, both methods should converge to the same answer. For a more in depth comparison see this \sphinxhref{http://eshinjolly.com/2019/02/18/rep\_measures/}{blog post} by Eshin Jolly.

A full mixed effects model can be written as,
\begin{equation*}
\begin{split}Y_i = X_i(X_g\beta_g + \eta) +\epsilon_i\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
     or
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}Y \sim \mathcal(XX_g\beta_g, X\sigma_g^2X^T + \sigma^2)\end{split}
\end{equation*}
\sphinxincludegraphics{{TwoLevelModel}.png}

from Poldrack, Mumford, \& Nichols (2011)

Let’s now try to recover the beta estimates from our 3 simulated subjects.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Create a design matrix with an intercept and predicted response}
\PYG{n}{task} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{amplitude}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{task}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{task}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}

\PYG{c+c1}{\PYGZsh{} Loop over each of the simulated participants and estimate the amplitude of the response.}
\PYG{n}{betas} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{sub} \PYG{o+ow}{in} \PYG{p}{[}\PYG{n}{sim1}\PYG{p}{,} \PYG{n}{sim2}\PYG{p}{,} \PYG{n}{sim3}\PYG{p}{]}\PYG{p}{:}
    \PYG{n}{beta}\PYG{p}{,}\PYG{n}{\PYGZus{}}\PYG{p}{,}\PYG{n}{\PYGZus{}}\PYG{p}{,}\PYG{n}{\PYGZus{}}\PYG{p}{,}\PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{regress}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{sub}\PYG{p}{)}
    \PYG{n}{betas}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot estimated amplitudes for each participant}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Subject1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Subject2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Subject3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{betas}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Estimated Beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Estimated Beta\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_16_1}.png}

What if we simulated lots of participants?  What would the distribution of betas look like?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Create a design matrix with an intercept and predicted response}
\PYG{n}{task} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{amplitude}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{task}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{task}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}

\PYG{c+c1}{\PYGZsh{} Loop over each of the simulated participants and estimate the amplitude of the response.}
\PYG{n}{betas} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{sub} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{sim} \PYG{o}{=} \PYG{n}{simulate\PYGZus{}timeseries}\PYG{p}{(}\PYG{n}{amplitude}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}
    \PYG{n}{beta}\PYG{p}{,}\PYG{n}{\PYGZus{}}\PYG{p}{,}\PYG{n}{\PYGZus{}}\PYG{p}{,}\PYG{n}{\PYGZus{}}\PYG{p}{,}\PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{regress}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{sim}\PYG{p}{)}
    \PYG{n}{betas}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot distribution of estimated amplitudes for each participant}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{betas}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Estimated Beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dashed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.lines.Line2D at 0x7f990c25ced0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_18_1}.png}

Now in a second level analysis, we are interested in whether there is a reliable effect across all participants in our sample. In other words, is there a response to our experiment for a specific voxel that is reliably present across our sample of participants?

We can test this hypothesis in our simulation by running a one\sphinxhyphen{}sample ttest across the estimated first\sphinxhyphen{}level betas at the second level. This allows us to test whether the sample has signal that is reliably different from zero (i.e., the null hypothesis).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ttest\PYGZus{}1samp}\PYG{p}{(}\PYG{n}{betas}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Ttest\PYGZus{}1sampResult(statistic=10.854776909716737, pvalue=1.50775361636617e\PYGZhy{}18)
\end{sphinxVerbatim}

What did we find?


\subsection{Running a Group Analysis}
\label{\detokenize{content/Group_Analysis:running-a-group-analysis}}
Okay, now let’s try and run our own group level analysis with real imaging data using the Pinel Localizer data. I have run a first level model for the first 10 participants using the procedure we used in the single\sphinxhyphen{}subject analysis notebook.

Here is the code I used to complete this for all participants. I wrote all of the betas and also a separate file for each individual regressor of interest.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{from} \PYG{n+nn}{glob} \PYG{k+kn}{import} \PYG{n}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{nibabel} \PYG{k}{as} \PYG{n+nn}{nib}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{zscore}\PYG{p}{,} \PYG{n}{regress}\PYG{p}{,} \PYG{n}{find\PYGZus{}spikes}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{,} \PYG{n}{Design\PYGZus{}Matrix}
\PYG{k+kn}{from} \PYG{n+nn}{bids} \PYG{k+kn}{import} \PYG{n}{BIDSLayout}\PYG{p}{,} \PYG{n}{BIDSValidator}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{file\PYGZus{}reader} \PYG{k+kn}{import} \PYG{n}{onsets\PYGZus{}to\PYGZus{}dm}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{,} \PYG{n}{Design\PYGZus{}Matrix}
\PYG{k+kn}{from} \PYG{n+nn}{nilearn}\PYG{n+nn}{.}\PYG{n+nn}{plotting} \PYG{k+kn}{import} \PYG{n}{view\PYGZus{}img}\PYG{p}{,} \PYG{n}{glass\PYGZus{}brain}\PYG{p}{,} \PYG{n}{plot\PYGZus{}stat\PYGZus{}map}

\PYG{n}{data\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../data/localizer}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{layout} \PYG{o}{=} \PYG{n}{BIDSLayout}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{n}{derivatives}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{tr} \PYG{o}{=} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get\PYGZus{}tr}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fwhm} \PYG{o}{=} \PYG{l+m+mi}{6}
\PYG{n}{spike\PYGZus{}cutoff} \PYG{o}{=} \PYG{l+m+mi}{3}

\PYG{k}{def} \PYG{n+nf}{load\PYGZus{}bids\PYGZus{}events}\PYG{p}{(}\PYG{n}{layout}\PYG{p}{,} \PYG{n}{subject}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}Create a design\PYGZus{}matrix instance from BIDS event file\PYGZsq{}\PYGZsq{}\PYGZsq{}}
    
    \PYG{n}{tr} \PYG{o}{=} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get\PYGZus{}tr}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}tr} \PYG{o}{=} \PYG{n}{nib}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{subject}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{raw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}

    \PYG{n}{onsets} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{subject}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{events}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{onsets}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Onset}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Duration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Stim}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{k}{return} \PYG{n}{onsets\PYGZus{}to\PYGZus{}dm}\PYG{p}{(}\PYG{n}{onsets}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{,} \PYG{n}{run\PYGZus{}length}\PYG{o}{=}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}motion\PYGZus{}covariates}\PYG{p}{(}\PYG{n}{mc}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z\PYGZus{}mc} \PYG{o}{=} \PYG{n}{zscore}\PYG{p}{(}\PYG{n}{mc}\PYG{p}{)}
    \PYG{n}{all\PYGZus{}mc} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{z\PYGZus{}mc}\PYG{p}{,} \PYG{n}{z\PYGZus{}mc}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{z\PYGZus{}mc}\PYG{o}{.}\PYG{n}{diff}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{z\PYGZus{}mc}\PYG{o}{.}\PYG{n}{diff}\PYG{p}{(}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{all\PYGZus{}mc}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{Design\PYGZus{}Matrix}\PYG{p}{(}\PYG{n}{all\PYGZus{}mc}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{sub} \PYG{o+ow}{in} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get\PYGZus{}subjects}\PYG{p}{(}\PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{data} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{sub}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{return\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{file}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{if} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{denoised}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n}{x}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{smoothed} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{smooth}\PYG{p}{(}\PYG{n}{fwhm}\PYG{o}{=}\PYG{n}{fwhm}\PYG{p}{)}

    \PYG{n}{dm} \PYG{o}{=} \PYG{n}{load\PYGZus{}bids\PYGZus{}events}\PYG{p}{(}\PYG{n}{layout}\PYG{p}{,} \PYG{n}{sub}\PYG{p}{)}
    \PYG{n}{covariates} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{sub}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{mc\PYGZus{}cov} \PYG{o}{=} \PYG{n}{make\PYGZus{}motion\PYGZus{}covariates}\PYG{p}{(}\PYG{n}{covariates}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{spikes} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{find\PYGZus{}spikes}\PYG{p}{(}\PYG{n}{global\PYGZus{}spike\PYGZus{}cutoff}\PYG{o}{=}\PYG{n}{spike\PYGZus{}cutoff}\PYG{p}{,} \PYG{n}{diff\PYGZus{}spike\PYGZus{}cutoff}\PYG{o}{=}\PYG{n}{spike\PYGZus{}cutoff}\PYG{p}{)}
    \PYG{n}{dm\PYGZus{}cov} \PYG{o}{=} \PYG{n}{dm}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{add\PYGZus{}dct\PYGZus{}basis}\PYG{p}{(}\PYG{n}{duration}\PYG{o}{=}\PYG{l+m+mi}{128}\PYG{p}{)}\PYG{o}{.}\PYG{n}{add\PYGZus{}poly}\PYG{p}{(}\PYG{n}{order}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{include\PYGZus{}lower}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{dm\PYGZus{}cov} \PYG{o}{=} \PYG{n}{dm\PYGZus{}cov}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{mc\PYGZus{}cov}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{Design\PYGZus{}Matrix}\PYG{p}{(}\PYG{n}{spikes}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{X} \PYG{o}{=} \PYG{n}{dm\PYGZus{}cov}
    \PYG{n}{stats} \PYG{o}{=} \PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{regress}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{file\PYGZus{}name} \PYG{o}{=} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{sub}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{return\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{file}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{dirname}\PYG{p}{(}\PYG{n}{file\PYGZus{}name}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sub\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sub}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZus{}betas\PYGZus{}denoised\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{file\PYGZus{}name}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{file\PYGZus{}name}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZus{}smoothed}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{fwhm}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{file\PYGZus{}name}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{name} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{dm\PYGZus{}cov}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{dirname}\PYG{p}{(}\PYG{n}{file\PYGZus{}name}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sub\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sub}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZus{}denoised\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{file\PYGZus{}name}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZus{}smoothed}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{fwhm}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{file\PYGZus{}name}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

Now, we are ready to run our first group analyses!

Let’s load our design matrix to remind ourselves of the various conditions


\subsubsection{One Sample t\sphinxhyphen{}test}
\label{\detokenize{content/Group_Analysis:one-sample-t-test}}
For our first group analysis, let’s try to examine which regions of the brain are consistently activated across participants. We will just load the first regressor in the design matrix \sphinxhyphen{} \sphinxstyleemphasis{horizontal\_checkerboard}.

We will use the \sphinxcode{\sphinxupquote{glob}} function to search for all files that contain the name \sphinxstyleemphasis{horizontal\_checkerboard} in each subject’s folder. We will then sort the list and load all of the files using the \sphinxcode{\sphinxupquote{Brain\_Data}} class.  This will take a little bit to load all of the data into ram.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{glob}

\PYG{n}{con1\PYGZus{}name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal\PYGZus{}checkerboard}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{con1\PYGZus{}file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub*\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{con1\PYGZus{}name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{*nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{con1\PYGZus{}file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{con1\PYGZus{}dat} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{con1\PYGZus{}file\PYGZus{}list}\PYG{p}{)}
\end{sphinxVerbatim}

Now that we have the data loaded, we can run quick operations such as, what is the mean activation in each voxel across participants?  Or, what is the standard deviation of the voxel activity across participants?

Notice how we can chain different commands like \sphinxcode{\sphinxupquote{.mean()}} and \sphinxcode{\sphinxupquote{.plot()}}.  This makes it easy to quickly manipulate the data similar to how we use tools like pandas.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{con1\PYGZus{}dat}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_26_1}.png}

We can use the \sphinxcode{\sphinxupquote{ttest()}} method to run a quick t\sphinxhyphen{}test across each voxel in the brain.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{con1\PYGZus{}stats} \PYG{o}{=} \PYG{n}{con1\PYGZus{}dat}\PYG{o}{.}\PYG{n}{ttest}\PYG{p}{(}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{con1\PYGZus{}stats}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dict\PYGZus{}keys([\PYGZsq{}t\PYGZsq{}, \PYGZsq{}p\PYGZsq{}])
\end{sphinxVerbatim}

This return a dictionary of a map of the t\sphinxhyphen{}values and a separate one containing the p\sphinxhyphen{}value for each voxel.

For now, let’s look at the results of the t\sphinxhyphen{}ttest and threshold them to something like t\textgreater{}4.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{con1\PYGZus{}stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{iplot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatText(value=0.0, description=\PYGZsq{}Threshold\PYGZsq{}), IntSlider(value=0, continuous\PYGZus{}update=Fals…
\end{sphinxVerbatim}

As you can see we see very clear activation in various parts of visual cortex, which we expected from the visual stimulation.

However, if wanted to test the hypothesis that there are specific areas of early visual cortex (e.g., V1) that process edge orientations, we could run a specific contrast comparing vertical orientations with horizontal orientations.

Now we need to load the vertical data and create a contrast between horizontal and vertical checkerboards.

Here a contrast is simply {[}1, \sphinxhyphen{}1{]} and can be achieved by simply subtracting the two images (assuming the subject images are sorted in the same order).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{con2\PYGZus{}name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{vertical\PYGZus{}checkerboard}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{con2\PYGZus{}file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub*\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{con2\PYGZus{}name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{*nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{con2\PYGZus{}file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{con2\PYGZus{}dat} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{con2\PYGZus{}file\PYGZus{}list}\PYG{p}{)}

\PYG{n}{con1\PYGZus{}v\PYGZus{}con2} \PYG{o}{=} \PYG{n}{con1\PYGZus{}dat}\PYG{o}{\PYGZhy{}}\PYG{n}{con2\PYGZus{}dat}
\end{sphinxVerbatim}

Again, we will now run a one\sphinxhyphen{}sample ttest on the contrast to find regions that are consistently different in viewing horizontal vs vertical checkerboards across participants at the group level.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{con1\PYGZus{}v\PYGZus{}con2\PYGZus{}stats} \PYG{o}{=} \PYG{n}{con1\PYGZus{}v\PYGZus{}con2}\PYG{o}{.}\PYG{n}{ttest}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{con1\PYGZus{}v\PYGZus{}con2\PYGZus{}stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{iplot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatText(value=0.0, description=\PYGZsq{}Threshold\PYGZsq{}), HTML(value=\PYGZsq{}Image is 3D\PYGZsq{}, description=\PYGZsq{}Vo…
\end{sphinxVerbatim}


\subsection{Group statistics using design matrices}
\label{\detokenize{content/Group_Analysis:group-statistics-using-design-matrices}}
For these analyses we ran a one\sphinxhyphen{}sample t\sphinxhyphen{}test to examine the average activation to horizontal checkerboards and the difference between viewing horizontal and vertical checkerboards. This is equivalent to a vector of ones at the second level. The latter analysis is technically a paired\sphinxhyphen{}samples t\sphinxhyphen{}test.

Do these tests sound familiar?

It turns out that most parametric statistical tests are just special cases of the general linear model.  Here are what the design matrices would look like for various types of statistical tests.

\sphinxincludegraphics{{DesignMatrices}.png}
from Poldrack, Mumford, \& Nichols 2011

In this section, we will explore how we can formulate different types of statistical tests using a regression through simulations.


\subsubsection{One Sample t\sphinxhyphen{}test}
\label{\detokenize{content/Group_Analysis:id1}}
Just to review, our one sample t\sphinxhyphen{}test can also be formulated as a regression, where the beta values for each subject in a voxel are predicted by a vector of ones. This \sphinxstyleemphasis{intercept} only model, computes the mean of \(y\). If the mean of \(y\) (i.e., the intercept) is consistently shifted away from zero, then we can reject the null hypothesis that the mean of the betas is zero.
\begin{equation*}
\begin{split}
\begin{bmatrix}
s_1 \\
s_2 \\
s_3 \\
s_4 \\
s_5 \\
s_6
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
1 \\
1 \\
1 \\
1 \\
1 \\
1
\end{bmatrix}
\begin{bmatrix}
\beta_0 
\end{bmatrix}
\end{split}
\end{equation*}
We can simulate this by generating data from a Gaussian distribution. We will generate two groups, where \(y\) reflects equal draws from each of these distributions \({group_1} = \mathcal{N}(10, 2)\) and \({group_2} = \mathcal{N}(5, 2)\). We then regress a vector of ones on \(y\).

We report the estimated value of beta and compare it to various summaries of the simulated data. This allows us to see exactly what each parameter in the regression is calculating.

First, let’s define a function \sphinxcode{\sphinxupquote{run\_regression\_simulation}} to help us generate plots and calculate various ways to summarize the simulation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{run\PYGZus{}regression\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{paired}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}This Function runs a regression and outputs results\PYGZsq{}\PYGZsq{}\PYGZsq{}}
    \PYG{c+c1}{\PYGZsh{} Estimate Regression}
    \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{paired}\PYG{p}{:}
        \PYG{n}{b}\PYG{p}{,} \PYG{n}{t}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{df}\PYG{p}{,} \PYG{n}{res} \PYG{o}{=} \PYG{n}{regress}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{betas: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{b}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{x}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{beta1 + beta2: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{b}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{+} \PYG{n}{b}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{beta1 \PYGZhy{} beta2: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{b}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{b}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean(group1): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{group1}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean(group2): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{group2}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean(group1) \PYGZhy{} mean(group2): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{group1}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{group2}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean(y): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{beta}\PYG{p}{,} \PYG{n}{t}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{df}\PYG{p}{,} \PYG{n}{res} \PYG{o}{=} \PYG{n}{regress}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
        \PYG{n}{a} \PYG{o}{=} \PYG{n}{y}\PYG{p}{[}\PYG{n}{x}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{]}
        \PYG{n}{b} \PYG{o}{=} \PYG{n}{y}\PYG{p}{[}\PYG{n}{x}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{==}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
        \PYG{n}{out} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{k}{for} \PYG{n}{sub} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{sub\PYGZus{}dat} \PYG{o}{=} \PYG{n}{y}\PYG{p}{[}\PYG{n}{X}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{sub}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{]}
            \PYG{n}{out}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{sub\PYGZus{}dat}\PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sub\PYGZus{}dat}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{avg\PYGZus{}sub\PYGZus{}mean\PYGZus{}diff} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{out}\PYG{p}{]}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{betas: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{b}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{contrast beta: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean(subject betas): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean(y): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean(a): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{a}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean(b): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{b}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean(a\PYGZhy{}b): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{a} \PYG{o}{\PYGZhy{}} \PYG{n}{b}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sum(a\PYGZus{}i\PYGZhy{}mean(y\PYGZus{}i))/n: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{avg\PYGZus{}sub\PYGZus{}mean\PYGZus{}diff}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Create Plot}
    \PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sharey}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{yticklabels}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{xticklabels}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{yticklabels}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Subject Values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}    
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}    
    \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

okay, now let’s run the simulation for the one sample t\sphinxhyphen{}test.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{group1\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{group2\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{group1} \PYG{o}{=} \PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{group2} \PYG{o}{=} \PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{group1}\PYG{p}{,} \PYG{n}{group2}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intercept}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    
\PYG{n}{run\PYGZus{}regression\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
    
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
betas: 7.767465428733612
mean(y): 7.767465428733613
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_39_1}.png}

The results of this simulation clearly demonstrate that the intercept of the regression is modeling the mean of \(y\).


\subsubsection{Independent\sphinxhyphen{}Samples T\sphinxhyphen{}Test \sphinxhyphen{} Dummy Codes}
\label{\detokenize{content/Group_Analysis:independent-samples-t-test-dummy-codes}}
Next, let’s explore how we can compute an independent\sphinxhyphen{}sample t\sphinxhyphen{}test using a regression. There are several different ways to compute this. Each of them provides a different way to test for differences between the means of the two samples.

First, we will explore how dummy codes can be used to test for group differences. We will create a design matrix with an intercept and also a column with a binary regressor indicating group membership. The target group will be ones, and the reference group will be zeros.
\begin{equation*}
\begin{split}
\begin{bmatrix}
s_1 \\
s_2 \\
s_3 \\
s_4 \\
s_5 \\
s_6
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
1 & 1\\
1 & 1\\
1 & 1\\
1 & 0\\
1 & 0\\
1 & 0
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}
\end{split}
\end{equation*}
Let’s run another simulation examining what the regression coefficients reflect using this dummy code approach.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{group1\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{group2\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{group1} \PYG{o}{=} \PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{group2} \PYG{o}{=} \PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{group1}\PYG{p}{,} \PYG{n}{group2}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intercept}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Contrast}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{run\PYGZus{}regression\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
betas: [5.59590636 4.86905406]
beta1 + beta2: 10.46496042000535
beta1 \PYGZhy{} beta2: 0.7268523061306578
mean(group1): 10.464960420005351
mean(group2): 5.595906363068004
mean(group1) \PYGZhy{} mean(group2): 4.869054056937347
mean(y): 8.030433391536677
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_42_1}.png}

Can you figure out what the beta estimates are calculating?

The intercept \(\beta_0\) is now the mean of the reference group, and the estimate of the dummy code regressor \(\beta_1\) indicates the difference of the mean of the target group from the reference group.

Thus, the mean of the reference group is \(\beta_0\) or the intercept, and the mean of the target group is \(\beta_1 + \beta_2\).


\subsubsection{Independent\sphinxhyphen{}Samples T\sphinxhyphen{}Test \sphinxhyphen{} Contrasts}
\label{\detokenize{content/Group_Analysis:independent-samples-t-test-contrasts}}
Another way to compare two different groups is by creating a model with an intercept and contrast between the two groups.
\begin{equation*}
\begin{split}
\begin{bmatrix}
s_1 \\
s_2 \\
s_3 \\
s_4 \\
s_5 \\
s_6
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
1 & 1\\
1 & 1\\
1 & 1\\
1 & -1\\
1 & -1\\
1 & -1
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}
\end{split}
\end{equation*}
Let’s now run another simulation to see how these beta estimates differ from the dummy code model.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{group1\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{group2\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{group1} \PYG{o}{=} \PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{group2} \PYG{o}{=} \PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{group1}\PYG{p}{,} \PYG{n}{group2}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intercept}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Contrast}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{run\PYGZus{}regression\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
betas: [7.60615263 3.16144295]
beta1 + beta2: 10.767595577579165
beta1 \PYGZhy{} beta2: 4.444709673800184
mean(group1): 10.767595577579163
mean(group2): 4.4447096738001814
mean(group1) \PYGZhy{} mean(group2): 6.322885903778982
mean(y): 7.606152625689674
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_45_1}.png}

So, just as before, the intercept reflects the mean of \(y\). Now can you figure out what \(\beta_1\) is calculating?

It is the average distance of each group to the mean. The mean of group 1 is \(\beta_0 + \beta_1\) and the mean of group 2 is \(\beta_0 - \beta_1\).

Remember that in our earlier discussion of contrast codes, we noted the importance of balanced codes across regressors. What if the group sizes are unbalanced?  Will this effect our results?

To test this, we will double the sample size of group1 and rerun the simulation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{group1\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{40}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{group2\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{group1} \PYG{o}{=} \PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{group2} \PYG{o}{=} \PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{group1}\PYG{p}{,} \PYG{n}{group2}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Intercept}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Contrast}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{run\PYGZus{}regression\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
betas: [7.98765299 2.16002722]
beta1 + beta2: 10.147680205027868
beta1 \PYGZhy{} beta2: 5.827625771635669
mean(group1): 10.147680205027864
mean(group2): 5.8276257716356685
mean(group1) \PYGZhy{} mean(group2): 4.320054433392196
mean(y): 8.707662060563798
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_47_1}.png}

Looks like the beta estimates are identical to the previous simulation. This demonstrates that we \sphinxstyleemphasis{do not} need to adjust the weights of the number of ones and zeros to sum to zero.  This is because the beta is estimating the average distance from the mean, which is invariant to group sizes.


\subsubsection{Independent\sphinxhyphen{}Samples T\sphinxhyphen{}Test \sphinxhyphen{} Group Intercepts}
\label{\detokenize{content/Group_Analysis:independent-samples-t-test-group-intercepts}}
The third way to calculate an independent samples t\sphinxhyphen{}test using a regression is to split the intercept into two separate binary regressors with each reflecting the membership of each group. There is no need to include an intercept as it is simply a linear combination of the other two regressors.
\begin{equation*}
\begin{split}
\begin{bmatrix}
s_1 \\
s_2 \\
s_3 \\
s_4 \\
s_5 \\
s_6
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
1 & 0\\
1 & 0\\
1 & 0\\
0 & 1\\
0 & 1\\
0 & 1
\end{bmatrix}
\begin{bmatrix}
b_0 \\
b_1
\end{bmatrix}
\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{group1\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{group2\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{group1} \PYG{o}{=} \PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{group1\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{group2} \PYG{o}{=} \PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{group2\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{group1}\PYG{p}{,} \PYG{n}{group2}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{group1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{group2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{group1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{group2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{run\PYGZus{}regression\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
betas: [9.41009741 4.34939829]
beta1 + beta2: 13.759495698990353
beta1 \PYGZhy{} beta2: 5.060699128393246
mean(group1): 9.410097413691801
mean(group2): 4.3493982852985535
mean(group1) \PYGZhy{} mean(group2): 5.060699128393248
mean(y): 6.879747849495177
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_50_1}.png}

This model is obviously separately estimating the means of each group, but how do we know if the difference is significant?  Any ideas?

Just like the single subject regression models, we would need to calculate a contrast, which would simply be \(c=[1 -1]\).

All three of these different approaches will yield identical results when performing a hypothesis test, but each is computing the t\sphinxhyphen{}test slightly differently.


\subsubsection{Paired\sphinxhyphen{}Samples T\sphinxhyphen{}Test}
\label{\detokenize{content/Group_Analysis:paired-samples-t-test}}
Now let’s demonstrate that a paired\sphinxhyphen{}samples t\sphinxhyphen{}test can also be computed using a regression. Here, we will need to create a long format dataset, in which each subject \(s_i\) has two data points (one for each condition \(a\) and \(b\)). One regressor will compute the contrast between condition \(a\) and condition \(b\). Just like before, we need to account for the mean, but instead of computing a grand mean for all of the data, we will separately model the mean of each participant by adding \(n\) more binary regressors where each subject is indicated in each regressor.
\begin{equation*}
\begin{split}
\begin{bmatrix}
s_1a \\
s_1b \\
s_2a \\
s_2b \\
s_3a \\
s_3b
\end{bmatrix}
\quad
=
\quad
\begin{bmatrix}
1 & 1 & 0 & 0\\
-1 & 1 & 0 & 0\\
1 & 0 & 1 & 0\\
-1 & 0 & 1 & 0\\
1 & 0 & 0 & 1\\
-1 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3
\end{bmatrix}
\end{split}
\end{equation*}
This simulation will be slightly more complicated as we will be adding subject level noise to each data point. In this simulation, we will assume that \(\epsilon_i = \mathcal{N}(30, 10)\)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{b\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{sample\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{\PYGZcb{}}

\PYG{n}{y} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{;} \PYG{n}{x} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{;} \PYG{n}{sub\PYGZus{}id} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{;}
\PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{sample\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{sub\PYGZus{}mean} \PYG{o}{=} \PYG{n}{sample\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{a} \PYG{o}{=} \PYG{n}{sub\PYGZus{}mean} \PYG{o}{+} \PYG{n}{a\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{p}{)} \PYG{o}{*} \PYG{n}{a\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{b} \PYG{o}{=} \PYG{n}{sub\PYGZus{}mean} \PYG{o}{+} \PYG{n}{b\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{p}{)} \PYG{o}{*} \PYG{n}{b\PYGZus{}params}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{y}\PYG{o}{.}\PYG{n}{extend}\PYG{p}{(}\PYG{p}{[}\PYG{n}{a}\PYG{p}{,}\PYG{n}{b}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{extend}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{sub\PYGZus{}id}\PYG{o}{.}\PYG{n}{extend}\PYG{p}{(}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}

\PYG{n}{sub\PYGZus{}means} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{[}\PYG{n}{sub\PYGZus{}id}\PYG{o}{==}\PYG{n}{x} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{np}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{n}{sub\PYGZus{}id}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}
\PYG{n}{sub\PYGZus{}means} \PYG{o}{=} \PYG{n}{sub\PYGZus{}means}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{k+kc}{True}\PYG{p}{:}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{k+kc}{False}\PYG{p}{:}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sub\PYGZus{}means}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    
\PYG{n}{run\PYGZus{}regression\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{paired}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
betas: [38.56447613 35.08912938 42.0287038  22.7343386  28.18762639 42.88390721
  7.0837409  43.58511337 22.66066626 42.75568132 37.21848932 19.9169548
 36.84854258 30.06708857 44.8072347  34.76752398 34.36573871 42.14435925
 22.91057627 52.88492285]
contrast beta: 2.545226964528254
mean(subject betas): 36.62046768344662
mean(y): 36.620467683446634
mean(a): 39.16569464797489
mean(b): 34.07524071891838
mean(a\PYGZhy{}b): 5.090453929056513
sum(a\PYGZus{}i\PYGZhy{}mean(y\PYGZus{}i))/n: 2.5452269645282555
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_53_1}.png}

Okay, now let’s try to make sense of all of these numbers. First, we now have \(n\) + 1 \(\beta\)’s. \(\beta_0\) corresponds to the between condition contrast. We will call this the \sphinxstyleemphasis{contrast \(\beta\)}. The rest of the \(\beta\)’s model each subject’s mean. We can see that the means of all of these subject \(\beta\)’s corresponds to the overall mean of \(y\).

Now what is the meaning of the contrast \(\beta\)?

We can see that it is not the average within subject difference between the two conditions as might be expected given a normal paired\sphinxhyphen{}samples t\sphinxhyphen{}test.

Instead, just like the independent samples t\sphinxhyphen{}test described above, the contrast value reflects the average deviation of a condition from each subject’s individual mean.
\begin{equation*}
\begin{split}\sum_{i=1}^n{\frac{a_i - mean(y_i)}{n}}\end{split}
\end{equation*}
where \(n\) is the number of subjects, \(a\) is the condition being compared to \(b\), and the \(mean(y_i)\) is the subject’s mean.


\subsubsection{Linear and Quadratic contrasts}
\label{\detokenize{content/Group_Analysis:linear-and-quadratic-contrasts}}
Hopefully, now you are starting to see that all of the different statistical tests you learned in intro stats (e.g., one\sphinxhyphen{}sample t\sphinxhyphen{}tests, two\sphinxhyphen{}sample t\sphinxhyphen{}tests, ANOVAs, and regressions) are really just a special case of the general linear model.

Contrasts allow us to flexibly test many different types of hypotheses within the regression framework. This allows us to test more complicated and precise hypotheses than might be possible than simply turning everything into a binary yes/no question (i.e., one sample t\sphinxhyphen{}test), or is condition \(a\) greater than condition \(b\) (i.e., two sample t\sphinxhyphen{}test). We’ve already explored how contrasts can be used to create independent and paired\sphinxhyphen{}samples t\sphinxhyphen{}tests in the above simulations. Here we will now provide examples of how to test more sophisticated hypotheses.

Suppose we manipulated the intensity of some type of experimental manipulation across many levels. For example, we increase the working memory load across 4 different levels. We might be interested in identifying regions that monotonically increase as a function of this manipulation. This would be virtually impossible to test using a paired contrast approach (e.g., t\sphinxhyphen{}tests, ANOVAs). Instead, we can simply specify a linear contrast by setting the contrast vector to linearly increase. This is as simple as \sphinxcode{\sphinxupquote{{[}0, 1, 2, 3{]}}}. However, remember that contrasts need to sum to zero (except for the one\sphinxhyphen{}sample t\sphinxhyphen{}test case).  So to make our contrast we can simply subtract the mean \sphinxhyphen{} \sphinxcode{\sphinxupquote{np.array({[}0, 1, 2, 3{]}) \sphinxhyphen{} np.mean((np.array({[}0, 1, 2, 3))}}, which becomes \(c_{linear} = [-1.5, -0.5,  0.5,  1.5]\).

Regions involved in working memory load might not have a linear increase, but instead might show an inverted u\sphinxhyphen{}shaped response, such that the region is not activated at small or high loads, but only at medium loads.  To test this hypothesis, we would need to construct a quadratic contrast \(c_{quadratic}=[-1, 1, 1, -1]\).

Let’s explore this idea with a simple simulation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} First let\PYGZsq{}s make up some hypothetical data based on different types of response we might expect to see.}
\PYG{n}{sim1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{sim2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{.}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Now let\PYGZsq{}s plot our simulated data}
\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sharey}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sim1}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sim2}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Simulated Voxel Response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Working Memory Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Working Memory Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Monotonic Increase to WM Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Inverted U\PYGZhy{}Response to WM Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Inverted U\PYGZhy{}Response to WM Load\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_56_1}.png}

See how the data appear to have a linear and quadratic response to working memory load?

Now let’s create some contrasts and see how a linear or quadratic contrast might be able to detect these different predicted responses.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} First let\PYGZsq{}s create some contrast codes.}
\PYG{n}{linear\PYGZus{}contrast} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{quadratic\PYGZus{}contrast} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Linear Contrast: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{linear\PYGZus{}contrast}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Quadratic Contrast: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{quadratic\PYGZus{}contrast}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Now let\PYGZsq{}s test our contrasts on each dataset.}
\PYG{n}{sim1\PYGZus{}linear} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{sim1}\PYG{p}{,} \PYG{n}{linear\PYGZus{}contrast}\PYG{p}{)}
\PYG{n}{sim1\PYGZus{}quad} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{sim1}\PYG{p}{,} \PYG{n}{quadratic\PYGZus{}contrast}\PYG{p}{)}
\PYG{n}{sim2\PYGZus{}linear} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{sim2}\PYG{p}{,} \PYG{n}{linear\PYGZus{}contrast}\PYG{p}{)}
\PYG{n}{sim2\PYGZus{}quad} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{sim2}\PYG{p}{,} \PYG{n}{quadratic\PYGZus{}contrast}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Now plot the contrast results}
\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sharey}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Linear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Quadratic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{sim1\PYGZus{}linear}\PYG{p}{,} \PYG{n}{sim1\PYGZus{}quad}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Linear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Quadratic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{sim2\PYGZus{}linear}\PYG{p}{,} \PYG{n}{sim2\PYGZus{}quad}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Contrast Value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Contrast}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Contrast}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Monotonic Increase to WM Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Inverted U\PYGZhy{}Response to WM Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Linear Contrast: [\PYGZhy{}1.5 \PYGZhy{}0.5  0.5  1.5]
Quadratic Contrast: [\PYGZhy{}1  1  1 \PYGZhy{}1]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Inverted U\PYGZhy{}Response to WM Load\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Group_Analysis_58_2}.png}

As you can see, the linear contrast is sensitive to detecting responses that monotonically increase, while the quadratic contrast is more sensitive to responses that show an inverted u\sphinxhyphen{}response. Both of these are also signed, so they could also detect responses in the opposite direction.

If we were to apply this to real brain data, we could now find regions that show a linear or quadratic responses to an experimental manipulation across the whole brain. We would then test the null hypothesis that there is no group effect of a linear or quadratic contrast at the second level.

Hopefully, this is starting you a sense of the power of contrasts to flexibly test any hypothesis that you can imagine.


\subsection{Exercises}
\label{\detokenize{content/Group_Analysis:exercises}}

\subsubsection{1. Which regions are more involved with visual compared to auditory sensory processing?}
\label{\detokenize{content/Group_Analysis:which-regions-are-more-involved-with-visual-compared-to-auditory-sensory-processing}}\begin{itemize}
\item {} 
Create a contrast to test this hypothesis

\item {} 
run a group level t\sphinxhyphen{}test

\item {} 
plot the results

\item {} 
write the file to your output folder.

\end{itemize}


\subsubsection{2. Which regions are more involved in processing numbers compared to words?}
\label{\detokenize{content/Group_Analysis:which-regions-are-more-involved-in-processing-numbers-compared-to-words}}\begin{itemize}
\item {} 
Create a contrast to test this hypothesis

\item {} 
run a group level t\sphinxhyphen{}test

\item {} 
plot the results

\item {} 
write the file to your output folder.

\end{itemize}


\subsubsection{3. Are there gender differences?}
\label{\detokenize{content/Group_Analysis:are-there-gender-differences}}
In this exercise, create a two sample design matrix comparing men and women on arithmetic vs reading.

You will first have to figure out the subjects gender using the using the \sphinxcode{\sphinxupquote{participants.tsv}} file.
\begin{itemize}
\item {} 
Create a contrast to test this hypothesis

\item {} 
run a group level t\sphinxhyphen{}test

\item {} 
plot the results

\item {} 
write the file to your output folder.

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{bids} \PYG{k+kn}{import} \PYG{n}{BIDSLayout}\PYG{p}{,} \PYG{n}{BIDSValidator}

\PYG{n}{data\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../data/localizer}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{layout} \PYG{o}{=} \PYG{n}{BIDSLayout}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{n}{derivatives}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{n}{meta\PYGZus{}data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{participants}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{meta\PYGZus{}data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Thresholding Group Analyses}
\label{\detokenize{content/Thresholding_Group_Analyses:thresholding-group-analyses}}\label{\detokenize{content/Thresholding_Group_Analyses::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

Now that we have learned how to estimate a single\sphinxhyphen{}subject model, create contrasts, and run a group\sphinxhyphen{}level analysis, the next important topic to cover is how we can threshold these group maps. This is not as straightforward as it might seem as we need to be able to correct for multiple comparisons.

In this tutorial, we will cover how we go from modeling brain responses in each voxel for a single participant to making inferences about the group. We will cover the following topics:
\begin{itemize}
\item {} 
Issues with correcting for multiple comparisons

\item {} 
Family Wise Error Rate

\item {} 
Bonferroni Correction

\item {} 
False Discovery Rate

\end{itemize}

Let’s get started by watching an overview of multiple comparisons by Martin Lindquist.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AalIM9\PYGZhy{}5\PYGZhy{}Pk}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_1_0}.jpg}

The primary goal in fMRI data analysis is to make inferences about how the brain processes information. These inferences can be in the form of predictions, but most often we are testing hypotheses about whether a particular region of the brain is involved in a specific type of process. This requires rejecting a \(H_0\) hypothesis (i.e., that there is no effect). Null hypothesis testing is traditionally performed by specifying contrasts between different conditions of an experimental design and assessing if these differences between conditions are reliably present across many participants. There are two main types of errors in null\sphinxhyphen{}hypothesis testing.

\sphinxstyleemphasis{Type I error}
\begin{itemize}
\item {} 
\(H_0\) is true, but we mistakenly reject it (i.e., False Positive)

\item {} 
This is controlled by significance level \(\alpha\).

\end{itemize}

\sphinxstyleemphasis{Type II error}
\begin{itemize}
\item {} 
\(H_0\) is false, but we fail to reject it (False Negative)

\end{itemize}

The probability that a hypothesis test will correctly reject a false null hypothesis is described as the \sphinxstyleemphasis{power} of the test.

Hypothesis testing in fMRI is complicated by the fact that we are running many tests across each voxel in the brain (hundreds of thousands of tests). Selecting an appropriate threshold requires finding a balance between sensitivity (i.e., true positive rate) and specificity (i.e., false negative rate). There are two main approaches to correcting for multiple tests in fMRI data analysis.

\sphinxstylestrong{Familywise Error Rate} (FWER) attempts to control the probability of finding \sphinxstyleemphasis{any} false positives. Mathematically, FWER can be defined as the probability \(P\) of observing any false positive \({FWER} = P({False Positives}\geq 1)\).

While, \sphinxstylestrong{False Discovery Rate} (FDR) attempts to control the proportion of false positives among rejected tests. Formally, this is the expected proportion of false positive to the observed number of significant tests \({FDR} = E(\frac{False Positives}{Significant Tests})\).

This should probably be no surprise to anyone, but fMRI studies are expensive and inherently underpowered. Here is a simulation by Jeannette Mumford to show approximately how many participants you would need to achieve 80\% power assuming a specific effect size in your contrast.

\sphinxincludegraphics{{fmri_power}.png}


\subsection{Simulations}
\label{\detokenize{content/Thresholding_Group_Analyses:simulations}}
Let’s explore the concept of false positives to get an intuition about what the overall goals and issues are in controlling for multiple tests.

Let’s load the modules we need for this tutorial. We will be using the SimulateGrid class which contains everything we need to run all of the simulations.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Brain\PYGZus{}Data}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{simulator} \PYG{k+kn}{import} \PYG{n}{SimulateGrid}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear\PYGZus{}model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear\PYGZus{}model. Anything that cannot be imported from sklearn.linear\PYGZus{}model is now part of the private API.
  warnings.warn(message, FutureWarning)
\end{sphinxVerbatim}

Okay, let’s get started and generate 100 x 100 voxels from \(\mathcal{N}(0,1)\) distribution for 20 independent participants.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{simulation} \PYG{o}{=} \PYG{n}{SimulateGrid}\PYG{p}{(}\PYG{n}{grid\PYGZus{}width}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{n\PYGZus{}subjects}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}

\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{nrows}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{sharey}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{counter} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{k}{for} \PYG{n}{col} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{n}{counter}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{a}\PYG{p}{[}\PYG{n}{row}\PYG{p}{,} \PYG{n}{col}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RdBu\PYGZus{}r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
        \PYG{n}{a}\PYG{p}{[}\PYG{n}{row}\PYG{p}{,}\PYG{n}{col}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Subject }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{counter}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
        \PYG{n}{counter} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_6_0}.png}

Each subject’s simulated data is on a 100 x 100 grid. Think of this as a slice from their brain, where each pixel corresponds to the same spatial location across all participants. We have generated random noise separately for each subject. We have not added any true signal in this simulation yet.

This figure is simply to highlight that we are working with 20 independent subjects. In the rest of the plots, we will be working with a single grid that aggregates the results across participants.

Now we are going to start running some simulations to get a sense of the number of false positives we might expect to observe with this data. We will now run an independent one\sphinxhyphen{}sample t\sphinxhyphen{}test on every pixel in the grid across all 20 participants.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{t\PYGZus{}values}\PYG{p}{,} \PYG{n}{square}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RdBu\PYGZus{}r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{T Values}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}T Values\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_8_1}.png}

Even though there was no signal in this simulation, you can see that there are a number of pixels in the grid that exceed a t\sphinxhyphen{}value above 2 and below \sphinxhyphen{}2, which is the approximate cutoff for p \textless{} 0.05. These are all false positives.

Now let’s apply a threshold. We can specify thresholds at a specific t\sphinxhyphen{}value using the \sphinxcode{\sphinxupquote{threshold\_type=\textquotesingle{}t\textquotesingle{}}}. Alternatively, we can specify a specific p\sphinxhyphen{}value using the \sphinxcode{\sphinxupquote{threshold\_type=\textquotesingle{}p\textquotesingle{}}}. To calculate the number of false positives, we can simply count the number of tests that exceed this threshold.

If we run this simulation again 100 times, we can estimate the false positive rate, which is the average number of false positives over all 100 simulations.

Let’s see what this looks like for a threshold of p \textless{} 0.05.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{threshold} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{05}
\PYG{n}{simulation} \PYG{o}{=} \PYG{n}{SimulateGrid}\PYG{p}{(}\PYG{n}{grid\PYGZus{}width}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{n\PYGZus{}subjects}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{plot\PYGZus{}grid\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{n}{threshold}\PYG{p}{,} \PYG{n}{threshold\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_10_0}.png}

The left panel is the average over all of the participants. The middle panel show voxels that exceed the statistical threshold. The right panel is the overall false\sphinxhyphen{}positive rate across the 100 simulations.

In this simulation, a threshold of p \textless{} 0.05 results in observing at least one voxels that is a false positive across every one of our 100 simulations.

What if we looked at a fewer number of voxels? How would this change our false positive rate?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{threshold} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{05}
\PYG{n}{simulation} \PYG{o}{=} \PYG{n}{SimulateGrid}\PYG{p}{(}\PYG{n}{grid\PYGZus{}width}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{n\PYGZus{}subjects}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{plot\PYGZus{}grid\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{n}{threshold}\PYG{p}{,} \PYG{n}{threshold\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_12_0}.png}

This simulation shows that examining fewer numbers of voxels will yield considerably less false positives. One common approach to controlling for multiple tests involves only looking for voxels within a specific region of interest (e.g., small volume correction), or looking at average activation within a larger region (e.g., ROI based analyses).

What about if we increase the threshold on our original 100 x 100 grid?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{threshold} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{0001}
\PYG{n}{simulation} \PYG{o}{=} \PYG{n}{SimulateGrid}\PYG{p}{(}\PYG{n}{grid\PYGZus{}width}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{n\PYGZus{}subjects}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{plot\PYGZus{}grid\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{n}{threshold}\PYG{p}{,} \PYG{n}{threshold\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_14_0}.png}

You can see that this dramatically decreases the number of false positives to the point that some of the simulations no longer contain any false positives.

What is the optimal threshold that will give us an \(\alpha=0.05\)?

To calculate this, we will run 100 simulations at different threshold levels to find the threshold that leads to a false positive rate that is lower than our alpha value.

We could search over t\sphinxhyphen{}values, or p\sphinxhyphen{}values. Let’s explore t\sphinxhyphen{}values first.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{alpha} \PYG{o}{=} \PYG{l+m+mf}{0.05}
\PYG{n}{n\PYGZus{}simulations} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{o}{.}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{sim\PYGZus{}all} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{x}\PYG{p}{:}
    \PYG{n}{sim} \PYG{o}{=} \PYG{n}{SimulateGrid}\PYG{p}{(}\PYG{n}{grid\PYGZus{}width}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{n\PYGZus{}subjects}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
    \PYG{n}{sim}\PYG{o}{.}\PYG{n}{run\PYGZus{}multiple\PYGZus{}simulations}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{n}{p}\PYG{p}{,} \PYG{n}{threshold\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{o}{=}\PYG{n}{n\PYGZus{}simulations}\PYG{p}{)}
    \PYG{n}{sim\PYGZus{}all}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{sim}\PYG{o}{.}\PYG{n}{fpr}\PYG{p}{)}

\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{sim\PYGZus{}all}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{False Positive Rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Threshold (t)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Simulations = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{n\PYGZus{}simulations}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{alpha}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dashed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.lines.Line2D at 0x7feb9fc76290\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_16_1}.png}

As you can see, the false positive rate is close to our alpha starting at a threshold of about 6.2. This means that when we test a hypothesis over 10,000 independent voxels, we can be confident that we will only observe false positives in approximately 5 out of 100 experiments. This means that we are effectively controlling the family wise error rate (FWER).

Let’s use that threshold for our simulation again.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{simulation} \PYG{o}{=} \PYG{n}{SimulateGrid}\PYG{p}{(}\PYG{n}{grid\PYGZus{}width}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{n\PYGZus{}subjects}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{plot\PYGZus{}grid\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{l+m+mf}{6.2}\PYG{p}{,} \PYG{n}{threshold\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_18_0}.png}

Notice, that we are now observing a false positive rate of approximately .05, though this number will slightly change each time you run the simulation.

Another way to find the threshold that controls FWER is to divide the alpha by the number of independent tests across voxels. This is called the \sphinxstylestrong{bonferroni correction}.

\({bonferroni} = \frac{\alpha}{M}\), where \(M\) is the number of voxels.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grid\PYGZus{}width} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{threshold} \PYG{o}{=} \PYG{l+m+mf}{0.05}\PYG{o}{/}\PYG{p}{(}\PYG{n}{grid\PYGZus{}width}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{simulation} \PYG{o}{=} \PYG{n}{SimulateGrid}\PYG{p}{(}\PYG{n}{grid\PYGZus{}width}\PYG{o}{=}\PYG{n}{grid\PYGZus{}width}\PYG{p}{,} \PYG{n}{n\PYGZus{}subjects}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{plot\PYGZus{}grid\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{n}{threshold}\PYG{p}{,} \PYG{n}{threshold\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_20_0}.png}

This seems like a great way to ensure that we minimize our false positives.

Now what happens when start adding signal to our simulation?

We will represent signal in a smaller square in the middle of the simulation. The width of the square can be changed using the \sphinxcode{\sphinxupquote{signal\_width}} parameter. The amplitude of this signal is controlled by the \sphinxcode{\sphinxupquote{signal\_amplitude}} parameter.

Let’s see how well the bonferroni threshold performs when we add 100 voxels of signal.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grid\PYGZus{}width} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{threshold} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{05}\PYG{o}{/}\PYG{p}{(}\PYG{n}{grid\PYGZus{}width}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{signal\PYGZus{}width} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{signal\PYGZus{}amplitude} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{n}{simulation} \PYG{o}{=} \PYG{n}{SimulateGrid}\PYG{p}{(}\PYG{n}{signal\PYGZus{}amplitude}\PYG{o}{=}\PYG{n}{signal\PYGZus{}amplitude}\PYG{p}{,} \PYG{n}{signal\PYGZus{}width}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{grid\PYGZus{}width}\PYG{o}{=}\PYG{n}{grid\PYGZus{}width}\PYG{p}{,} \PYG{n}{n\PYGZus{}subjects}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{plot\PYGZus{}grid\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{n}{threshold}\PYG{p}{,} \PYG{n}{threshold\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_22_0}.png}

Here we show how many voxels were identified using the bonferroni correction.

In the left panel is the average data across all 20 participants. The second panel, shows the voxels that exceed the statistical threshold. The third panel shows the false positive rate, and the 4th panel furthest on the right shows the average signal recovery (how many voxels survived within the true signal square across all 100 simulations.

We can see that we have an effective false positive rate approximately equal to our alpha threshold. However, our threshold is so high, that we can barely detect any true signal with this amplitude. In fact, we are only recovering about 12\% of the voxels that should have signal.

This simulation highlights the main issue with using bonferroni correction in practice. The threshold is so conservative that the magnitude of an effect needs to be unreasonably large to survive correction over hundreds of thousands of voxels.


\subsection{Family Wise Error Rate}
\label{\detokenize{content/Thresholding_Group_Analyses:family-wise-error-rate}}
At this point you may be wondering if it even makes sense to assume that each test is independent. It seems reasonable to expect some degree of spatial correlation in our data. Our simulation is a good example of this as we have a square that contains signal across contiguous voxels. In practice, most of our functional neuroanatomy that we are investigating is larger than a single voxel and our spatial smoothing preprocessing step increase the spatial correlation.

It can be shown that the Bonferroni correction is overally conservative in the presence of spatial dependence and results in a decreased power to detect voxels that are truly active.

Let’s watch a video by Martin Lindquist to learn more about different ways to control for the Family Wise Error Rate.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MxQeEdVNihg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_25_0}.jpg}


\subsubsection{Cluster Extent}
\label{\detokenize{content/Thresholding_Group_Analyses:cluster-extent}}
Another approach to controlling the FWER is called cluster correction, or cluster extent. In this approach, the goal is to identify a threshold such that the maximum statistic exceeds it at a specified alpha. The distribution of the maximum statistic can be approximated using Gaussian Random Field Theory (RFT), which attempts to account for the spatial dependence of the data.

\sphinxincludegraphics{{fwer}.png}

This requires specifying an initial threshold to determine the \sphinxstyleemphasis{Euler Characteristic} or the number of blobs minus the number of holes in the thresholded image. The number of voxels in the blob and the overall smoothness can be used to calculate something called \sphinxstyleemphasis{resels} or resolution elements and can be effectively thought of as the spatial units that need to be controlled for using FWER. We won’t be going into too much detail with this approach as the mathematical details are somewhat complicated. In practice, if the image is smooth and the number of subjects is high enough (around 20), cluster correction seems to provide control closer to the true false positive rate than Bonferroni correction. Though we won’t be spending time simulating this today, I encourage you to check out this Python \sphinxhref{https://matthew-brett.github.io/teaching/random\_fields.html}{simulation} by Matthew Brett and this \sphinxhref{https://www.fil.ion.ucl.ac.uk/spm/doc/books/hbf2/pdfs/Ch14.pdf}{chapter} for an introduction to random field theory.

\sphinxincludegraphics{{grf}.png}

Cluster extent thresholding has recently become somewhat controversial due to several high profile papers that have found that it appears to lead to an inflated false positive rate in practice (see \sphinxhref{https://www.pnas.org/content/113/28/7900}{Ekland et al., 2017}). A recent paper by \sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4214144/}{Woo et al. 2014} has shown that a liberal initial threshold (i.e. higher than p \textless{} 0.001) will inflate the number of false positives above the nominal level of 5\%. There is no optimal way to select the initial threshold and often slight changes will give very different results. Furthermore, this approach does not appear to work equally well across all types of findings. For example, this approach can work well with some amounts of smoothing results that have a particular spatial extent, but not equally well for all types of signals. In other words, it seems potentially problematic to assume that spatial smoothness is constant over the brain and also that it is adequately represented using a Gaussian distribution. Finally, it is important to note that this approach only allows us to make inferences for the entire cluster. We can say that there is some voxel in the cluster that is significant, but we can’t really pinpoint which voxels within the cluster may be driving the effect.

This is one of the more popular ways to control for multiple comparisons as it is particularly sensitive when there is a weak, but spatially contiguous signal. However, in practice, we don’t recommend using this approach as it has a lot of assumptions that are rarely met and the spatial inference is fairly weak.

There are several other popular FWER approaches to correcting for multiple tests that try to address these issues.


\subsubsection{Threshold Free Cluster Extent}
\label{\detokenize{content/Thresholding_Group_Analyses:threshold-free-cluster-extent}}
One interesting solution to the issue of finding an initial threshold seems to be addressed by the threshold free cluster enhancement method presented in \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1053811908002978?via\%3Dihub}{Smith \& Nichols, 2009}. In this approach, the authors propose a way to combine cluster extent and voxel height into a single metric that does not require specifying a specific initial threshold. It essentially involves calculating the integral of the overall product of a signal intensity and spatial extent over multiple thresholds. It has been shown to perform particularly well when combined with non\sphinxhyphen{}parameteric resampling approaches such as \sphinxhref{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Randomise/UserGuide}{randomise} in FSL. This method is implemented in FSL and also in \sphinxhref{https://github.com/markallenthornton/MatlabTFCE}{Matlab} by Mark Thornton. For more details about this approach check out this \sphinxhref{http://markallenthornton.com/blog/matlab-tfce/}{blog post} by Mark Thornton, this \sphinxhref{https://mumfordbrainstats.tumblr.com/post/130127249926/paper-overview-threshold-free-cluster-enhancement}{video} by Jeanette Mumford, and the original \sphinxhref{https://www.fmrib.ox.ac.uk/datasets/techrep/tr08ss1/tr08ss1.pdf}{technical report}.


\subsubsection{Parametric simulations}
\label{\detokenize{content/Thresholding_Group_Analyses:parametric-simulations}}
One approach to estimating the inherent smoothness in the data, or it’s spatial autocorrelation, is using parametric simulations. This was the approach originally adopted in AFNI’s AlphaSim/3DClustSim. After it was \sphinxhref{https://www.pnas.org/content/113/28/7900}{demonstrated} that real fMRI data was not adequately modeled by a standard Gaussian distribution, the AFNI group quickly updated their software and implemented a range of different algorithms in their \sphinxhref{https://afni.nimh.nih.gov/pub/dist/doc/program\_help/3dClustSim.html}{3DClustSim} tool. See this \sphinxhref{https://www.biorxiv.org/content/10.1101/065862v1}{paper} for an overview of these changes.


\subsubsection{Nonparametric approaches}
\label{\detokenize{content/Thresholding_Group_Analyses:nonparametric-approaches}}
As an alternative to RFT, nonparametric methods use the data themselves to find the appropriate distribution. These methods can provide substantial improvements in power and validity, particularly with small sample sizes, so we recommend these in general over cluster extent. These tests can verify the validity of the less computationally expensive parametric approaches. However, it is important to note that this is much more computationally expensive as 5\sphinxhyphen{}10k permutations need to be run at every voxel. The FSL tool \sphinxhref{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Randomise/UserGuide}{randomise} is probably the current gold standard and there are versions that run on GPUs, such as \sphinxhref{https://github.com/wanderine/BROCCOLI}{BROCCOLI} to speed up the computation time.

Here we will run a simulation using a one\sphinxhyphen{}sample permutation test (i.e., sign test) on our data. We will make the grid much smaller to speed up the simulation and will decrease the number of simulations by an order of magnitude, but you will see that it is still very slow (5,000 permutations times 9 voxels times 10 simulations). This approach makes no distributional assumptions, but still requires correcting for multiple tests using either FWER or FDR approaches. Don’t worry about running this cell if it is taking too long.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grid\PYGZus{}width} \PYG{o}{=} \PYG{l+m+mi}{3}
\PYG{n}{threshold} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{05}
\PYG{n}{signal\PYGZus{}amplitude} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{simulation} \PYG{o}{=} \PYG{n}{SimulateGrid}\PYG{p}{(}\PYG{n}{signal\PYGZus{}amplitude}\PYG{o}{=}\PYG{n}{signal\PYGZus{}amplitude}\PYG{p}{,} \PYG{n}{signal\PYGZus{}width}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{grid\PYGZus{}width}\PYG{o}{=}\PYG{n}{grid\PYGZus{}width}\PYG{p}{,} \PYG{n}{n\PYGZus{}subjects}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{t\PYGZus{}values}\PYG{p}{,} \PYG{n}{simulation}\PYG{o}{.}\PYG{n}{p\PYGZus{}values} \PYG{o}{=} \PYG{n}{simulation}\PYG{o}{.}\PYG{n}{\PYGZus{}run\PYGZus{}permutation}\PYG{p}{(}\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{isfit} \PYG{o}{=} \PYG{k+kc}{True}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{threshold\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{threshold}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{plot\PYGZus{}grid\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{n}{threshold}\PYG{p}{,} \PYG{n}{threshold\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{False Discovery Rate (FDR)}
\label{\detokenize{content/Thresholding_Group_Analyses:false-discovery-rate-fdr}}
You may be wondering why we need to control for \sphinxstyleemphasis{any} false positive when testing across hundreds of thousands of voxels. Surely a few are okay as long as they don’t overwhelm the true signal.

Let’s learn about the False Discovery Rate (FDR) from another video by Martin Lindquist.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W9ogBO4GEzA}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_29_0}.jpg}

The \sphinxstyleemphasis{false discovery rate} (FDR) is a more recent development in multiple testing correction originally described by \sphinxhref{https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02031.x}{Benjamini \& Hochberg, 1995}. While FWER is the probability of any false positives occurring in a family of tests, the FDR is the expected proportion of false positives among significant tests.

The FDR is fairly straightforward to calculate.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
We select a desired limit \(q\) on FDR, which is the proportion of false positives we are okay with observing (e.g., 5/100 tests or 0.05).

\item {} 
We rank all of the p\sphinxhyphen{}values over all the voxels from the smallest to largest.

\item {} 
We find the threshold \(r\) such that \(p \leq i/m * q\)

\item {} 
We reject any \(H_0\) that is lower than \(r\).

\end{enumerate}

\sphinxincludegraphics{{fdr_calc}.png}

In a brain map, this means that we expect approximately 95\% of the voxels reported at q \textless{} .05 FDR\sphinxhyphen{}corrected to be true activations (note we use q instead of p). The FDR procedure adaptively identifies a threshold based on the overall signal across all voxels. Larger signals results in lower thresholds. Importantly, if all of the null hypotheses are true, then the FDR will be equivalent to the FWER. This means that any FWER procedure will \sphinxstyleemphasis{also} control the FDR. For these reasons, any procedure which controls the FDR is necessarily less stringent than a FWER controlling procedure, which leads to an overall increased power. Another nice feature of FDR, is that it operates on p\sphinxhyphen{}values instead of test statistics, which means it can be applied to most statistical tests.

This figure is taken from Poldrack, Mumford, \& Nichols (2011) and compares different procedures to control for multiple tests.
\sphinxincludegraphics{{fdr}.png}

For a more indepth overview of FDR, see this \sphinxhref{https://matthew-brett.github.io/teaching/fdr.html}{tutorial} by Matthew Brett.

Let’s now try to apply FDR to our own simulations. All we need to do is add a \sphinxcode{\sphinxupquote{correction=\textquotesingle{}fdr\textquotesingle{}}} flag to our simulation plot. We need to make sure that the \sphinxcode{\sphinxupquote{threshold=0.05}} to use the correct \(q\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grid\PYGZus{}width} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{threshold} \PYG{o}{=} \PYG{o}{.}\PYG{l+m+mi}{05}
\PYG{n}{signal\PYGZus{}amplitude} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{simulation} \PYG{o}{=} \PYG{n}{SimulateGrid}\PYG{p}{(}\PYG{n}{signal\PYGZus{}amplitude}\PYG{o}{=}\PYG{n}{signal\PYGZus{}amplitude}\PYG{p}{,} \PYG{n}{signal\PYGZus{}width}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{grid\PYGZus{}width}\PYG{o}{=}\PYG{n}{grid\PYGZus{}width}\PYG{p}{,} \PYG{n}{n\PYGZus{}subjects}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{plot\PYGZus{}grid\PYGZus{}simulation}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{n}{threshold}\PYG{p}{,} \PYG{n}{threshold\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{q}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{correction}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fdr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FDR q \PYGZlt{} 0.05 corresponds to p\PYGZhy{}value of }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{corrected\PYGZus{}threshold}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
FDR q \PYGZlt{} 0.05 corresponds to p\PYGZhy{}value of 0.00030256885357668906
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_31_1}.png}

Okay, using FDR of q \textless{} 0.05 for our simulation identifies a p\sphinxhyphen{}value threshold of p \textless{} 0.00034. This is more liberal than the bonferroni threshold of p \textless{} 0.000005 and allows us to recover much more signal as a consequence. You can see that at this threshold there are more false positives, which leads to a much higher overall false positive rate. Remember, this metric is only used for calculating the family wise error rate and indicates the presence of \sphinxstyleemphasis{any} false positive across each of our 100 simulations.

To calculate the empirical false discovery rate, we need to calculate the percent of any activated voxels that were false positives.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{simulation}\PYG{o}{.}\PYG{n}{multiple\PYGZus{}fdr}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{False Discovery Rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{False Discovery Rate of Simulations}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 0, \PYGZsq{}False Discovery Rate of Simulations\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_33_1}.png}

In our 100 simulations, the majority had a false discovery rate below our q \textless{} 0.05.


\subsubsection{Thresholding Brain Maps}
\label{\detokenize{content/Thresholding_Group_Analyses:thresholding-brain-maps}}
In the remainder of the tutorial, we will move from simulation to playing with real data.

Let’s watch another video by Tor Wager on how multiple comparison approaches are used in practice, highlighting some of the pitfalls with some of the different approaches.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{N7Iittt8HrU}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_36_0}.jpg}

We will be exploring two simple and fast ways to threshold your group analyses.

First, we will simply threshold based on selecting an arbitrary statistical threshold. The values are completely arbitrary, but it is common to start with something like p \textless{} .001. We call this \sphinxstyleemphasis{uncorrected} because this is simply the threshold for any voxel as we are not controlling for multiple tests.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{con1\PYGZus{}name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal\PYGZus{}checkerboard}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{data\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../data/localizer}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{con1\PYGZus{}file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub*\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{con1\PYGZus{}name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{*nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{con1\PYGZus{}file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{con1\PYGZus{}dat} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{con1\PYGZus{}file\PYGZus{}list}\PYG{p}{)}
\PYG{n}{con1\PYGZus{}stats} \PYG{o}{=} \PYG{n}{con1\PYGZus{}dat}\PYG{o}{.}\PYG{n}{ttest}\PYG{p}{(}\PYG{n}{threshold\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{unc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{o}{.}\PYG{l+m+mi}{001}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{con1\PYGZus{}stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{thr\PYGZus{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_38_1}.png}

We see some significant activations in visual cortex, but we also see strong t\sphinxhyphen{}tests in the auditory cortex.

Why do you think this is?

We can also easily run FDR correction by changing the inputs of the \sphinxcode{\sphinxupquote{threshold\_dict}}. We will be using a q value of 0.05 to control our false discovery rate.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{con1\PYGZus{}stats} \PYG{o}{=} \PYG{n}{con1\PYGZus{}dat}\PYG{o}{.}\PYG{n}{ttest}\PYG{p}{(}\PYG{n}{threshold\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fdr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{o}{.}\PYG{l+m+mi}{05}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{con1\PYGZus{}stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{thr\PYGZus{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_41_1}.png}

You can see that at least for this particular contrast, the FDR threshold appears to be more liberal than p \textless{} 0.001 uncorrected.

Let’s look at another contrast between vertical and horizontal checkerboards.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{con2\PYGZus{}name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{vertical\PYGZus{}checkerboard}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{con2\PYGZus{}file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub*\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{con2\PYGZus{}name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{*nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{con2\PYGZus{}file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{con2\PYGZus{}dat} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{con2\PYGZus{}file\PYGZus{}list}\PYG{p}{)}

\PYG{n}{con1\PYGZus{}v\PYGZus{}con2} \PYG{o}{=} \PYG{n}{con1\PYGZus{}dat}\PYG{o}{\PYGZhy{}}\PYG{n}{con2\PYGZus{}dat}

\PYG{n}{con1\PYGZus{}v\PYGZus{}con2\PYGZus{}stats} \PYG{o}{=} \PYG{n}{con1\PYGZus{}v\PYGZus{}con2}\PYG{o}{.}\PYG{n}{ttest}\PYG{p}{(}\PYG{n}{threshold\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{unc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{o}{.}\PYG{l+m+mi}{001}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{con1\PYGZus{}v\PYGZus{}con2\PYGZus{}stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{thr\PYGZus{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_43_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{con1\PYGZus{}v\PYGZus{}con2\PYGZus{}stats} \PYG{o}{=} \PYG{n}{con1\PYGZus{}v\PYGZus{}con2}\PYG{o}{.}\PYG{n}{ttest}\PYG{p}{(}\PYG{n}{threshold\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fdr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{o}{.}\PYG{l+m+mi}{05}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{con1\PYGZus{}v\PYGZus{}con2\PYGZus{}stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{thr\PYGZus{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Thresholding_Group_Analyses_44_1}.png}

Looks like there are some significant differences that survive in early visual cortex and also in other regions of the brain.

This concludes are very quick overview to performing univariate analyses in fMRI data analysis.

We will continue to add more advanced tutorials to the dartbrains.org website. Stay tuned!


\subsection{Exercises}
\label{\detokenize{content/Thresholding_Group_Analyses:exercises}}

\subsubsection{Exercise 1. Bonferroni Correction Simulation}
\label{\detokenize{content/Thresholding_Group_Analyses:exercise-1-bonferroni-correction-simulation}}
Using the Grid Simulation code above, try to find how much larger the signal needs to be using a Bonferroni Correction until we can recover 100\% of the true signal, while controlling a family wise error false\sphinxhyphen{}positive rate of p \textless{} 0.05.


\subsubsection{Exercise 2. Which regions are more involved with visual compared to auditory sensory processing?}
\label{\detokenize{content/Thresholding_Group_Analyses:exercise-2-which-regions-are-more-involved-with-visual-compared-to-auditory-sensory-processing}}\begin{itemize}
\item {} 
run a group level t\sphinxhyphen{}test and threshold using an uncorrected voxel\sphinxhyphen{}wise threshold of p \textless{} 0.05, p \textless{} 0.005, and p \textless{} 0.001.

\item {} 
plot each of the results

\item {} 
write each file to your output folder.

\end{itemize}


\subsubsection{Exercise 3. Which regions are more involved in processing numbers compared to words?}
\label{\detokenize{content/Thresholding_Group_Analyses:exercise-3-which-regions-are-more-involved-in-processing-numbers-compared-to-words}}\begin{itemize}
\item {} 
run a group level t\sphinxhyphen{}test, using a correcte FDR threshold of q \textless{} 0.05.

\item {} 
plot the results

\item {} 
write the file to your output folder.

\end{itemize}


\section{Connectivity}
\label{\detokenize{content/Connectivity:connectivity}}\label{\detokenize{content/Connectivity::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

So far, we have primarily been focusing on analyses related to task evoked brain activity. However, an entirely different way to study the brain is to characterize how it is intrinsically connected. There are many different ways to study functional connectivity.

The primary division is studying how brain regions are \sphinxstyleemphasis{structurally} connected. In animal studies this might involve directly tracing bundles of neurons that are connected to other neurons. Diffusion imaging is a common way in which we can map how bundles of white matter are connected to each region, based on the direction in which water diffuses along white matter tracks. There are many different techniques such as fractional ansiotropy and probablistic tractography. We will not be discussing structural connectivity in this course.

An alternative approach to studying connectivity is to examine how brain regions covary with each other in time. This is referred to as \sphinxstyleemphasis{functional connectivity}, but it is better to think about it as temporal covariation between regions as this does not necessarily imply that two regions are directly communication with each other.

\sphinxincludegraphics{{mediation}.png}

For example, regions can \sphinxstyleemphasis{directly} influence each other, or they can \sphinxstyleemphasis{indirectly} influence each other via a mediating region, or they can be affected similarly by a \sphinxstyleemphasis{shared influence}. These types of figures are often called \sphinxstyleemphasis{graphs}. These types of \sphinxstyleemphasis{graphical} models can be \sphinxstyleemphasis{directed} or \sphinxstyleemphasis{undirected}. Directed graphs imply a causal relationship, where one region A directly influence another region B. Directed graphs or \sphinxstyleemphasis{causal models} are typically described as \sphinxstyleemphasis{effective connectivity}, while undirected graphs in which the relationship is presumed to be bidirectional are what we typically describe as \sphinxstyleemphasis{functional connectivity}.

In this tutorial, we will work through examples on:
\begin{itemize}
\item {} 
Seed\sphinxhyphen{}based functional connectivity

\item {} 
Psychophysiological interactions

\item {} 
Principal Components Analysis

\item {} 
Graph Theory

\end{itemize}

Let’s start by watching a short overview of connectivity by Martin Lindquist.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{J0KX\PYGZus{}rW0hmc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_1_0}.jpg}

Now, let’s dive in a little bit deeper into the specific details of functional connectivity.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OVAQujut\PYGZus{}1o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_3_0}.jpg}


\subsection{Functional Connectivity}
\label{\detokenize{content/Connectivity:functional-connectivity}}

\subsubsection{Seed Voxel Correlations}
\label{\detokenize{content/Connectivity:seed-voxel-correlations}}
One relatively simple way to calculate functional connectivity is to compute the temporal correlation between two regions of interest (ROIs). Typically, this is done by extracting the temporal response from a \sphinxstyleemphasis{seed voxel} or the average response within a \sphinxstyleemphasis{seed region}. Then this time course is regressed against all other voxels in the brain to produce a whole brain map of anywhere that shares a similar time course to the seed.

Let’s try it ourselves with an example subject from the Pinel Localizer dataset. First, let’s import the modules we need for this tutorial and set our paths.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{,} \PYG{n}{Design\PYGZus{}Matrix}\PYG{p}{,} \PYG{n}{Adjacency}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{mask} \PYG{k+kn}{import} \PYG{n}{expand\PYGZus{}mask}\PYG{p}{,} \PYG{n}{roi\PYGZus{}to\PYGZus{}brain}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{zscore}\PYG{p}{,} \PYG{n}{fdr}\PYG{p}{,} \PYG{n}{one\PYGZus{}sample\PYGZus{}permutation}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{file\PYGZus{}reader} \PYG{k+kn}{import} \PYG{n}{onsets\PYGZus{}to\PYGZus{}dm}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{plotting} \PYG{k+kn}{import} \PYG{n}{component\PYGZus{}viewer}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{binom}\PYG{p}{,} \PYG{n}{ttest\PYGZus{}1samp}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{pairwise\PYGZus{}distances}
\PYG{k+kn}{from} \PYG{n+nn}{copy} \PYG{k+kn}{import} \PYG{n}{deepcopy}
\PYG{k+kn}{import} \PYG{n+nn}{networkx} \PYG{k}{as} \PYG{n+nn}{nx}
\PYG{k+kn}{from} \PYG{n+nn}{nilearn}\PYG{n+nn}{.}\PYG{n+nn}{plotting} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}stat\PYGZus{}map}\PYG{p}{,} \PYG{n}{view\PYGZus{}img\PYGZus{}on\PYGZus{}surf}
\PYG{k+kn}{from} \PYG{n+nn}{bids} \PYG{k+kn}{import} \PYG{n}{BIDSLayout}\PYG{p}{,} \PYG{n}{BIDSValidator}
\PYG{k+kn}{import} \PYG{n+nn}{nibabel} \PYG{k}{as} \PYG{n+nn}{nib}

\PYG{n}{base\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/Users/lukechang/Dropbox/Dartbrains}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{data\PYGZus{}dir} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{base\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Localizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{layout} \PYG{o}{=} \PYG{n}{BIDSLayout}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{n}{derivatives}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

Now let’s load an example participant’s preprocessed functional data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sub} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{fwhm}\PYG{o}{=}\PYG{l+m+mi}{6}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{sub}\PYG{p}{,} \PYG{n}{task}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{localizer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{return\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{file}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{smoothed} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{smooth}\PYG{p}{(}\PYG{n}{fwhm}\PYG{o}{=}\PYG{n}{fwhm}\PYG{p}{)}
\end{sphinxVerbatim}

Next we need to pick an ROI. Pretty much any type of ROI will work.

In this example, we will be using a whole brain parcellation based on similar patterns of coactivation across over 10,000 published studies available in neurosynth (see this paper for more \sphinxhref{http://cosanlab.com/static/papers/delaVega\_2016\_JNeuro.pdf}{details}). We will be using a parcellation of 50 different functionally similar ROIs.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mask} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://neurovault.org/media/images/8423/k50\PYGZus{}2mm.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{mask}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_9_0}.png}

Each ROI in this parcellation has its own unique number. We can expand this so that each ROI becomes its own binary mask using \sphinxcode{\sphinxupquote{nltools.mask.expand\_mask}}.

Let’s plot the first 5 masks.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mask\PYGZus{}x} \PYG{o}{=} \PYG{n}{expand\PYGZus{}mask}\PYG{p}{(}\PYG{n}{mask}\PYG{p}{)}

\PYG{n}{f} \PYG{o}{=} \PYG{n}{mask\PYGZus{}x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_11_0}.png}

To use any mask we just need to index it by the correct label.

Let’s start by using the vmPFC mask (ROI=32) to use as a seed in a functional connectivity analysis.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{vmpfc} \PYG{o}{=} \PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{extract\PYGZus{}roi}\PYG{p}{(}\PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask\PYGZus{}x}\PYG{p}{[}\PYG{l+m+mi}{32}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{vmpfc}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mean Intensitiy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (TRs)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 0, \PYGZsq{}Time (TRs)\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_13_1}.png}

Okay, now let’s build our regression design matrix to perform the whole\sphinxhyphen{}brain functional connectivity analysis.

The goal is to find which regions in the brain have a similar time course to the vmPFC, controlling for all of our covariates (i.e., nuisance regressors).

Functional connectivity analyses are particularly sensitive to artifacts that might induce a temporal relationship, particularly head motion (See this \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1053811911011815}{article} by Jonathan Power for more details). This means that we will need to use slightly different steps to preprocess data for this type of analyis then a typical event related mass univariate analysis.

We are going to remove the mean from our vmPFC signal. We are also going to include the average activity in CSF as an additional nuisance regressor to remove physiological artifacts. Finally, we will be including our 24 motion covariates as well as linear and quadratic trends. We need to be a little careful about filtering as the normal high pass filter for an event related design might be too short and will remove potential signals of interest.

Resting state researchers also often remove the global signal, which can reduce physiological and motion related artifacts and also increase the likelihood of observing negative relationships with your seed regressor (i.e., anticorrelated). This procedure has remained quite controversial in practice (see \sphinxhref{https://www.physiology.org/doi/full/10.1152/jn.90777.2008}{here} \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1053811908010264}{here}, \sphinxhref{https://www.pnas.org/content/107/22/10238.short}{here}, and \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1053811916306711}{here} for a more in depth discussion). We think that in general including covariates like CSF should be sufficient. It is also common to additionally include covariates from white matter masks, and also multiple principal components of this signal rather than just the mean (see more details about \sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2214855/}{compcorr}.

Overall, this code should seem very familiar as it is pretty much the same procedure we used in the single subject GLM tutorial. However, instead of modeling the task design, we are interested in calculating the functional connectivity with the vmPFC.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tr} \PYG{o}{=} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get\PYGZus{}tr}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fwhm} \PYG{o}{=} \PYG{l+m+mi}{6}
\PYG{n}{n\PYGZus{}tr} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}motion\PYGZus{}covariates}\PYG{p}{(}\PYG{n}{mc}\PYG{p}{,} \PYG{n}{tr}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{z\PYGZus{}mc} \PYG{o}{=} \PYG{n}{zscore}\PYG{p}{(}\PYG{n}{mc}\PYG{p}{)}
    \PYG{n}{all\PYGZus{}mc} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{z\PYGZus{}mc}\PYG{p}{,} \PYG{n}{z\PYGZus{}mc}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{z\PYGZus{}mc}\PYG{o}{.}\PYG{n}{diff}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{z\PYGZus{}mc}\PYG{o}{.}\PYG{n}{diff}\PYG{p}{(}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{all\PYGZus{}mc}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{n}{value}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{Design\PYGZus{}Matrix}\PYG{p}{(}\PYG{n}{all\PYGZus{}mc}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{)}


\PYG{n}{vmpfc} \PYG{o}{=} \PYG{n}{zscore}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{vmpfc}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{vmpfc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{csf\PYGZus{}mask} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{base\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{masks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{csf.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{csf} \PYG{o}{=} \PYG{n}{zscore}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{extract\PYGZus{}roi}\PYG{p}{(}\PYG{n}{mask}\PYG{o}{=}\PYG{n}{csf\PYGZus{}mask}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{csf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{spikes} \PYG{o}{=} \PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{find\PYGZus{}spikes}\PYG{p}{(}\PYG{n}{global\PYGZus{}spike\PYGZus{}cutoff}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{diff\PYGZus{}spike\PYGZus{}cutoff}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{covariates} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{sub}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{mc} \PYG{o}{=} \PYG{n}{covariates}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{mc\PYGZus{}cov} \PYG{o}{=} \PYG{n}{make\PYGZus{}motion\PYGZus{}covariates}\PYG{p}{(}\PYG{n}{mc}\PYG{p}{,} \PYG{n}{tr}\PYG{p}{)}
\PYG{n}{dm} \PYG{o}{=} \PYG{n}{Design\PYGZus{}Matrix}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{vmpfc}\PYG{p}{,} \PYG{n}{csf}\PYG{p}{,} \PYG{n}{mc\PYGZus{}cov}\PYG{p}{,} \PYG{n}{spikes}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{labels}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TR}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{)}
\PYG{n}{dm} \PYG{o}{=} \PYG{n}{dm}\PYG{o}{.}\PYG{n}{add\PYGZus{}poly}\PYG{p}{(}\PYG{n}{order}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{include\PYGZus{}lower}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{X} \PYG{o}{=} \PYG{n}{dm}
\PYG{n}{stats} \PYG{o}{=} \PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{regress}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{vmpfc\PYGZus{}conn} \PYG{o}{=} \PYG{n}{stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{vmpfc\PYGZus{}conn}\PYG{o}{.}\PYG{n}{threshold}\PYG{p}{(}\PYG{n}{upper}\PYG{o}{=}\PYG{l+m+mi}{25}\PYG{p}{,} \PYG{n}{lower}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{25}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_16_0}.png}

Notice how this analysis identifies the default network? This analysis is very similar to the \sphinxhref{https://www.pnas.org/content/102/27/9673/}{original papers} that identified the default mode network using resting state data.

For an actual analysis, we would need to repeat this procedure over all of the participants in our sample and then perform a second level group analysis to identify which voxels are consistently coactive with the vmPFC. We will explore group level analyses in the exercises.


\subsubsection{Psychophysiological Interactions}
\label{\detokenize{content/Connectivity:psychophysiological-interactions}}
Suppose we were interested in seeing if the vmPFC was connected to other regions differently when performing a finger tapping task compared to all other conditions. To compute this analysis, we will need to create a new design matrix that combines the motor regressors and then calculates an interaction term between the seed region activity (e.g., vmpfc) and the condition of interest (e.g., motor).

This type of analysis called, \sphinxstyleemphasis{psychophysiological} interactions was originally \sphinxhref{https://www.fil.ion.ucl.ac.uk/spm/doc/papers/karl\_ppi.pdf}{proposed} by Friston et al., 1997. For a more hands on and practical discussion read this \sphinxhref{https://pdfs.semanticscholar.org/dd86/1acdb332ea7fa9de8fb677a4048651eaea02.pdf}{paper} and watch this \sphinxhref{https://www.youtube.com/watch?v=L3iBhfEYEgE}{video} by Jeanette Mumford and a follow up \sphinxhref{https://www.youtube.com/watch?v=M8APlF6oBgA}{video}) of a more generalized method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nib}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{raw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{FileNotFoundError}\PYG{g+gWhitespace}{                         }Traceback (most recent call last)
\PYG{n+nn}{\PYGZti{}/anaconda3/lib/python3.7/site\PYGZhy{}packages/nibabel/loadsave.py} in \PYG{n+ni}{load}\PYG{n+nt}{(filename, **kwargs)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{41}     \PYG{k}{try}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{42}         \PYG{n}{stat\PYGZus{}result} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{stat}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{43}     \PYG{k}{except} \PYG{n+ne}{OSError}\PYG{p}{:}

\PYG{n+ne}{FileNotFoundError}: [Errno 2] No such file or directory: \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/sub\PYGZhy{}S01/func/sub\PYGZhy{}S01\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}

\PYG{n}{During} \PYG{n}{handling} \PYG{n}{of} \PYG{n}{the} \PYG{n}{above} \PYG{n}{exception}\PYG{p}{,} \PYG{n}{another} \PYG{n}{exception} \PYG{n}{occurred}\PYG{p}{:}

\PYG{n+ne}{FileNotFoundError}\PYG{g+gWhitespace}{                         }Traceback (most recent call last)
\PYG{o}{\PYGZlt{}}\PYG{n}{ipython}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{input}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{25}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{6}\PYG{n}{aac70d09b2b}\PYG{o}{\PYGZgt{}} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n}{nib}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{raw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{)}

\PYG{n+nn}{\PYGZti{}/anaconda3/lib/python3.7/site\PYGZhy{}packages/nibabel/loadsave.py} in \PYG{n+ni}{load}\PYG{n+nt}{(filename, **kwargs)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{42}         \PYG{n}{stat\PYGZus{}result} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{stat}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{43}     \PYG{k}{except} \PYG{n+ne}{OSError}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{44}         \PYG{k}{raise} \PYG{n+ne}{FileNotFoundError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{No such file or no access: }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{filename}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{45}     \PYG{k}{if} \PYG{n}{stat\PYGZus{}result}\PYG{o}{.}\PYG{n}{st\PYGZus{}size} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{:}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{46}         \PYG{k}{raise} \PYG{n}{ImageFileError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Empty file: }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{filename}\PYG{p}{)}

\PYG{n+ne}{FileNotFoundError}: No such file or no access: \PYGZsq{}/Users/lukechang/Dropbox/Dartbrains/data/Localizer/sub\PYGZhy{}S01/func/sub\PYGZhy{}S01\PYGZus{}task\PYGZhy{}localizer\PYGZus{}bold.nii.gz\PYGZsq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{load\PYGZus{}bids\PYGZus{}events}\PYG{p}{(}\PYG{n}{layout}\PYG{p}{,} \PYG{n}{subject}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}Create a design\PYGZus{}matrix instance from BIDS event file\PYGZsq{}\PYGZsq{}\PYGZsq{}}
    
    \PYG{n}{tr} \PYG{o}{=} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get\PYGZus{}tr}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{n\PYGZus{}tr} \PYG{o}{=} \PYG{n}{nib}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{subject}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{raw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}

    \PYG{n}{onsets} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{subject}\PYG{p}{,} \PYG{n}{suffix}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{events}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{onsets}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Onset}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Duration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Stim}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{k}{return} \PYG{n}{onsets\PYGZus{}to\PYGZus{}dm}\PYG{p}{(}\PYG{n}{onsets}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{,} \PYG{n}{run\PYGZus{}length}\PYG{o}{=}\PYG{n}{n\PYGZus{}tr}\PYG{p}{)}

\PYG{n}{dm} \PYG{o}{=} \PYG{n}{load\PYGZus{}bids\PYGZus{}events}\PYG{p}{(}\PYG{n}{layout}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{motor\PYGZus{}variables} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{video\PYGZus{}left\PYGZus{}hand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{audio\PYGZus{}left\PYGZus{}hand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{video\PYGZus{}right\PYGZus{}hand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{audio\PYGZus{}right\PYGZus{}hand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{ppi\PYGZus{}dm} \PYG{o}{=} \PYG{n}{dm}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{motor\PYGZus{}variables}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{ppi\PYGZus{}dm}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{motor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{n}{dm}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{motor\PYGZus{}variables}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ppi\PYGZus{}dm\PYGZus{}conv} \PYG{o}{=} \PYG{n}{ppi\PYGZus{}dm}\PYG{o}{.}\PYG{n}{convolve}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ppi\PYGZus{}dm\PYGZus{}conv}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{vmpfc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{vmpfc}
\PYG{n}{ppi\PYGZus{}dm\PYGZus{}conv}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{vmpfc\PYGZus{}motor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{ppi\PYGZus{}dm\PYGZus{}conv}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{vmpfc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{*}\PYG{n}{ppi\PYGZus{}dm\PYGZus{}conv}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{motor\PYGZus{}c0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{dm} \PYG{o}{=} \PYG{n}{Design\PYGZus{}Matrix}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{ppi\PYGZus{}dm\PYGZus{}conv}\PYG{p}{,} \PYG{n}{csf}\PYG{p}{,} \PYG{n}{mc\PYGZus{}cov}\PYG{p}{,} \PYG{n}{spikes}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{labels}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TR}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{)}
\PYG{n}{dm} \PYG{o}{=} \PYG{n}{dm}\PYG{o}{.}\PYG{n}{add\PYGZus{}poly}\PYG{p}{(}\PYG{n}{order}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{include\PYGZus{}lower}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{dm}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/nltools\PYGZhy{}0.4.2\PYGZhy{}py3.7.egg/nltools/file\PYGZus{}reader.py:141: UserWarning: Computed onsets for video\PYGZus{}right\PYGZus{}hand are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!
  f\PYGZdq{}Computed onsets for \PYGZob{}data.Stim.unique()[i]\PYGZcb{} are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!\PYGZdq{}
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/nltools\PYGZhy{}0.4.2\PYGZhy{}py3.7.egg/nltools/file\PYGZus{}reader.py:141: UserWarning: Computed onsets for video\PYGZus{}left\PYGZus{}hand are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!
  f\PYGZdq{}Computed onsets for \PYGZob{}data.Stim.unique()[i]\PYGZcb{} are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!\PYGZdq{}
/Users/lukechang/anaconda3/lib/python3.7/site\PYGZhy{}packages/nltools\PYGZhy{}0.4.2\PYGZhy{}py3.7.egg/nltools/file\PYGZus{}reader.py:141: UserWarning: Computed onsets for audio\PYGZus{}computation are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!
  f\PYGZdq{}Computed onsets for \PYGZob{}data.Stim.unique()[i]\PYGZcb{} are inconsistent with expected values. Please manually verify the outputted Design\PYGZus{}Matrix!\PYGZdq{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_20_1}.png}

Okay, now we are ready to run the regression analysis and inspect the interaction term to find regions where the connectivity profile changes as a function of the motor task.

We will run the regression and smooth all of the images, and then examine the beta image for the PPI interaction term.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{X} \PYG{o}{=} \PYG{n}{dm}
\PYG{n}{ppi\PYGZus{}stats} \PYG{o}{=} \PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{regress}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{vmpfc\PYGZus{}motor\PYGZus{}ppi} \PYG{o}{=} \PYG{n}{ppi\PYGZus{}stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{beta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{X}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{vmpfc\PYGZus{}motor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}

\PYG{n}{vmpfc\PYGZus{}motor\PYGZus{}ppi}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_22_0}.png}

This analysis tells us which regions are more functionally connected with the vmPFC during the motor conditions relative to the rest of experiment.

We can make a thresholded interactive plot to interrogate these results, but it looks like it identifies the ACC/pre\sphinxhyphen{}SMA.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{vmpfc\PYGZus{}motor\PYGZus{}ppi}\PYG{o}{.}\PYG{n}{iplot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatText(value=0.0, description=\PYGZsq{}Threshold\PYGZsq{}), HTML(value=\PYGZsq{}Image is 3D\PYGZsq{}, description=\PYGZsq{}Vo…
\end{sphinxVerbatim}


\subsubsection{Dynamic Connectivity}
\label{\detokenize{content/Connectivity:dynamic-connectivity}}
All of the methods we have discussed so far assume that the relationship between two regions is stationary \sphinxhyphen{} or remains constant over the entire dataset. However, it is possible that voxels are connected to other voxels at specific points in time, but then change how they are connected when they are computing a different function or in different psychological state.

Time\sphinxhyphen{}varying connectivity is beyond the scope of the current tutorial, but we encourage you to watch this \sphinxhref{https://www.youtube.com/watch?v=lV9thGD18JI\&list=PLfXA4opIOVrEFBitfTGRPppQmgKp7OO-F\&index=22\&t=0s}{video} from Principles of fMRI for more details


\subsection{Effective Connectivity}
\label{\detokenize{content/Connectivity:effective-connectivity}}
Effective connectivity refers to the degree that one brain region has a directed influence on another region. This approach requires making a number of assumptions about the data and requires testing how well a particular model describes the data. Typically, most researchers will create a model of a small number of nodes and compare different models to each other. This is because the overall model fit is typically in itself uninterpretable and because formulating large models can be quite difficult and computationally expensive. The number of connections can be calculated as:

\(connections = \frac{n(n-1)}{2}\), where \(n\) is the total number of nodes.

Let’s watch a short video by Martin Lindquist that provides an overview to different approaches to effectivity connectivity.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gv5ENgW0bbs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_27_0}.jpg}


\subsubsection{Structural Equation Modeling}
\label{\detokenize{content/Connectivity:structural-equation-modeling}}
Structural equation modeling (SEM) is one early technique that was used to model the causal relationship between multiple nodes. SEM requires specifying a causal relationship between nodes in terms of a set of linear equations. The parameters of this system of equations reflects the connectivity matrix. Users are expected to formulate their own hypothesized relationship between variables with a value of one when there is an expected relationship, and zero when there is no relationship. Then we estimate the parameters of the model and evaluate how well the model describes the observed data.

\sphinxincludegraphics{{sem}.png}

We will not be discussing this method in much detail. In practice, this method is more routinely used to examine how brain activations mediate relationships between other regions, or between different psychological constructs (e.g., X \sphinxhyphen{}\textgreater{} Z \sphinxhyphen{}\textgreater{} Y).

Here are a couple of videos specifically examining how to conduct mediation and moderation analyses from Principles of fMRI (\sphinxhref{https://www.youtube.com/watch?v=0YqWXIfpu20}{Mediation and Moderation Part I},
\sphinxhref{https://www.youtube.com/watch?v=0YqWXIfpu20}{Mediation and Moderation Part II})


\subsubsection{Granger Causality}
\label{\detokenize{content/Connectivity:granger-causality}}
Granger causality was originally developed in econometrics and is used to determine temporal causality. The idea is to quantify how past values of one brain region predict the current value of another brain region. This analysis can also be performed in the frequency domain using measures of coherence between two regions. In general, this technique is rarely used in fMRI data analysis as it requires making assumptions that all regions have the same hemodynamic response function (which does not seem to be true), and that the relationship is stationary, or not varying over time.

Here is a \sphinxhref{https://www.youtube.com/watch?v=yE9aBHQ7bnA}{video} from Principles of fMRI explaining Granger Causality in more detail.


\subsubsection{Dynamic Causal Modeling}
\label{\detokenize{content/Connectivity:dynamic-causal-modeling}}
Dynamic Causal Modeling (DCM) is a method specifically developed for conducting causal analyses between regions of the brain for fMRI data. The key innovation is that the developers of this method have specified a generative model for how neuronal firing will be reflected in observed BOLD activity. This addresess one of the problems with SEM, which assumes that each ROI has the same hemodynamic response.

In practice, DCM is computationally expensive to estimate and researchers typically specify a couple small models and perform a model comparison (e.g., bayesian model comparison) to determine, which model best explains the data from a set of proposed models.

Here is a \sphinxhref{https://www.youtube.com/watch?v=JoJKoq5gmH8}{video} from Principles of fMRI explaining Dynamic Causal Modeling in more detail.


\subsection{Multivariate Decomposition}
\label{\detokenize{content/Connectivity:multivariate-decomposition}}
So far we have discussed functional connectivity in terms of pairs of regions. However, voxels are most likely not independent from each other and we may want to figure out some latent spatial components that are all functionally connected with each (i.e., covary similarly in time).

To do this type of analysis, we typically use what are called \sphinxstyleemphasis{multivariate decomposition} methods, which attempt to factorize a data set (i.e., time by voxels) into a lower dimensional set of components, where each has their own unique time course.

The most common decomposition methods or Principal Components Analysis (PCA) and Independent Components Analysis (ICA).

Let’s watch a short video by Martin Lindquist to learn more about decomposition.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Klp\PYGZhy{}8t5GLEg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_32_0}.jpg}


\subsubsection{Principal Components Analysis}
\label{\detokenize{content/Connectivity:principal-components-analysis}}
Principal Components Analysis (PCA) is a multivariate procedure that attempts to explain the variance\sphinxhyphen{}covariance structure of a high dimensional random vector. In this procedure, a set of correlated variables are transformed int a set of uncorrelated variables, ordered by the amount of variance in the data that they explain.

In fMRI, we use PCA to find spatial maps or \sphinxstyleemphasis{eigenimages} in the data. This is usually computed using Singular Value Decomposition (SVD). This operation is defined as:

\(X = USV^T\), where \(V^T V = I\), \(U^T U = I\), and \(S\) is a diagonal matrix whose elements are called singular values.

In practice, \(V\) corresponds to the eigenimages or spatial components and \(U\) corresponds to the transformation matrix to convert the eigenimages into a timecourse. \(S\) reflects the amount of scaling for each component.

\sphinxincludegraphics{{svd}.png}

SVD is conceptually very similar to regression. We are trying to explain a matrix \(X\) as a linear combination of components. Each term in the equation reflects a unique (i.e., orthogonal) multivariate signal present in \(X\). For example, the \(nth\) signal in X can be described by the dot product of a time course \(u_n\) and the spatial map \(Vn^T\)  scaled by \(s_n\).

\(X = s_1 u_1 v_1^T + s_2 u_2 v_2^T + s_n u_n v_n^T\)

Let’s try running a PCA on our single subject data.

First, let’s denoise our data using a GLM comprised only of nuisance regressors. We will then work with the \sphinxstyleemphasis{residual} of this model, or what remains of our data that was not explained by the denoising model. This is essentially identical to the vmPFC analysis, except that we will not be including any seed regressors. We will then be working with the residual of our regression, which is the remaining signal after removing any variance associated with our covariates.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{csf\PYGZus{}mask} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{base\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{masks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{csf.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{csf} \PYG{o}{=} \PYG{n}{zscore}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{extract\PYGZus{}roi}\PYG{p}{(}\PYG{n}{mask}\PYG{o}{=}\PYG{n}{csf\PYGZus{}mask}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{csf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{spikes} \PYG{o}{=} \PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{find\PYGZus{}spikes}\PYG{p}{(}\PYG{n}{global\PYGZus{}spike\PYGZus{}cutoff}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{diff\PYGZus{}spike\PYGZus{}cutoff}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{covariates} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{subject}\PYG{o}{=}\PYG{n}{sub}\PYG{p}{,} \PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{extension}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{path}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{mc} \PYG{o}{=} \PYG{n}{covariates}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{trans\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rot\PYGZus{}z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{mc\PYGZus{}cov} \PYG{o}{=} \PYG{n}{make\PYGZus{}motion\PYGZus{}covariates}\PYG{p}{(}\PYG{n}{mc}\PYG{p}{,} \PYG{n}{tr}\PYG{p}{)}
\PYG{n}{dm} \PYG{o}{=} \PYG{n}{Design\PYGZus{}Matrix}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{vmpfc}\PYG{p}{,} \PYG{n}{csf}\PYG{p}{,} \PYG{n}{mc\PYGZus{}cov}\PYG{p}{,} \PYG{n}{spikes}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{labels}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TR}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sampling\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{tr}\PYG{p}{)}
\PYG{n}{dm} \PYG{o}{=} \PYG{n}{dm}\PYG{o}{.}\PYG{n}{add\PYGZus{}poly}\PYG{p}{(}\PYG{n}{order}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{include\PYGZus{}lower}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{X} \PYG{o}{=} \PYG{n}{dm}
\PYG{n}{stats} \PYG{o}{=} \PYG{n}{smoothed}\PYG{o}{.}\PYG{n}{regress}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{smoothed\PYGZus{}denoised}\PYG{o}{=}\PYG{n}{stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{residual}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

Now let’s run a PCA on this participant’s denoised data. To do this, we will use the \sphinxcode{\sphinxupquote{.decompose()}} method from nltools. All we need to do is specify the algorithm we want to use, the dimension we want to reduce (i.e., time \sphinxhyphen{} ‘images’ or space ‘voxels’), and the number of components to estimate. Usually, we will be looking at reducing space based on similarity in time, so we will set \sphinxcode{\sphinxupquote{axis=\textquotesingle{}images\textquotesingle{}}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}components} \PYG{o}{=} \PYG{l+m+mi}{10}

\PYG{n}{pca\PYGZus{}stats\PYGZus{}output} \PYG{o}{=} \PYG{n}{smoothed\PYGZus{}denoised}\PYG{o}{.}\PYG{n}{decompose}\PYG{p}{(}\PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pca}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{n}{n\PYGZus{}components}\PYG{p}{)}
\end{sphinxVerbatim}

Now let’s inspect the components with our interactive component viewer. Remember the ICA tutorial? Hopefully, you are now better able to understand everything.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{component\PYGZus{}viewer}\PYG{p}{(}\PYG{n}{pca\PYGZus{}stats\PYGZus{}output}\PYG{p}{,} \PYG{n}{tr}\PYG{o}{=}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{get\PYGZus{}tr}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(BoundedIntText(value=0, description=\PYGZsq{}Component\PYGZsq{}, max=9), BoundedFloatText(value=2.0, des…
\end{sphinxVerbatim}

We can also examine the eigenvalues/singular values or scaling factor of each, which are the diagonals of \(S\).

These values are stored in the \sphinxcode{\sphinxupquote{\textquotesingle{}decomposition\_object\textquotesingle{}}} of the stats\_output and are in the variable called \sphinxcode{\sphinxupquote{.singular\_values\_}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{pca\PYGZus{}stats\PYGZus{}output}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{decomposition\PYGZus{}object}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{singular\PYGZus{}values\PYGZus{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Component}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Singular Values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Singular Values\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_40_1}.png}

We can use these values to calculate the overall variance explained by each component. These values are stored in the \sphinxcode{\sphinxupquote{\textquotesingle{}decomposition\_object\textquotesingle{}}} of the stats\_output and are in the variable called \sphinxcode{\sphinxupquote{.explained\_variance\_ratio\_}}.

These values can be used to create what is called a \sphinxstyleemphasis{scree} plot to figure out the percent variance of \(X\) explained by each component. Remember, in PCA, components are ordered by descending variance explained.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f}\PYG{p}{,}\PYG{n}{a} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{pca\PYGZus{}stats\PYGZus{}output}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{decomposition\PYGZus{}object}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}ratio\PYGZus{}}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Percent Variance Explained}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Component}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variance Explained}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{pca\PYGZus{}stats\PYGZus{}output}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{decomposition\PYGZus{}object}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{explained\PYGZus{}variance\PYGZus{}ratio\PYGZus{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Percent Variance Explained}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Component}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cumulative Variance Explained}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, \PYGZsq{}Cumulative Variance Explained\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_42_1}.png}


\subsubsection{Independent Components Analysis}
\label{\detokenize{content/Connectivity:independent-components-analysis}}
Independent Components Analysis (ICA) is a method to blindly separate a source signal into spatially independent components. This approach assumes that the data consts of \(p\) spatially independent components, which are linearly mixed and spatially fixed. PCA assumes orthonormality constraint, while ICA only assumes independence.

\(X = AS\), where \(A\) is the \sphinxstyleemphasis{mixing matrix} and \(S\) is the \sphinxstyleemphasis{source matrix}

In ICA we find an un\sphinxhyphen{}mixing matrix \(W\), such that \(Y = WX\) provides an approximation to \(S\). To estimate the mixing matrix, ICA assumes that the sources are (1) linearly mixed, (2) the components are statistically independent, and (3) the components are non\sphinxhyphen{}Gaussian.

It is trivial to run ICA on our data as it only requires switching \sphinxcode{\sphinxupquote{algorithm=\textquotesingle{}pca\textquotesingle{}}} to \sphinxcode{\sphinxupquote{algorithm=\textquotesingle{}ica\textquotesingle{}}} when using the \sphinxcode{\sphinxupquote{decompose()}} method.

We will experiment with this in our exercises.


\subsection{Graph Theory}
\label{\detokenize{content/Connectivity:graph-theory}}
Similar to describing the structure of social networks, graph theory has also been used to characterize regions of the brain based on how they connected to other regions. Nodes in the network typically describe specific brain regions and edges represent the strength of the association between each edge. That is, the network can be represented as a graph of pairwise relationships between each region of the brain.

There are many different metrics of graphs that can be used to describe the overall efficiency of a network (e.g., small worldness), or how connected a region is to other regions (e.g., degree, centrality), or how long it would take to send information from one node to another node (e.g., path length, connectivity).

\sphinxincludegraphics{{graph}.png}

Let’s watch a short video by Martin Lindquist providing a more in depth introduction to graph theory.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{v8ls5VED1ng}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_45_0}.jpg}

Suppose, we were interested in identifying which regions of the brain had the highest degree of centrality based on functional connectivity. There are many different ways to do this, but they all involve specifying a set of nodes (i.e., ROIs) and calculating the edges between each node. Finally, we would need to pick a centrality metric and calculate the overall level of centrality for each region.

Let’s do this quickly building off of our seed\sphinxhyphen{}based functional connectivity analysis.

Similar, to the PCA example, let’s work with the denoised data. First, let’s extract the average time course within each ROI from our 50 parcels and plot the results.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rois} \PYG{o}{=} \PYG{n}{smoothed\PYGZus{}denoised}\PYG{o}{.}\PYG{n}{extract\PYGZus{}roi}\PYG{p}{(}\PYG{n}{mask}\PYG{o}{=}\PYG{n}{mask}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{rois}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mean Intensitiy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time (TRs)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 0, \PYGZsq{}Time (TRs)\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_47_1}.png}

Now that we have specified our 50 nodes, we need to calculate the edges of the graph. We will be using pearson correlations. We will be using the \sphinxcode{\sphinxupquote{pairwise\_distances}} function from scikit\sphinxhyphen{}learn as it is much faster than most other correlation measures. We will then convert the distance metric into similarities by subtracting all of the values from 1.

Let’s visualize the resulting correlation matrix as a heatmap using seaborn.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{roi\PYGZus{}corr} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{pairwise\PYGZus{}distances}\PYG{p}{(}\PYG{n}{rois}\PYG{p}{,} \PYG{n}{metric}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{correlation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{roi\PYGZus{}corr}\PYG{p}{,} \PYG{n}{square}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RdBu\PYGZus{}r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_49_1}.png}

Now we need to convert this correlation matrix into a graph and calculate a centrality measure. We will use the \sphinxcode{\sphinxupquote{Adjacency}} class from nltools as it has many functions that are useful for working with this type of data, including casting these type of matrices into networkx graph objects.

We will be using the \sphinxhref{https://networkx.github.io/documentation/stable/}{networkx} python toolbox to work with graphs and compute different metrics of the graph.

Let’s calculate degree centrality, which is the total number of nodes each node is connected with. Unfortunately, many graph theory metrics require working with adjacency matrices, which are binary matrices indicating the presence of an edge or not. To create this, we will simply apply an arbitrary threshold to our correlation matrix.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{a} \PYG{o}{=} \PYG{n}{Adjacency}\PYG{p}{(}\PYG{n}{roi\PYGZus{}corr}\PYG{p}{,} \PYG{n}{matrix\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{similarity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{n}{x} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{a\PYGZus{}thresholded} \PYG{o}{=} \PYG{n}{a}\PYG{o}{.}\PYG{n}{threshold}\PYG{p}{(}\PYG{n}{upper}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{binarize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{a\PYGZus{}thresholded}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_51_0}.png}

Okay, now that we have a thresholded binary matrix, let’s cast our data into a networkx object and calculate the degree centrality of each ROI and make a quick plot of the graph.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{G} \PYG{o}{=} \PYG{n}{a\PYGZus{}thresholded}\PYG{o}{.}\PYG{n}{to\PYGZus{}graph}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{pos} \PYG{o}{=} \PYG{n}{nx}\PYG{o}{.}\PYG{n}{kamada\PYGZus{}kawai\PYGZus{}layout}\PYG{p}{(}\PYG{n}{G}\PYG{p}{)}
\PYG{n}{node\PYGZus{}and\PYGZus{}degree} \PYG{o}{=} \PYG{n}{G}\PYG{o}{.}\PYG{n}{degree}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{draw\PYGZus{}networkx\PYGZus{}edges}\PYG{p}{(}\PYG{n}{G}\PYG{p}{,} \PYG{n}{pos}\PYG{p}{,} \PYG{n}{width}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{draw\PYGZus{}networkx\PYGZus{}labels}\PYG{p}{(}\PYG{n}{G}\PYG{p}{,} \PYG{n}{pos}\PYG{p}{,} \PYG{n}{font\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{14}\PYG{p}{,} \PYG{n}{font\PYGZus{}color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{darkslategray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{nx}\PYG{o}{.}\PYG{n}{draw\PYGZus{}networkx\PYGZus{}nodes}\PYG{p}{(}\PYG{n}{G}\PYG{p}{,} \PYG{n}{pos}\PYG{p}{,} \PYG{n}{nodelist}\PYG{o}{=}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{node\PYGZus{}and\PYGZus{}degree}\PYG{p}{)}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                       \PYG{n}{node\PYGZus{}size}\PYG{o}{=}\PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{*}\PYG{l+m+mi}{100} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{node\PYGZus{}and\PYGZus{}degree}\PYG{p}{]}\PYG{p}{,}
                       \PYG{n}{node\PYGZus{}color}\PYG{o}{=}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{node\PYGZus{}and\PYGZus{}degree}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                       \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{Reds\PYGZus{}r}\PYG{p}{,} \PYG{n}{linewidths}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{edgecolors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{darkslategray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.collections.PathCollection at 0x7facb0faf810\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_53_1}.png}

We can also plot the distribution of degree using this threshold.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{G}\PYG{o}{.}\PYG{n}{degree}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Degree}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0.5, 0, \PYGZsq{}Degree\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_55_1}.png}

What if we wanted to map the degree of each node back onto the brain?

This would allow us to visualize which of the parcels had more direct pairwise connections.

To do this, we will simply scale our expanded binary mask object by the node degree. We will then combine the masks by concatenating through recasting as a brain\_data object and then summing across all ROIs.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{degree} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{G}\PYG{o}{.}\PYG{n}{degree}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{brain\PYGZus{}degree} \PYG{o}{=} \PYG{n}{roi\PYGZus{}to\PYGZus{}brain}\PYG{p}{(}\PYG{n}{degree}\PYG{p}{,} \PYG{n}{mask\PYGZus{}x}\PYG{p}{)}
\PYG{n}{brain\PYGZus{}degree}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Connectivity_57_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{view\PYGZus{}img\PYGZus{}on\PYGZus{}surf}\PYG{p}{(}\PYG{n}{brain\PYGZus{}degree}\PYG{o}{.}\PYG{n}{to\PYGZus{}nifti}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.html\PYGZus{}surface.SurfaceView at 0x7fac724072d0\PYGZgt{}
\end{sphinxVerbatim}

This analysis shows that the insula is one of the regions that appears to have the highest degree in this analysis. This is a fairly classic \sphinxhref{https://link.springer.com/article/10.1007/s00429-010-0262-0}{finding} with the insula frequently found to be highly connected with other regions. Of course, we are only looking at one subject in a very short task (and selecting a completely arbitrary cutoff). We would need to show this survives correction after performing a group analysis.


\subsection{Exercises}
\label{\detokenize{content/Connectivity:exercises}}
Let’s practice what we learned through a few different exercises.


\subsubsection{1) Let’s calculate seed\sphinxhyphen{}based functional connectivity using a different ROI \sphinxhyphen{} the right motor cortex}
\label{\detokenize{content/Connectivity:let-s-calculate-seed-based-functional-connectivity-using-a-different-roi-the-right-motor-cortex}}\begin{itemize}
\item {} 
Calculate functional connectivity using roi=48 with the whole brain.

\end{itemize}


\subsubsection{2) Calculate a group level analysis for this connectivity analysis}
\label{\detokenize{content/Connectivity:calculate-a-group-level-analysis-for-this-connectivity-analysis}}\begin{itemize}
\item {} 
this will require running this analysis over all subjects

\item {} 
then running a one sample t\sphinxhyphen{}test

\item {} 
then correcting for multiple tests with fdr.

\end{itemize}


\subsubsection{3) Calculate an ICA}
\label{\detokenize{content/Connectivity:calculate-an-ica}}\begin{itemize}
\item {} 
run an ICA analysis for subject01 with 5 components

\item {} 
plot each spatial component and its associated timecourse

\end{itemize}


\subsubsection{4) Calculate Eigenvector Centrality for each Region}
\label{\detokenize{content/Connectivity:calculate-eigenvector-centrality-for-each-region}}\begin{itemize}
\item {} 
figure out how to calculate eigenvector centrality and compute it for each region.

\end{itemize}


\subsubsection{5) Calculate a group level analysis for this graph theoretic analysis}
\label{\detokenize{content/Connectivity:calculate-a-group-level-analysis-for-this-graph-theoretic-analysis}}\begin{itemize}
\item {} 
this will require running this analysis over all subjects

\item {} 
then running a one sample t\sphinxhyphen{}test

\item {} 
then correcting for multiple tests with fdr.

\end{itemize}


\section{Multivariate Prediction}
\label{\detokenize{content/Multivariate_Prediction:multivariate-prediction}}\label{\detokenize{content/Multivariate_Prediction::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

The statistical methods we have discussed in this course so far have primarily been concerned with modeling activation in a \sphinxstyleemphasis{single voxel} and testing hypotheses in the form of “where in the brain is activation significantly greater in condition A relative condition B”. As you may recall this involved using multilevel modeling with the GLM, where a voxel’s time course was modeled using a first level GLM and then the contrast effect was aggregated across participants in the second level model. This procedure is often referred to as mass univariate testing and requires carefully considering how to correct for the many tests across voxels.
\begin{equation*}
\begin{split}\text{voxel} = \beta \cdot \text{task model} + \beta \cdot \text{covariates} + \epsilon\end{split}
\end{equation*}
A completely different approach to the problem is to reverse the regression equation and identify patterns of voxel activations that predict an outcome. This might be \sphinxstylestrong{classifying} between different conditions of a task, or \sphinxstylestrong{predicting} the intensity of a continuous outcome measure (e.g., emotion, pain, working memory load, etc).
\begin{equation*}
\begin{split}\text{outcome} = \sum_{i}^n \beta_i\cdot \text{voxel}_i + \epsilon\end{split}
\end{equation*}
Here we are learning a model of \(\beta\) values across the brain that when multiplied by new data will predict the intensity of a psychological state or outcome or the probability of being a specific state.
\begin{equation*}
\begin{split}\text{predicted outcome} = \text{model} \cdot \text{brain data}\end{split}
\end{equation*}
This is the general approach behind \sphinxstyleemphasis{supervised learning} algorithms. The intuition behind this approach is that brain signal might not be functionally localizable to a single region, but instead might be distributed throughout the brain. Patterns of brain activity can thus be used to \sphinxstyleemphasis{decode} psychological states.

The focus of this supervised learning approach is to accurately predict or classify the outcome, whereas the goal in classical statistics is to test hypotheses about \sphinxstyleemphasis{which} regressor explains the most independent variance of the dependent variable. These two approaches are complementary, but require thinking carefully about different issues.

In mass\sphinxhyphen{}univariate testing, we spent a lot of time thinking carefully about independence of errors (e.g., multi\sphinxhyphen{}level models) and correcting for multiple hypothesis tests. In multivariate prediction/classification or \sphinxstylestrong{multivoxel pattern analysis} as it is often called, we need to carefully think about \sphinxstylestrong{feature selection} \sphinxhyphen{}  which voxels we will include in our model, and \sphinxstylestrong{cross\sphinxhyphen{}validation} \sphinxhyphen{} how well our model will \sphinxstyleemphasis{generalize} to new data. In MVPA, we typically have more features (i.e., voxels) then data points (i.e., n \textless{} p), so this requires performing feature selection or a data reduction step. The algorithms used to learn patterns come from the field of machine\sphinxhyphen{}learning are very good at detecting patterns, but have a tendency to \sphinxstyleemphasis{overfit} the training data.

In this tutorial, we will cover the basic steps of multivariate prediction/classification:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{Data Extraction} \sphinxhyphen{} what data should we use to train model?

\item {} 
\sphinxstylestrong{Feature Selection} \sphinxhyphen{} which features or voxels should we use?

\item {} 
\sphinxstylestrong{Cross\sphinxhyphen{}validation} \sphinxhyphen{} how do we train and test the model?

\item {} 
\sphinxstylestrong{Model Interpretation} \sphinxhyphen{} how should we interpret the model?

\end{enumerate}


\subsection{Why MVPA?}
\label{\detokenize{content/Multivariate_Prediction:why-mvpa}}
For most of our tutorials, we have tended to focus on the basics of \sphinxstyleemphasis{how} to perform a specific type of analysis, and have largely ignored questions about \sphinxstyleemphasis{why} we might be interested in specific questions. While univariate GLM analyses allow us to localize which regions might be associated with a specific psychological process, MVPA analyses, and specifically multivariate decoding, allows us to identify distributed representations throughout the brain. These might be localizable to a single region, or may be diffusely encompass many different brain systems.

Before we dive into the details of how to conduct these analyses, let’ learn a little bit about the theoretical background motivating this appraoch in two videos from Tor Wager.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{YouTubeVideo}

\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{87yKz23sPnE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_2_0}.jpg}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FAyPEr7eu4M}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_3_0}.jpg}


\subsection{Important MVPA Concepts}
\label{\detokenize{content/Multivariate_Prediction:important-mvpa-concepts}}
Now, we are ready to dive into the details. In this tutorial, we will be using the nltools toolsbox to run these models, but also see (\sphinxhref{https://nilearn.github.io/}{nilearn}, \sphinxhref{https://brainiak.org/tutorials/}{brainiak}, and \sphinxhref{http://www.pymvpa.org/}{pyMPVA}) for excellent alternatives.

Running MVPA style analyses using multivariate regression is surprisingly easier and faster than univariate methods. All you need to do is specify the algorithm and cross\sphinxhyphen{}validation parameters. Currently, we have several different linear algorithms implemented from \sphinxhref{http://scikit-learn.org/stable/}{scikit\sphinxhyphen{}learn} in the nltools package.

To make sure you understand all of the key concepts involved in the practical aspects of conducting MVPA, let’s watch two short videos by Martin Lindquist before we dive into the code.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dJIb5bzkQHQ}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_5_0}.jpg}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{zKMsJyiL5Dc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_6_0}.jpg}


\subsubsection{Data Extraction}
\label{\detokenize{content/Multivariate_Prediction:data-extraction}}
The first step in MVPA is to decide what data you want to use to predict your outcome variable. Typically, researchers perform a temporal data reduction step, which involves estimating a standard univariate GLM using a single subject first\sphinxhyphen{}level model. This model will specify regressors for a single trial, or model a specific condition type over many trials. Just as in the standard univariate approach, these regressors are convolved with an HRF function. These models also usually include nuisance covariates (e.g., motion parameters, spikes, filters, linear trends, etc.). The estimated beta maps from this temporal reduction step are then used as the input data into the prediction model. Note that it is also possible to learn a \sphinxstyleemphasis{spatiotemporal} model that includes the voxels from each TR measured during a a given trial, but this is less common in practice.

First, let’s load the modules we need for this analysis.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Brain\PYGZus{}Data}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{mask} \PYG{k+kn}{import} \PYG{n}{expand\PYGZus{}mask}
\PYG{k+kn}{from} \PYG{n+nn}{bids} \PYG{k+kn}{import} \PYG{n}{BIDSLayout}\PYG{p}{,} \PYG{n}{BIDSValidator}
\PYG{k+kn}{from} \PYG{n+nn}{nilearn}\PYG{n+nn}{.}\PYG{n+nn}{plotting} \PYG{k+kn}{import} \PYG{n}{view\PYGZus{}img\PYGZus{}on\PYGZus{}surf}

\PYG{n}{data\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../data/localizer}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{layout} \PYG{o}{=} \PYG{n}{BIDSLayout}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{n}{derivatives}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

Now let’s load some data to train a model.

In this example, let’s continue to use data from the Pinel Localizer Task that we have been using throughout all of our tutorials. For our first analysis, let’s attempt to classify \sphinxstyleemphasis{Left} from \sphinxstyleemphasis{Right} motor activation. We will load a single beta image for each subject that we already estimated in earlier tutorials. We are sorting the files so that subjects are in the same order, then we are stacking all of the images together using \sphinxcode{\sphinxupquote{.append()}} such that the data looks like \(Subject_{1, left}, ... Subject_{n, left}, Subject_{1, right}, ... Subject_{n, right}\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{left\PYGZus{}file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*\PYGZus{}video\PYGZus{}left*.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{left\PYGZus{}file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)} 
\PYG{n}{left} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{left\PYGZus{}file\PYGZus{}list}\PYG{p}{)}

\PYG{n}{right\PYGZus{}file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*\PYGZus{}video\PYGZus{}right*.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{right\PYGZus{}file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{right} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{right\PYGZus{}file\PYGZus{}list}\PYG{p}{)}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{left}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{right}\PYG{p}{)}
\end{sphinxVerbatim}

Next, we need to create the labels or outcome variable to train the model. We will make a vector of ones and zeros to indicate left images and right images, respectively.

We assign this vector to the \sphinxcode{\sphinxupquote{data.Y}} attribute of the Brain\_Data instance.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Y} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{left\PYGZus{}file\PYGZus{}list}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{left\PYGZus{}file\PYGZus{}list}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{data}\PYG{o}{.}\PYG{n}{Y} \PYG{o}{=} \PYG{n}{Y}
\end{sphinxVerbatim}

okay, we are ready to go. Let’s now train our first model. We will use a support vector machine (SVM) to learn a pattern that can discriminate left from right motor responses across all 9 participants.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{svm\PYGZus{}stats} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{svm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{linear}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
overall accuracy: 1.00
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_14_1}.png}

the results of this analysis are stored in a dictionary.
\begin{itemize}
\item {} 
\sphinxstylestrong{Y}: training labels

\item {} 
\sphinxstylestrong{yfit\_all}: predicted labels

\item {} 
\sphinxstylestrong{dist\_from\_hyperplane\_all}: how far the prediction is from the classifier hyperplane through feature space, \textgreater{} 0 indicates left, while \textless{} 0 indicates right.

\item {} 
\sphinxstylestrong{intercept}: scalar value which indicates how much to add to the prediction to get the correct class label.

\item {} 
\sphinxstylestrong{weight\_map}: multivariate brain model

\item {} 
\sphinxstylestrong{mcr\_all}: overall model accuracy in classifying training data

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{svm\PYGZus{}stats}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dict\PYGZus{}keys([\PYGZsq{}Y\PYGZsq{}, \PYGZsq{}yfit\PYGZus{}all\PYGZsq{}, \PYGZsq{}dist\PYGZus{}from\PYGZus{}hyperplane\PYGZus{}all\PYGZsq{}, \PYGZsq{}intercept\PYGZsq{}, \PYGZsq{}weight\PYGZus{}map\PYGZsq{}, \PYGZsq{}mcr\PYGZus{}all\PYGZsq{}])
\end{sphinxVerbatim}

You can see that that the model can perfectly discriminate between left and right using the training data. This is great, but we definitely shouldn’t get our hopes up as this model is completely being overfit to the training data. To get an unbiased estimate of the accuracy we will need to test the model on independent data.

We can also examine the model weights more thoroughly by plotting it.  This shows that we see a very nice expected motor cortex representation, but notice that there are many other regions also contributing to the prediction.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{view\PYGZus{}img\PYGZus{}on\PYGZus{}surf}\PYG{p}{(}\PYG{n}{svm\PYGZus{}stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weight\PYGZus{}map}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}nifti}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.html\PYGZus{}surface.SurfaceView at 0x7f8214bf1650\PYGZgt{}
\end{sphinxVerbatim}


\subsubsection{Feature Selection}
\label{\detokenize{content/Multivariate_Prediction:feature-selection}}
Feature selection describes the process of deciding which features to include when training the model.  Here it is simply, which voxels should we use to train the model?

There are several ways to perform feature selection.  Searchlights are a popular approach.  I personally have a preference for using parcellation schemes.
\begin{itemize}
\item {} 
Parcellations are orders of magnitude computationally less expensive than searchlights.

\item {} 
Parcellations are easier to correct for multiple comparisons (50 vs 300k)

\item {} 
Parcellations can include regions distributed throughout the brain (searchlights are only local)

\item {} 
Parcellations can be integrated into a meta\sphinxhyphen{}model.

\end{itemize}

Here we download a single 50 parcel map from a forthcoming paper on conducting automated parcellations using neurosynth.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Yarkoni, T., de la Vega, A., \PYGZam{} Chang, L.J. (In Prep).  Fully automated meta\PYGZhy{}analytic clustering and decoding of human brain activity
\end{sphinxVerbatim}

Some of the details can be found \sphinxhref{http://cosanlab.com/static/papers/delaVega\_2016\_JNeuro.pdf}{here}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mask} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{..}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{masks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k50\PYGZus{}2mm.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{mask\PYGZus{}x} \PYG{o}{=} \PYG{n}{expand\PYGZus{}mask}\PYG{p}{(}\PYG{n}{mask}\PYG{p}{)}

\PYG{n}{mask}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_20_0}.png}

Let’s combine two parcels (left\sphinxhyphen{}26 and right\sphinxhyphen{}47 motor) to make a mask and use this as a feature extract method.

This means that we will only be training voxels to discriminate between the two conditions if they are in the right or left motor cortex.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{motor} \PYG{o}{=} \PYG{n}{mask\PYGZus{}x}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{26}\PYG{p}{,}\PYG{l+m+mi}{47}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{data\PYGZus{}masked} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{apply\PYGZus{}mask}\PYG{p}{(}\PYG{n}{motor}\PYG{p}{)}

\PYG{n}{svm\PYGZus{}stats\PYGZus{}masked} \PYG{o}{=} \PYG{n}{data\PYGZus{}masked}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{svm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{linear}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
overall accuracy: 1.00
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_22_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{svm\PYGZus{}stats\PYGZus{}masked}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weight\PYGZus{}map}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{iplot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
interactive(children=(FloatText(value=0.0, description=\PYGZsq{}Threshold\PYGZsq{}), HTML(value=\PYGZsq{}Image is 3D\PYGZsq{}, description=\PYGZsq{}Vo…
\end{sphinxVerbatim}

We can see that this also correctly learns that left motor cortex is positive while right cortex is negative \sphinxhyphen{} left vs right classification.  In addition, the training accuracy is still 100\%.


\subsubsection{Cross\sphinxhyphen{}Validation}
\label{\detokenize{content/Multivariate_Prediction:cross-validation}}
Clearly, our model is overfitting our training data. The next thing we need to do is to estimate how well our model will generalize to \sphinxstyleemphasis{new} data.  Ideally, we would have left out some data to test after we are done training and tuning our models.  This is called \sphinxstylestrong{holdout data} and should only be tested once when you are ready to write up your paper.

However, we don’t always have the luxury of having so much extra data and also we might want to tune our model using different algorithms, features, or adjusting hyperparameters of the model.

The best way to do this, is to use \sphinxstylestrong{cross\sphinxhyphen{}validation}. The idea behind this is to subdivide the data into training and testing partitions \sphinxhyphen{} k\sphinxhyphen{}folds cross\sphinxhyphen{}validation is a common method \sphinxhyphen{} divide the data into \(k\) separate folds and use all of the data except for one fold to train the model and then test the model using the left out fold. We iterate over this process for each fold. For example, consider k=2 or split\sphinxhyphen{}half cross\sphinxhyphen{}validation.

\sphinxincludegraphics{{cv}.png}

We divide the data into two partitions. We estimate the model using half of the data and test it on the other half and then evaluate how well the model performed. As you can see from this simulation, the model will almost always fit the training data better than the test data, because it is overfitting to the noise inherent to the training data, which is presumably independent across folds. More training data will lead to better estimation. This means that a k \textgreater{} 2 will usually result in better model estimates. When k=number of subjects, we call this \sphinxstyleemphasis{leave\sphinxhyphen{}one\sphinxhyphen{}subject\sphinxhyphen{}out} cross\sphinxhyphen{}validation.

One key concept to note is that it is very important to ensure that the data is independent across folds or this will lead to a biased and usually overly optimistic generalization. This can happen if you have multiple data from the same participant. You will need to make sure that the data from the same participants are held out together. We can do this by passing a vector of group labels to make sure that data within the same group are held out together. Another approach is to make sure that the data is equally representative across folds. We can use something called stratified sampling to achieve this (see \sphinxhref{http://cosanlab.com/static/papers/Changetal2015PLoSBiology.pdf}{here} for more details)

Let’s add cross\sphinxhyphen{}validation to our SVM model.  We will start with \(k=5\), and will pass a vector indicating subject labels as our grouping variable.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sub\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{basename}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{right\PYGZus{}file\PYGZus{}list}\PYG{p}{]}
\PYG{n}{subject\PYGZus{}id} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{sub\PYGZus{}list} \PYG{o}{+} \PYG{n}{sub\PYGZus{}list}\PYG{p}{)}

\PYG{n}{svm\PYGZus{}stats} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{svm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{cv\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kfolds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}folds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subject\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{subject\PYGZus{}id}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{linear}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
overall accuracy: 1.00
overall CV accuracy: 0.78
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_26_1}.png}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_26_2}.png}

Now we see that our whole\sphinxhyphen{}brain model is still performing very well \textasciitilde{}78\% accuracy.

What about our masked version?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{motor} \PYG{o}{=} \PYG{n}{mask\PYGZus{}x}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{26}\PYG{p}{,}\PYG{l+m+mi}{47}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{data\PYGZus{}masked} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{apply\PYGZus{}mask}\PYG{p}{(}\PYG{n}{motor}\PYG{p}{)}

\PYG{n}{svm\PYGZus{}stats\PYGZus{}masked} \PYG{o}{=} \PYG{n}{data\PYGZus{}masked}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{svm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{cv\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kfolds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}folds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subject\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{subject\PYGZus{}id}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{linear}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
overall accuracy: 1.00
overall CV accuracy: 0.83
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_28_1}.png}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_28_2}.png}

Wow, it looks like the model with feature selection actually outperforms the whole\sphinxhyphen{}brain model in cross\sphinxhyphen{}validation! 83\% \textgreater{} 78\% accuracy.

Why do you think this is the case?


\subsubsection{Regularization}
\label{\detokenize{content/Multivariate_Prediction:regularization}}
Another key concept that is used to help with feature selection is called regularization. Regularization is a method to help deal with \sphinxhref{https://en.wikipedia.org/wiki/Multicollinearity}{multicollinearity} and also avoid \sphinxhref{https://en.wikipedia.org/wiki/Overfitting}{overfitting}. Overfitting occurs when you have an overly complex model such as having more features(Xs) than observations(Y) (n \textless{} p). For instance, if you try to fit 10 observations with 10 features, each coefficient can be adjusted for a perfect fit but it wouldn’t generalize well. In other cases, you might face the problem of feature selection. If you have numerous variables, it is time consuming to try every single combination of features in your model to see what yields the best result.


Regularization attempts to solve this problem by introducting a loss function that penalizes the model for each additional features added to the model. There are two common types of loss functions \sphinxstyleemphasis{L1} and \sphinxstyleemphasis{L2}. L1 regularization is commonly referred to as \sphinxstyleemphasis{lasso} and leads to sparse solutions, where some regressors are set to zero. L2 regularization, does not lead to a sparse solution, but instead shrinks collinear variables towards zero. Elastic Nets are a type of model that combines L1 and L2 penalizations.


\paragraph{Lasso regression \sphinxhyphen{} L1 Regularization}
\label{\detokenize{content/Multivariate_Prediction:lasso-regression-l1-regularization}}
In short, \sphinxhref{http://stats.stackexchange.com/questions/17251/what-is-the-lasso-in-regression-analysis}{Lasso} is a feature selection method that reduces the number of features to use in a regression.This is useful if you have a lot of variables that are correlated or you have more variables than observations.


\paragraph{Ridge Regression \sphinxhyphen{} L2 Regularization}
\label{\detokenize{content/Multivariate_Prediction:ridge-regression-l2-regularization}}
The goal of the ridge function is to choose a penalty \(\lambda\) for which the coefficients are not rapidly changing and have “sensible” signs. It is especially useful when data suffers from multicollinearity, that is some of your predictor variables are highly correlated. Unlike LASSO, ridge does not produce a sparse solution, but rather shrinks variables that are highly collinear towards zero.


\paragraph{How do we determine the penalty value?}
\label{\detokenize{content/Multivariate_Prediction:how-do-we-determine-the-penalty-value}}
Both Lasso and Ridge regressions have a penalty hyperparameter \(\lambda\). Essentially, we want to select the regularization parameter by identifying the one from a set of possible values (e.g. grid search) that results in the best fit of the model to the data.  However, it is important to note that it is easy to introduce bias into this process by trying a bunch of alphas and selecting the one that works best.  This can lead to optimistic evaluations of how well your model works.

Cross\sphinxhyphen{}validation is an ideal method to deal with this.  We can use cross\sphinxhyphen{}validation to select the alpha while adding minimal bias to the overall model prediction.

Here we will demonstrate using both to select an optimal value of the regularization parameter alpha of the Lasso estimator from an example provided by \sphinxhref{http://scikit-learn.org/stable/auto\_examples/linear\_model/plot\_lasso\_model\_selection.html}{scikit\sphinxhyphen{}learn}. For cross\sphinxhyphen{}validation, we will use a nested cross\sphinxhyphen{}validation as implemented by the LassoCV algorithm.  Note that these examples with nested cross\sphinxhyphen{}validation take much longer to run.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ridge\PYGZus{}stats} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ridgeClassifier}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} 
                        \PYG{n}{cv\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kfolds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}folds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subject\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{subject\PYGZus{}id}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                           \PYG{o}{*}\PYG{o}{*}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alpha}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{o}{.}\PYG{l+m+mi}{01}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
overall accuracy: 1.00
overall CV accuracy: 0.72
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_31_1}.png}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_31_2}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ridge\PYGZus{}stats} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ridgeCV}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{n}{cv\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kfolds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}folds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subject\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{subject\PYGZus{}id}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
overall Root Mean Squared Error: 0.00
overall Correlation: 1.00
overall CV Root Mean Squared Error: 0.50
overall CV Correlation: 0.56
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_32_1}.png}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_32_2}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{lasso\PYGZus{}cv\PYGZus{}stats} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lassoCV}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{n}{cv\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kfolds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}folds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subject\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{subject\PYGZus{}id}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Classification and Class Imbalance}
\label{\detokenize{content/Multivariate_Prediction:classification-and-class-imbalance}}
One important thing to note is that when you use classification, it is important to account for class imbalances. i.e., that there might be unequal amounts of data in each group.  The reason why this is a problem is that chance classification is no longer at 50\% when there is a class imbalance.  Suppose you were trying to classify A from B, but 80\% of the data were instances of B. A classifier that always says B, would be correct 80\% of the time.

There are several different ways to deal with class imbalance.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{Make the Class Sizes Equal} You can randomly sample data that is overrepresented to create your own balanced dataset. Advantages are that the data classes will be balanced. The disadvantage of this approach is that you are not using all of your data.

\item {} 
\sphinxstylestrong{Average Data} You can average all of the data within a class so that each participant only has one data point per class. Advantages are the data are balanced. Disadvantages are that you have dramatically reduced the amount of data going into training the model.

\item {} 
\sphinxstylestrong{Balance Class Weights} If you are using SVM, you can set \sphinxcode{\sphinxupquote{class\_weight=balanced}}. The general idea is to increase the penalty for misclassifying minority classes to prevent them from being “overwhelmed” by the majority class. See \sphinxhref{https://chrisalbon.com/machine\_learning/support\_vector\_machines/imbalanced\_classes\_in\_svm/}{here} for a brief overview.

\end{enumerate}

When testing your model you can also make adjustments to calculate a balanced accuracy. Scikit\sphinxhyphen{}learn has the \sphinxhref{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced\_accuracy\_score.html}{\sphinxcode{\sphinxupquote{balanced\_accuracy\_score}} method}, which implements the technique outlined in \sphinxhref{https://ieeexplore.ieee.org/document/5597285}{this} paper. It essentially defines accuracy as the average recall obtained on each class.

Let’s test an example using the \sphinxcode{\sphinxupquote{\textquotesingle{}class\_weight\textquotesingle{}=\textquotesingle{}balanced\textquotesingle{}}} approach.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{svm\PYGZus{}stats} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{svm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{cv\PYGZus{}dict}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kfolds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n\PYGZus{}folds}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subject\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{subject\PYGZus{}id}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class\PYGZus{}weight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{balanced}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{linear}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
overall accuracy: 1.00
overall CV accuracy: 0.78
threshold is ignored for simple axial plots
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_35_1}.png}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_35_2}.png}


\subsection{MVPA Patterns as Biomarkers}
\label{\detokenize{content/Multivariate_Prediction:mvpa-patterns-as-biomarkers}}
Now that we know how to train multivariate patterns, what can we do with them? There has been a lot of interest in their potential to serve as neural biomarkers of psychological states.  If you would like to learn about how these can be used to better understand how we process and experience pain, watch these two videos by Tor Wager, where he summarizes some of the groundbreaking work he has been doing in this space.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{LV51\PYGZus{}3jHg\PYGZus{}c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_37_0}.jpg}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{YouTubeVideo}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3iXh0FzuAjY}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Multivariate_Prediction_38_0}.jpg}


\subsection{Additional Resources}
\label{\detokenize{content/Multivariate_Prediction:additional-resources}}
If you are feeling like you would like to learn more about some of the details and possibilities of this approach, we encourage you to read some of the many review papers from \sphinxhref{https://www.nature.com/articles/nrn1931}{Haynes \& Rees, 2006}, \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1053811910010657}{Naselaris et al., 2011}, \sphinxhref{https://www.annualreviews.org/doi/full/10.1146/annurev-neuro-062012-170325}{Haxby et al., 2014}, \sphinxhref{http://cosanlab.com/static/papers/Woo\_2017\_NN.pdf}{Woo et al, 2017}.

There are also many great books covering the machine\sphinxhyphen{}learning including the freely available \sphinxhref{https://web.stanford.edu/~hastie/Papers/ESLII.pdf}{the elements of statistical learning} and \sphinxhref{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf}{pattern recognition and machine\sphinxhyphen{}learning}.

Finally, Here is a helpful \sphinxhref{http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/}{blog post} on different algorithms and reasonable default parameters.


\subsection{Exercises}
\label{\detokenize{content/Multivariate_Prediction:exercises}}

\subsubsection{Exercise 1. Vertical vs Horizontal Checkerboard Classification.}
\label{\detokenize{content/Multivariate_Prediction:exercise-1-vertical-vs-horizontal-checkerboard-classification}}
For this exercise, find a multivariate pattern that can discriminate between horizontal and vertical checkerboards in new participants with SVM and leave\sphinxhyphen{}one\sphinxhyphen{}subject out cross\sphinxhyphen{}validation.


\subsubsection{Exercise 2. Generalizing Patterns.}
\label{\detokenize{content/Multivariate_Prediction:exercise-2-generalizing-patterns}}
Now, let’s see how well this pattern generalizes to other conditions. See which other conditions this pattern appears to generalize too by applying the pattern to all of the participants. Does it only get \sphinxstyleemphasis{confused} for conditions involving visual information?


\section{Representational Similarity Analysis}
\label{\detokenize{content/RSA:representational-similarity-analysis}}\label{\detokenize{content/RSA::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

Representational Similarity Analysis (RSA) is a multivariate technique that allows one to link disparate types of data based on shared structure in their similarity (or distance) matrices. This technique was initially proposed by \sphinxhref{https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full?utm\_source=FWEB\&utm\_medium=NBLOG\&utm\_campaign=ECO\_10YA\_top-research}{Nikolaus Kriegskorte in 2008}. Unlike multivariate prediction/classification, RSA does not attempt to directly map brain activity onto a measure, but instead compares the similarities between brain activity and the measure using second\sphinxhyphen{}order isomorphisms. This sounds complicated, but is actually quite conceptually simple.

In this tutorial we will cover:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
how to embed a stimuli in a feature space

\item {} 
how to compute similarity or distance within this feature space

\item {} 
how to map variations in position within this embedding space onto brain processes.

\end{enumerate}

For a more in depth tutorial, we recommend reading this excellent \sphinxhref{https://github.com/Summer-MIND/mind\_2018/tree/master/tutorials/representational\_similarity}{tutorial} written by \sphinxhref{http://markallenthornton.com/}{Mark Thornton} for the \sphinxhref{http://mindsummerschool.org/2018/07/30/narratives-and-naturalistic-contexts.html}{2018 MIND Summer School}, or watching his \sphinxhref{https://www.youtube.com/watch?v=ufGtuT\_J75w\&index=29\&t=0s\&list=PLEE6ggCEJ0H0KOlMKx\_PUVB\_16VoCfGj9}{video walkthrough}.

Let’s work through a quick example to outline the specific steps involved in RSA to make this more concrete.


\subsection{RSA Overview}
\label{\detokenize{content/RSA:rsa-overview}}
Imagine that you view 96 different images in the scanner and we are interested in learning more about how the brain processes information about these stimuli. Do we categorize these images into different groups? If so, how do we do this? It might depend on what features, or aspects, of the stimuli that we consider.


\subsubsection{Feature embedding space}
\label{\detokenize{content/RSA:feature-embedding-space}}
Each image can be described by a number of different variables. These variables could be more abstract, such as is it animate or inanimate? or they can be more low level, such as what color is each object?

We can group together different variables into a feature space and then describe each image using this space. Think of each feature as an axis in a multidimensional space. For example, consider the two different dimensions of \sphinxstyleemphasis{animacy} and \sphinxstyleemphasis{softness}. Where would a puppy, rock, and pillow be positioned within this two dimensional embedding space?

\sphinxincludegraphics{{Feature_Embedding}.png}

Of course, this is a just a 2\sphinxhyphen{}dimensional example, this idea can be extended to n\sphinxhyphen{}dimensions. What if were were interested in more low\sphinxhyphen{}level visual features?

\sphinxincludegraphics{{visual_features}.jpg}
from \sphinxhref{https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full?utm\_source=FWEB\&utm\_medium=NBLOG\&utm\_campaign=ECO\_10YA\_top-research}{Kriegeskorte et al., 2008}

We can compute the \sphinxstyleemphasis{representational space} of these stimuli by calculating the pairwise distance between each image in this feature space (e.g., euclidean, correlation, cosine, etc). This will yield an image x image matrix. There are many different metrics to calculate distance. For example, the absolute distance is often computed using \sphinxhref{https://en.wikipedia.org/wiki/Euclidean\_distance}{euclidean distance}, but if we don’t care about the absolute magnitude in each dimension, the relative distance can be computed using \sphinxhref{https://cmci.colorado.edu/classes/INFO-1301/files/borgatti.htm}{correlation distance}. Finally, distance is inversely proportional to similarity and these can be used relatively interchangeability (note that we will probably switch back and forth between describing similarity and distance between stimuli).

For example, if we were interested in regions that process visual information, we might extract different low\sphinxhyphen{}level visual features of an image (e.g., edges, luminance, salience, color, etc).

\sphinxincludegraphics{{visual_features_pairwise_similarity}.jpg}
from \sphinxhref{https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full?utm\_source=FWEB\&utm\_medium=NBLOG\&utm\_campaign=ECO\_10YA\_top-research}{Kriegeskorte et al., 2008}


\subsubsection{Brain embedding space}
\label{\detokenize{content/RSA:brain-embedding-space}}
We can also embed each image in brain space. Suppose we were interested in identifying the relationship between images within the visual cortex. Here the embedding space is each voxel’s activation within the visual cortex. Voxels are now the axes of the representational space.

To calculate this we need to create a brain map for every single image using a single trial model. We can use a standard first level GLM to estimate a separate beta map for each image while simultaneously removing noise from the signal by including nuisance covariates (e.g., head motion, spikes, discrete cosine transform filters, etc.)

\sphinxincludegraphics{{design_matrix}.jpg}
from \sphinxhref{https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full?utm\_source=FWEB\&utm\_medium=NBLOG\&utm\_campaign=ECO\_10YA\_top-research}{Kriegeskorte et al., 2008}

After we have a single beta map for each image, we can extract patterns of activity for a single ROI to yield an image by ROI voxel matrix. This allows us to calculate the representational space of how this region responds to each image by computing the pairwise similarity of the pattern of activation viewing one picture to all other pictures. In other words, each voxel within a region becomes an axis in a high dimensional space, and how the region responds to a single picture will be a point in that space. Images that have a similar response in this region will be closer in this high dimensional voxel space.

\sphinxincludegraphics{{brain_extraction}.jpg}
from \sphinxhref{https://www.annualreviews.org/doi/full/10.1146/annurev-neuro-062012-170325}{Haxby et al., 2014}

For fMRI, we are typically not concerned with the magnitude of activation, so we often use a relative pairwise distance metric such as correlation or cosine distance.

\sphinxincludegraphics{{brain_similarity}.jpg}
from \sphinxhref{https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full?utm\_source=FWEB\&utm\_medium=NBLOG\&utm\_campaign=ECO\_10YA\_top-research}{Kriegeskorte et al., 2008}


\subsubsection{Mapping stimuli relationships onto brain}
\label{\detokenize{content/RSA:mapping-stimuli-relationships-onto-brain}}
Now we can test different hypotheses about how the brain might be processing information about each image by mapping the representational structure of the brain space to different features spaces.

To make an inference about what each region of the brain is computing, we can evaluate if there are any regions in the brain that exhibit a similar structure as the feature representational space. Remember, these matrices are typically symmetrical (unless there is a directed relationship), so you can extract either the upper or lower triangle of these matrices. Then these triangles are flattened or vectorized, which makes it easy to evaluate the similarity between the triangle of the brain and feature matrices. Because these relationships might not necessarily be linear, it is common to use a spearman rank correlation to examine monotonic relationships between different similarity matrices.

\sphinxincludegraphics{{similarity}.jpg}
from \sphinxhref{https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full?utm\_source=FWEB\&utm\_medium=NBLOG\&utm\_campaign=ECO\_10YA\_top-research}{Kriegeskorte et al., 2008}

We can make inferences across subjects, by transforming each correlation value for a given region to a continuous metric using a \sphinxhref{https://en.wikipedia.org/wiki/Fisher\_transformation}{fisher r\sphinxhyphen{}to\sphinxhyphen{}z transformation} and then computing a one\sphinxhyphen{}sample t\sphinxhyphen{}test over participants to test if the observed distribution is significantly different from zero. This is typically computed using resampling methods such as a permutation test. If inferences are over participants, then a sign test similar to a one\sphinxhyphen{}sample t\sphinxhyphen{}test is appropriate. This is a fairly standard practice when all participants viewed the same stimuli and you are interested in making inferences over participants.

There are extensions to apply this method to other types of data. For example, in Intersubject\sphinxhyphen{}RSA (IS\sphinxhyphen{}RSA), one might be interested in whether participants exhibit a particular representational structure over some feature space that maps on to inter\sphinxhyphen{}subject differences in brain patterns \sphinxhref{https://www.nature.com/articles/s41467-019-09161-6}{van Baar et al., 2019}. In this particular, extension we cannot use the same type of permutation test to make our inferences (unless we have many different groups of participants). Instead, we might randomly shuffle one of the matrices and repeatedly re\sphinxhyphen{}calculate the similarity to produce an empirical distribution of similarity values. It is important to note that this type of permutation violates the exchangeability hypothesis and might yield overly optimistic p\sphinxhyphen{}values \sphinxhref{https://www.sciencedirect.com/science/article/pii/S1053811916304141}{see Chen et al., 2017}. Instead, a more conservative hypothesis testing approach is to use the \sphinxhref{https://en.wikipedia.org/wiki/Mantel\_test}{mantel test}, in which we only permute the rows and columns with respect to one another. Personally, I think this approach is too conservative for small sample sizes and it is still an open statistical question about how to best make inferences in this particular type of use.

\sphinxincludegraphics{{rsa}.jpg}
from \sphinxhref{https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full?utm\_source=FWEB\&utm\_medium=NBLOG\&utm\_campaign=ECO\_10YA\_top-research}{Kriegeskorte et al., 2008}

One of the reasons why this technique is so powerful is that it allows the possibility of testing different computational hypotheses. Different feature representations might be associated with specific regions involved in various computations. Alternatively, different types of data can be mapped onto each other using this technique. For example, \sphinxhref{https://www.sciencedirect.com/science/article/pii/S0896627308009434}{Kriegeskorte et al., 2008} have demonstrated that it is possible to map the function of IT cortex across humans and monkeys using this technique.

Now that we understand the basic steps of RSA, let’s apply it to some test data. First, let’s load the modules we will use for this tutorial.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{,} \PYG{n}{Adjacency}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{mask} \PYG{k+kn}{import} \PYG{n}{expand\PYGZus{}mask}\PYG{p}{,} \PYG{n}{roi\PYGZus{}to\PYGZus{}brain}
\PYG{k+kn}{from} \PYG{n+nn}{nltools}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{fdr}\PYG{p}{,} \PYG{n}{threshold}\PYG{p}{,} \PYG{n}{fisher\PYGZus{}r\PYGZus{}to\PYGZus{}z}\PYG{p}{,} \PYG{n}{one\PYGZus{}sample\PYGZus{}permutation}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{pairwise\PYGZus{}distances}
\PYG{k+kn}{from} \PYG{n+nn}{nilearn}\PYG{n+nn}{.}\PYG{n+nn}{plotting} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}glass\PYGZus{}brain}\PYG{p}{,} \PYG{n}{plot\PYGZus{}stat\PYGZus{}map}\PYG{p}{,} \PYG{n}{view\PYGZus{}img\PYGZus{}on\PYGZus{}surf}\PYG{p}{,} \PYG{n}{view\PYGZus{}img}
\PYG{k+kn}{from} \PYG{n+nn}{bids} \PYG{k+kn}{import} \PYG{n}{BIDSLayout}\PYG{p}{,} \PYG{n}{BIDSValidator}

\PYG{n}{data\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../data/localizer}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{layout} \PYG{o}{=} \PYG{n}{BIDSLayout}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{n}{derivatives}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Single Subject Pattern Similarity}
\label{\detokenize{content/RSA:single-subject-pattern-similarity}}
Recall that in the Single Subject Model Lab that we ran single subject models for 10 different regressors for the Pinel Localizer task.  In this tutorial, we will use our results to learn how to conduct RSA style analyses.

First, let’s get a list of all of the subject IDs and load the beta values from each condition for a single subject into a \sphinxcode{\sphinxupquote{Brain\_Data}} object.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sub} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S01}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sub}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*denoised*.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{file\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{n}{x} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{file\PYGZus{}list} \PYG{k}{if} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{betas}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n}{x}\PYG{p}{]}
\PYG{n}{file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{conditions} \PYG{o}{=} \PYG{p}{[}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{basename}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sub}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}denoised}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{file\PYGZus{}list}\PYG{p}{]}
\PYG{n}{beta} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{file\PYGZus{}list}\PYG{p}{)}
\end{sphinxVerbatim}

Next we will compute the pattern similarity across each beta image. We could do this for the whole brain, but it probably makes more sense to look within a single region. Many papers use a searchlight approach in which they examine the pattern similarity within a small sphere centered on each voxel. The advantage of this approach is that it uses the same number of voxels across searchlights and allows one to investigate the spatial topography at a relatively fine\sphinxhyphen{}scale. However, this procedure is fairly computationally expensive as it needs to be computed over each voxel and just like univariate analyses, will require stringent correction for multiple tests as we learned about in tutorial 12: Thresholding Group Analyses. Personally, I prefer to use whole\sphinxhyphen{}brain parcellations as they provide a nice balance between spatial specificity and computational efficiency. In this tutorial, we will continue to use functional regions of interest from our 50 ROI Neurosynth parcellation. This allows us to cover the entire brain with a relatively course spatial granularity, but requires several orders of magnitude of less computations than using a voxelwise searchlight approach. This means it will run much faster and will require us to use a considerably less stringent statistical threshold to correct for all independent tests. For example, for 50 tests bonferroni correction is p \textless{} 0.001 (i.e., .05/50). If we ever wanted better spatial granularity we could use increasingly larger parcellations (e.g., \sphinxhref{https://neurovault.org/collections/2099/}{100 or 200}).

Let’s load our parcellation mask so that we can examine the pattern similarity across these conditions for each ROI.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mask} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{..}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{masks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k50\PYGZus{}2mm.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{mask\PYGZus{}x} \PYG{o}{=} \PYG{n}{expand\PYGZus{}mask}\PYG{p}{(}\PYG{n}{mask}\PYG{p}{)}

\PYG{n}{mask}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{RSA_6_0}.png}

Ok, now we will want to calculate the pattern similar within each ROI across the 10 conditions.

We will loop over each ROI and extract the pattern data across all conditions and then compute the correlation distance between each condition. This data will now be an \sphinxcode{\sphinxupquote{Adjacency}} object that we discussed in the Lab 13: Connectivity. We will temporarily store this in a list.

Notice that for each iteration of the loop we apply the ROI mask to our beta images and then calculate the correlation distance.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{out} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{m} \PYG{o+ow}{in} \PYG{n}{mask\PYGZus{}x}\PYG{p}{:}
    \PYG{n}{out}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{apply\PYGZus{}mask}\PYG{p}{(}\PYG{n}{m}\PYG{p}{)}\PYG{o}{.}\PYG{n}{distance}\PYG{p}{(}\PYG{n}{metric}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{correlation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

Let’s plot an example ROI and it’s associated distance matrix.

Here is a left motor parcel. Notice how the distance is small between the motor left auditory and visual and motor right auditory and visual beta images?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{roi} \PYG{o}{=} \PYG{l+m+mi}{26}
\PYG{n}{plot\PYGZus{}glass\PYGZus{}brain}\PYG{p}{(}\PYG{n}{mask\PYGZus{}x}\PYG{p}{[}\PYG{n}{roi}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}nifti}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{out}\PYG{p}{[}\PYG{n}{roi}\PYG{p}{]}\PYG{o}{.}\PYG{n}{labels} \PYG{o}{=} \PYG{n}{conditions}
\PYG{n}{f2} \PYG{o}{=} \PYG{n}{out}\PYG{p}{[}\PYG{n}{roi}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RdBu\PYGZus{}r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{RSA_10_0}.png}

\noindent\sphinxincludegraphics{{RSA_10_1}.png}

We can also visualize this distance matrix using multidimensional scaling with the \sphinxcode{\sphinxupquote{plot\_mds()}} method. This method projects the images into either a 2D or 3D plane for ease of visualization. There are many other distance based projection methods such as T\sphinxhyphen{}SNE or UMAP, we encourage readers to check out the excellent \sphinxhref{https://hypertools.readthedocs.io/en/latest/}{hypertools} package that has a great implementation of all of these methods.

Notice how the motor right visual and auditory are near each other, while the motor left visual and auditory are grouped together further away?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{f} \PYG{o}{=} \PYG{n}{out}\PYG{p}{[}\PYG{n}{roi}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot\PYGZus{}mds}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{view}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{RSA_12_0}.png}

It’s completely fine to continue to work with distance values, but just to make this slightly more intuitive to understand what is going on we will convert this to similarity.  For correlation distance, this entails subtracting each value from 1. This will yield similarity scores in the form of pearson correlations. If you are using unbounded metrics (e.g., euclidean distance), then use the \sphinxcode{\sphinxupquote{distance\_to\_similarity()}} Adjacency method.

We are also adding conditions as labels to the object, which make the plots easier to read.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{out\PYGZus{}sim} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{m} \PYG{o+ow}{in} \PYG{n}{out}\PYG{p}{:}
    \PYG{n}{mask\PYGZus{}tmp} \PYG{o}{=} \PYG{n}{m}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{mask\PYGZus{}tmp} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{mask\PYGZus{}tmp}
    \PYG{n}{mask\PYGZus{}tmp}\PYG{o}{.}\PYG{n}{labels} \PYG{o}{=} \PYG{n}{conditions}
    \PYG{n}{out\PYGZus{}sim}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{mask\PYGZus{}tmp}\PYG{p}{)}
\end{sphinxVerbatim}

Let’s look at the heatmap of the similarity matrix to see how more red colors indicate greater similarity between patterns within the left motor cortex across conditions.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{roi} \PYG{o}{=} \PYG{l+m+mi}{26}
\PYG{n}{plot\PYGZus{}glass\PYGZus{}brain}\PYG{p}{(}\PYG{n}{mask\PYGZus{}x}\PYG{p}{[}\PYG{n}{roi}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}nifti}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{f} \PYG{o}{=} \PYG{n}{out\PYGZus{}sim}\PYG{p}{[}\PYG{n}{roi}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RdBu\PYGZus{}r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{RSA_16_0}.png}

\noindent\sphinxincludegraphics{{RSA_16_1}.png}


\subsubsection{Testing a Representation Hypothesis}
\label{\detokenize{content/RSA:testing-a-representation-hypothesis}}
Ok, now that we have a sense of what the similarity of patterns look like in left motor cortex, let’s create an adjacency matrix indicating a specific relationship between left hand finger tapping across the auditory and visual conditions. This type of adjacency matrix is one way in which we can test a specific hypotheses about the representational structure of the data across all images.

As you can see this only includes edges for the motor\sphinxhyphen{}left auditory and motor\sphinxhyphen{}left visual conditions.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{motor} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{conditions}\PYG{p}{)}\PYG{p}{,}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{conditions}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{motor}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{diag\PYGZus{}indices}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{conditions}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1} 
\PYG{n}{motor}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{motor}\PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{motor}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{motor}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{motor} \PYG{o}{=} \PYG{n}{Adjacency}\PYG{p}{(}\PYG{n}{motor}\PYG{p}{,} \PYG{n}{matrix\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{distance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{conditions}\PYG{p}{)}
\PYG{n}{motor}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{RSA_18_0}.png}

Now let’s search over all ROIs to see if any match this representational structure of left motor\sphinxhyphen{}cortex using the \sphinxcode{\sphinxupquote{similarity()}} method from the \sphinxcode{\sphinxupquote{Adjacency}} class. This function uses spearman rank correlations by default. This is probably a good idea as we are most interested in monotonic relationships between the two similarity matrices.

The \sphinxcode{\sphinxupquote{similarity()}} method also computes permutations within columns and rows by default. To speed up the analysis, we will set the number of permutations to zero (i.e., \sphinxcode{\sphinxupquote{n\_permute=0}}).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{motor\PYGZus{}sim\PYGZus{}r} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{m} \PYG{o+ow}{in} \PYG{n}{out\PYGZus{}sim}\PYG{p}{:}
    \PYG{n}{s} \PYG{o}{=} \PYG{n}{m}\PYG{o}{.}\PYG{n}{similarity}\PYG{p}{(}\PYG{n}{motor}\PYG{p}{,} \PYG{n}{metric}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spearman}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}permute}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{motor\PYGZus{}sim\PYGZus{}r}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{correlation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

This will return a vector of similarity scores for each ROI, we can plot the distribution of these 50 \(\rho\) values.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{motor\PYGZus{}sim\PYGZus{}r}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Similarity (rho)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Frequency\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{RSA_22_1}.png}

We can also plot these RSA values back onto the brain to see the spatial distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rsa\PYGZus{}motor} \PYG{o}{=} \PYG{n}{roi\PYGZus{}to\PYGZus{}brain}\PYG{p}{(}\PYG{n}{motor\PYGZus{}sim\PYGZus{}r}\PYG{p}{,} \PYG{n}{mask\PYGZus{}x}\PYG{p}{)}

\PYG{n}{plot\PYGZus{}stat\PYGZus{}map}\PYG{p}{(}\PYG{n}{rsa\PYGZus{}motor}\PYG{o}{.}\PYG{n}{to\PYGZus{}nifti}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{draw\PYGZus{}cross}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{display\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{black\PYGZus{}bg}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{cut\PYGZus{}coords}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{70}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.displays.ZSlicer at 0x7fc04c15bd50\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{RSA_24_1}.png}

Notice how left motor cortex is among the ROIs with the highest similarity value?  Unfortunately, we can only plot the similarity values and can’t threshold them yet because we didn’t calculate any p\sphinxhyphen{}values.

We could calculate p\sphinxhyphen{}values using a permutation test, but this would require us to repeatedly recalculate the similarity between the two matrices and would take a long time (i.e., 5,000 correlations X 50 ROIS). Plus, the inference we want to make isn’t really at the single\sphinxhyphen{}subject level, but across participants.

Let’s now run this same analysis across all participants and run a one\sphinxhyphen{}sample t\sphinxhyphen{}test across each ROI.


\subsubsection{RSA Group Inference}
\label{\detokenize{content/RSA:rsa-group-inference}}
Here we calculate the RSA for each ROI for every participant.  This will take a little bit of time to run (30 participants X 50 ROIs).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sub\PYGZus{}list} \PYG{o}{=} \PYG{n}{layout}\PYG{o}{.}\PYG{n}{get\PYGZus{}subjects}\PYG{p}{(}\PYG{n}{scope}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{all\PYGZus{}sub\PYGZus{}similarity} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{;} \PYG{n}{all\PYGZus{}sub\PYGZus{}motor\PYGZus{}rsa} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{k}{for} \PYG{n}{sub} \PYG{o+ow}{in} \PYG{n}{sub\PYGZus{}list}\PYG{p}{:}
    \PYG{n}{file\PYGZus{}list} \PYG{o}{=} \PYG{n}{glob}\PYG{o}{.}\PYG{n}{glob}\PYG{p}{(}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{data\PYGZus{}dir}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{derivatives}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fmriprep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sub}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{func}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{*denoised*.nii.gz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{file\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{n}{x} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{file\PYGZus{}list} \PYG{k}{if} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{betas}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n}{x}\PYG{p}{]}
    \PYG{n}{file\PYGZus{}list}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{conditions} \PYG{o}{=} \PYG{p}{[}\PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{basename}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sub\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sub}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}denoised}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{file\PYGZus{}list}\PYG{p}{]}
    \PYG{n}{beta} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{n}{file\PYGZus{}list}\PYG{p}{)}
    
    \PYG{n}{sub\PYGZus{}pattern} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{;} \PYG{n}{motor\PYGZus{}sim\PYGZus{}r} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{;}
    \PYG{k}{for} \PYG{n}{m} \PYG{o+ow}{in} \PYG{n}{mask\PYGZus{}x}\PYG{p}{:}
        \PYG{n}{sub\PYGZus{}pattern\PYGZus{}similarity} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{beta}\PYG{o}{.}\PYG{n}{apply\PYGZus{}mask}\PYG{p}{(}\PYG{n}{m}\PYG{p}{)}\PYG{o}{.}\PYG{n}{distance}\PYG{p}{(}\PYG{n}{metric}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{correlation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{sub\PYGZus{}pattern\PYGZus{}similarity}\PYG{o}{.}\PYG{n}{labels} \PYG{o}{=} \PYG{n}{conditions}
        \PYG{n}{s} \PYG{o}{=} \PYG{n}{sub\PYGZus{}pattern\PYGZus{}similarity}\PYG{o}{.}\PYG{n}{similarity}\PYG{p}{(}\PYG{n}{motor}\PYG{p}{,} \PYG{n}{metric}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spearman}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}permute}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{sub\PYGZus{}pattern}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{sub\PYGZus{}pattern\PYGZus{}similarity}\PYG{p}{)}
        \PYG{n}{motor\PYGZus{}sim\PYGZus{}r}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{correlation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{n}{all\PYGZus{}sub\PYGZus{}similarity}\PYG{p}{[}\PYG{n}{sub}\PYG{p}{]} \PYG{o}{=} \PYG{n}{sub\PYGZus{}pattern}
    \PYG{n}{all\PYGZus{}sub\PYGZus{}motor\PYGZus{}rsa}\PYG{p}{[}\PYG{n}{sub}\PYG{p}{]} \PYG{o}{=} \PYG{n}{motor\PYGZus{}sim\PYGZus{}r}
\PYG{n}{all\PYGZus{}sub\PYGZus{}motor\PYGZus{}rsa} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{all\PYGZus{}sub\PYGZus{}motor\PYGZus{}rsa}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}
\end{sphinxVerbatim}

Now let’s calculate a one sample t\sphinxhyphen{}test on each ROI, to see which ROI is consistently different from zero across our sample of participants. Because these are r\sphinxhyphen{}values, we will first perform a \sphinxhref{https://en.wikipedia.org/wiki/Fisher\_transformation}{fisher r to z transformation}. We will use a \sphinxhref{https://en.wikipedia.org/wiki/Sign\_test}{non\sphinxhyphen{}parametric permutation sign test} to perform our null hypothesis test. This will take a minute to run as we will be calculating 5000 permutations for each of 50 ROIs (though these permutations are parallelized across cores).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rsa\PYGZus{}stats} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{all\PYGZus{}sub\PYGZus{}motor\PYGZus{}rsa}\PYG{p}{:}
    \PYG{n}{rsa\PYGZus{}stats}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{one\PYGZus{}sample\PYGZus{}permutation}\PYG{p}{(}\PYG{n}{fisher\PYGZus{}r\PYGZus{}to\PYGZus{}z}\PYG{p}{(}\PYG{n}{all\PYGZus{}sub\PYGZus{}motor\PYGZus{}rsa}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

We can plot a thresholded map using fdr correction as the threshold

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fdr\PYGZus{}p} \PYG{o}{=} \PYG{n}{fdr}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{rsa\PYGZus{}stats}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{q}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{fdr\PYGZus{}p}\PYG{p}{)}

\PYG{n}{rsa\PYGZus{}motor\PYGZus{}r} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{o}{*}\PYG{n}{y}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x}\PYG{p}{,}\PYG{n}{y} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{mask\PYGZus{}x}\PYG{p}{,} \PYG{n}{rsa\PYGZus{}stats}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{rsa\PYGZus{}motor\PYGZus{}p} \PYG{o}{=} \PYG{n}{Brain\PYGZus{}Data}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{o}{*}\PYG{n}{y}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x}\PYG{p}{,}\PYG{n}{y} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{mask\PYGZus{}x}\PYG{p}{,} \PYG{n}{rsa\PYGZus{}stats}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{thresholded} \PYG{o}{=} \PYG{n}{threshold}\PYG{p}{(}\PYG{n}{rsa\PYGZus{}motor\PYGZus{}r}\PYG{p}{,} \PYG{n}{rsa\PYGZus{}motor\PYGZus{}p}\PYG{p}{,} \PYG{n}{thr}\PYG{o}{=}\PYG{n}{fdr\PYGZus{}p}\PYG{p}{)}

\PYG{n}{plot\PYGZus{}glass\PYGZus{}brain}\PYG{p}{(}\PYG{n}{thresholded}\PYG{o}{.}\PYG{n}{to\PYGZus{}nifti}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{coolwarm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZhy{}1
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.displays.OrthoProjector at 0x7fbf3929acd0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{RSA_31_2}.png}

Looks like nothing survives FDR. Let’s try a more liberal uncorrected threshold.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{thresholded} \PYG{o}{=} \PYG{n}{threshold}\PYG{p}{(}\PYG{n}{rsa\PYGZus{}motor\PYGZus{}r}\PYG{p}{,} \PYG{n}{rsa\PYGZus{}motor\PYGZus{}p}\PYG{p}{,} \PYG{n}{thr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}

\PYG{n}{plot\PYGZus{}glass\PYGZus{}brain}\PYG{p}{(}\PYG{n}{thresholded}\PYG{o}{.}\PYG{n}{to\PYGZus{}nifti}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{coolwarm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.displays.OrthoProjector at 0x7fbf2cd13c90\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{RSA_33_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{view\PYGZus{}img}\PYG{p}{(}\PYG{n}{thresholded}\PYG{o}{.}\PYG{n}{to\PYGZus{}nifti}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}nilearn.plotting.html\PYGZus{}stat\PYGZus{}map.StatMapView at 0x7fc01b3063d0\PYGZgt{}
\end{sphinxVerbatim}

This looks a little better and makes it clear that the ROI that has the highest similarity with our model specifying the representational structure of motor cortex is precisely the left motor cortex. Though this only shows up using a liberal uncorrected threshold, remember that we are only using about 9 subjects for this demo.

Though this was a very simple model using a very simple dataset, hopefully this example has illustrated how straightforward it is to run RSA style analyses using any type of data. Most people use this method to examine representations of much more complicated feature spaces.

It is also possible to combine different hypotheses or models to decompose a brain representational similarity matrix. This method typically uses regression rather than correlations (see \sphinxhref{https://www.nature.com/articles/s41562-017-0072}{Parkinson et al., 2017})

We have recently used this technique in our own work to map similarity in brain space to individual variability in a computational model of decision\sphinxhyphen{}making. If you are interested in seeing an example of intersubject RSA (IS\sphinxhyphen{}RSA), check out the \sphinxhref{https://www.nature.com/articles/s41467-019-09161-6}{paper}, and accompanying python \sphinxhref{https://github.com/jeroenvanbaar/MoralStrategiesFMRI}{code}.


\section{Resampling Statistics}
\label{\detokenize{content/Resampling_Statistics:resampling-statistics}}\label{\detokenize{content/Resampling_Statistics::doc}}
\sphinxstyleemphasis{Written by Luke Chang}

Most the statistics you have learned in introductory statistics are based on parametric statistics and assume a normal distribution. However, in applied data analysis these assumptions are rarely met as we typically have small sample sizes from non\sphinxhyphen{}normal distributions. Though these concepts will seem a little foreign at first, I personally find them to be more intuitive than the classical statistical approaches, which are based on theoretical distributions. Our lab relies heavily on resampling statistics and they are amenable to most types of modeling applications such as fitting abstract computational models, multivariate predictive models, and hypothesis testing.

There are 4 main types of resampling statistics:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxstylestrong{bootstrap} allows us to calculate the precision of an estimator by resampling with replacement

\item {} 
\sphinxstylestrong{permutation test} allows us to perform null\sphinxhyphen{}hypothesis testing by empirically computing the proportion of times a test statistic exceeds a permuted null distribution.

\item {} 
\sphinxstylestrong{jackknife} allows us to estimate the bias and standard error of an estimator by creating samples that drop one or more samples.

\item {} 
\sphinxstylestrong{cross\sphinxhyphen{}validation} provides a method to provide an unbiased estimate of the out\sphinxhyphen{}of\sphinxhyphen{}sample predictive accuracy of a model by dividing the data into separate training and test samples, where each data serves as both training and test for different models.

\end{enumerate}

In this tutorial, we will focus on the bootstrap and permutation test. Jackknifing and bootstrapping are both used to calculate the variability of an estimator and often provide numerically similar results. We tend to prefer the bootstrap procedure over the Jackknife, but there are specific use cases where you will want to use the jackknife. We will not be covering the jackknife in this tutorial, but encourage the interested reader to review the \sphinxhref{https://en.wikipedia.org/wiki/Resampling\_(statistics)\#Jackknife}{wikipedia page} for more information. We will also not be covering cross\sphinxhyphen{}validation as this is discussed in the multivariate prediction tutorial.


\subsection{Bootstrap}
\label{\detokenize{content/Resampling_Statistics:bootstrap}}
In statistics, we are typically trying to make inferences about the parameters of a population based on a limited number of randomly drawn samples. How reliable are the parameters estimated from this sample? Would we observe the same parameter if we ran the model on a different independent sample? \sphinxhref{https://projecteuclid.org/download/pdf\_1/euclid.aos/1176344552}{\sphinxstyleemphasis{Bootstrapping}} offers a way to empirically estimate the precision of the estimated parameter by resampling with replacement from our sample distribution and estimating the parameters with each new subsample. This allows us to capitalize on naturally varying error within our sample to create a distribution of the range of parameters we might expect to observe from other independent samples. This procedure is reasonably robust to the presence of outliers as they should rarely be randomly selected across the different subsamples. Together, the subsamples create a distribution of the parameters we might expect to encounter from independent random draws from the population and allow us to assess the precision of a sample statistic. This technique assumes that the original samples are independent and are random samples representative of the population.

Let’s demonstrate how this works using a simulation.

First, let’s create population data by sampling from a normal distribution. For this simulation, we will assume that there are 10,000 participants in the population that are normally distributed \(\mathcal{N}(\mu=50, \sigma=10)\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{n}{mean} \PYG{o}{=} \PYG{l+m+mi}{50}
\PYG{n}{std} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{population\PYGZus{}n} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{n}{population} \PYG{o}{=} \PYG{n}{mean} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{population\PYGZus{}n}\PYG{p}{)}\PYG{o}{*}\PYG{n}{std}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{population}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{population}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population Std: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{population}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Population Mean: 50.0
Population Std: 10.1
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Resampling_Statistics_2_1}.png}

Now, let’s run a single experiment where we randomly sample 20 participants from the population. You can see that the mean and standard deviation of this distribution are fairly close to the population even though we are not full sampling the distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}n} \PYG{o}{=} \PYG{l+m+mi}{20}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{population}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{sample\PYGZus{}n}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Single Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Distribution: n=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sample\PYGZus{}n}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
      
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Std: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Sample Mean: 51.4
Sample Std: 9.18
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Resampling_Statistics_4_1}.png}

Now let’s estimate the mean of this sample via bootstrapping 5,000 times to estimate our certainty in this estimate from our single small sample.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}bootstrap} \PYG{o}{=} \PYG{l+m+mi}{5000}

\PYG{n}{bootstrap\PYGZus{}means} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{b} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}bootstrap}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{bootstrap\PYGZus{}means}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{sample\PYGZus{}n}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{bootstrap\PYGZus{}means} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bootstrap}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Bootstrapped Means}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bootstrapped Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bootstrapped Std: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Bootstrapped Mean: 51.4
Bootstrapped Std: 2.09
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Resampling_Statistics_6_1}.png}

From this simulation, we can see that the mean of the bootstraps is the same as the original mean of the sample.

How confident are we in the precision of our estimated mean? In other words, if we were to look through all 5,000 of our subsamples, how many of them would be close to 50.1? We can define a confidence interval to describe our uncertainty in our estimate. For example, we can use the percentile method to demonstrate the range of the estimated parameter in 95\% of our samples. To do this we compute the upper and lower quantiles of our bootstrap estimates centered at 50\% (i.e., 2.5\% \& 97.5\%).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}bootstrap} \PYG{o}{=} \PYG{l+m+mi}{5000}

\PYG{n}{bootstrap\PYGZus{}means} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{b} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}bootstrap}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{bootstrap\PYGZus{}means}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{sample\PYGZus{}n}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{bootstrap\PYGZus{}means} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bootstrap}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Bootstrapped Means}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n}{lower\PYGZus{}bound} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{,} \PYG{l+m+mf}{2.5}\PYG{p}{)}
\PYG{n}{upper\PYGZus{}bound} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{,} \PYG{l+m+mf}{97.5}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{lower\PYGZus{}bound}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{upper\PYGZus{}bound}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bootstrapped Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{95\PYGZpc{} Confidence Intervals: [}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lower\PYGZus{}bound}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{upper\PYGZus{}bound}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Bootstrapped Mean: 51.4
95\PYGZpc{} Confidence Intervals: [47.4, 55.6]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Resampling_Statistics_8_1}.png}

The percentile method reveals that 95\% of our bootstrap samples lie between the interval {[}47.4, 55.6{]}. While the percentile method is easy to compute and intuitive to understand, it has some issues. First, if the original sample was small and not representative of the population, the confidence interval may be biased and too narrow. Second, if the bootstrapped distribution is not symmetric and is skewed, the percentile based confidence intervals will not accurately reflect the distribution. Efron (1987) proposed the bias\sphinxhyphen{}corrected and accelerated bootstrap, which attempts to address these issues. We will not be explaining this in detail at the moment and encourage the interested reader to review the original \sphinxhref{https://www.jstor.org/stable/pdf/2289144.pdf}{paper}.

Now let’s see how the bootstrap compares to if we had run real independent experiments. Let’s simulate 1000 experiments where we randomly sample independent participants from the population and examine the distribution of the means from these independent experiments.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{1000}

\PYG{n}{sample\PYGZus{}means} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{b} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{sample\PYGZus{}means}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{population}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{sample\PYGZus{}n}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sample\PYGZus{}means} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Random Samples}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Random Sample Means}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n}{lower\PYGZus{}bound} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{,} \PYG{l+m+mf}{2.5}\PYG{p}{)}
\PYG{n}{upper\PYGZus{}bound} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{,} \PYG{l+m+mf}{97.5}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{lower\PYGZus{}bound}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{upper\PYGZus{}bound}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bootstrapped Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{95\PYGZpc{} Confidence Intervals: [}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lower\PYGZus{}bound}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{upper\PYGZus{}bound}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Bootstrapped Mean: 50.0
95\PYGZpc{} Confidence Intervals: [45.7, 54.3]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Resampling_Statistics_10_1}.png}

We see that the mean is closer to the population mean, but our certainty is approximately equal to what we estimated from bootstrapping a single sample.

Finally, let’s compare the bootstrapped distribution of 20 samples to the 1000 random samples.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}means}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bootstrap}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Random Samples}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bootstrapped vs Randomly Sampled Precision Estimates}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bootstrap}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Random Samples}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x7fc2f92cdf10\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Resampling_Statistics_12_1}.png}

This technique is certainly not perfect, but it is very impressive how well we can estimate the precision of a population level statistic from a single small experiment using bootstrapping. Though our example focuses on estimating the mean of a population, this approach should work for many different types of estimators. Hopefully, you can see how this technique might be applied to your own work.


\subsection{Permutation Test}
\label{\detokenize{content/Resampling_Statistics:permutation-test}}
After we have estimated a parameter for a sample, we often want to perform a hypothesis test to assess if the observed distribution is statistically different from a null distribution at a specific alpha criterion (e.g., p \textless{} 0.05). This is called null hypothesis testing, and classical statistical tests, such as the t\sphinxhyphen{}test, F\sphinxhyphen{}test, and \(\chi^2\) tests, rely on theoretical probability distributions. This can be problematic when your data are not well approximated by the theoretical distributions. Using resampling statistics, we can empirically evaluate the null hypothesis by randomly shuffling the labels and re\sphinxhyphen{}running the statistic. Assuming that the labels are exchangeable under the null hypothesis, then the resulting tests yield the exact significance levels, i.e., the number of times we observed our result by chance. This class of non\sphinxhyphen{}parametric tests are called permutation tests, and are also occasionally referred to as randomization, re\sphinxhyphen{}rerandomization, or exact tests. Assuming the data is exchangeable, permutation tests can provide a “p\sphinxhyphen{}value” for pretty much any test statistic regardless if the distribution is known. This provides a relatively straightforward statistic that is easy to compute and understand. Permutation tests can be computationally expensive and often require writing custom code. However, because they are independent they can be run in parallel using multiple CPUs or on a high performance computing system.


\subsubsection{One Sample Permutation Test}
\label{\detokenize{content/Resampling_Statistics:one-sample-permutation-test}}
Let’s simulate some data to demonstrate how to run a permutation test to evaluate if the mean of the simulated sample is statistically different from zero.

We will sample 20 data points from a normal distribution, \(\mathcal{N}(\mu=1, \sigma=1)\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{n}{mean} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{std} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{sample\PYGZus{}n} \PYG{o}{=} \PYG{l+m+mi}{20}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{mean} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{sample\PYGZus{}n}\PYG{p}{)}\PYG{o}{*}\PYG{n}{std}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Std: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Sample Mean: 0.953
Sample Std: 1.04
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Resampling_Statistics_16_1}.png}

In this example, the null hypothesis is that the sample does not significantly differ from zero. We can empirically evaluate this by randomly multiplying each sample by a \(1\) or \(-1\) and then calculating the mean for each permutation. This will yield an empirical distribution of null means and we can evaluate the number of times the mean of our sample exceeds the mean of the null distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}permutations} \PYG{o}{=} \PYG{l+m+mi}{5000}

\PYG{n}{permute\PYGZus{}means} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}permutations}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{permute\PYGZus{}means}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sample\PYGZus{}n}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{permute\PYGZus{}means} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{permute\PYGZus{}means}\PYG{p}{)}

\PYG{n}{p\PYGZus{}value} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{permute\PYGZus{}means} \PYG{o}{\PYGZlt{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{permute\PYGZus{}means}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{permute\PYGZus{}means}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Null Distribution Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{permute\PYGZus{}means}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n permutations \PYGZlt{} Sample Mean = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{permute\PYGZus{}means} \PYG{o}{\PYGZlt{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p\PYGZhy{}value = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{p\PYGZus{}value}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Sample Mean: 0.953
Null Distribution Mean: 0.00544
n permutations \PYGZlt{} Sample Mean = 4998
p\PYGZhy{}value = 0.0004
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Resampling_Statistics_18_1}.png}

As you can see from a null distribution that it is very rare for us to randomly observe a mean of .953. In fact, this only occurred twice in 5,000 random permutations of the data, which makes our p\sphinxhyphen{}value = 0.0004. The precision of our p\sphinxhyphen{}value is tied to the number of permutations we run. More samples will allow us to observe a higher precision of our p\sphinxhyphen{}value, but will also increase our computation time. We tend to use 5,000 or 10,000 permutations as defaults.


\subsubsection{Two Sample Permutation Test}
\label{\detokenize{content/Resampling_Statistics:two-sample-permutation-test}}
When we were computed a one\sphinxhyphen{}sample permutation test above, we randomly multiplied each data point by a \(1\) or \(-1\) to create a null distribution. If we are interested in comparing two different groups using a permutation test, we can randomly swap group labels and can recompute the difference between the two distribution. This assumes the data are exchangeable.

Let’s start by simulating two different groups. Sample 1 is randomly drawn from this normal distribution, \(\mathcal{N}(\mu=10, \sigma=5)\), while Sample 2 is drawn from this normal distribution \(\mathcal{N}(\mu=7, \sigma=5)\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}n} \PYG{o}{=} \PYG{l+m+mi}{50}

\PYG{n}{mean\PYGZus{}1} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{std\PYGZus{}1} \PYG{o}{=} \PYG{l+m+mi}{5}

\PYG{n}{mean\PYGZus{}2} \PYG{o}{=} \PYG{l+m+mi}{7}
\PYG{n}{std\PYGZus{}2} \PYG{o}{=} \PYG{l+m+mi}{5}

\PYG{n}{sample\PYGZus{}1} \PYG{o}{=} \PYG{n}{mean\PYGZus{}1} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{sample\PYGZus{}n}\PYG{p}{)}\PYG{o}{*}\PYG{n}{std\PYGZus{}1}
\PYG{n}{sample\PYGZus{}2} \PYG{o}{=} \PYG{n}{mean\PYGZus{}2} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{sample\PYGZus{}n}\PYG{p}{)}\PYG{o}{*}\PYG{n}{std\PYGZus{}2}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{sample\PYGZus{}1}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{sample\PYGZus{}2}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample1 Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample\PYGZus{}1}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample1 Std: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{sample\PYGZus{}1}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample2 Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample\PYGZus{}2}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample2 Std: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{sample\PYGZus{}2}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Sample1 Mean: 10.6
Sample1 Std: 4.15
Sample2 Mean: 7.46
Sample2 Std: 5.15
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Resampling_Statistics_21_1}.png}

Ok, now to compute a permutation test to assess if the two distributions are different, we need to generate a null distribution by permuting the group labels and recalculating the mean difference between the groups.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sample\PYGZus{}1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{sample\PYGZus{}1}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sample\PYGZus{}2}\PYG{p}{)}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{sample\PYGZus{}2}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{permute\PYGZus{}diffs} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}permutations}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{permutation\PYGZus{}label} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{permutation}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{diff} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{permutation\PYGZus{}label} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{permutation\PYGZus{}label} \PYG{o}{==} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{permute\PYGZus{}diffs}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{diff}\PYG{p}{)}
    
\PYG{n}{difference} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample\PYGZus{}1}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample\PYGZus{}2}\PYG{p}{)}
\PYG{n}{p\PYGZus{}value} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{permute\PYGZus{}diffs} \PYG{o}{\PYGZlt{}} \PYG{n}{difference}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{permute\PYGZus{}means}\PYG{p}{)}

\PYG{n}{sns}\PYG{o}{.}\PYG{n}{distplot}\PYG{p}{(}\PYG{n}{permute\PYGZus{}diffs}\PYG{p}{,} \PYG{n}{kde}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{difference}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Difference between Sample1 \PYGZam{} Sample2 Means: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{difference}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{n permutations \PYGZlt{} Sample Mean Difference = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{permute\PYGZus{}diffs} \PYG{o}{\PYGZlt{}} \PYG{n}{difference}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p\PYGZhy{}value = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{p\PYGZus{}value}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Difference between Sample1 \PYGZam{} Sample2 Means: 3.16
n permutations \PYGZlt{} Sample Mean Difference = 4996
p\PYGZhy{}value = 0.0008
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Resampling_Statistics_23_1}.png}

The difference we observed between the two distributions does not occur very frequently by chance. By permuting the labels and recomputing the difference, we found that the sample difference exceeded the permutated label differences 4,996/5000 times.

As long as your data are exchangeable, you can compute p\sphinxhyphen{}values for pretty much any type of test statistic using a permutation test. For example, to compute a p\sphinxhyphen{}value for a correlation between \(X\) and \(Y\) you would just shuffle one of the vectors and recompute the correlation. The p\sphinxhyphen{}value is the proportion of times that the correlation value exceeds the permuted correlations.

We hope that you will consider using these resampling statistics in your own work. There are many packages in python that can help you calculate bootstrap and permutation tests, but you can see that you can always write your code fairly easily if you understand the core concepts.


\section{Spring 2019}
\label{\detokenize{content/2019_Spring:spring-2019}}\label{\detokenize{content/2019_Spring::doc}}
In Spring 2019, students developed an experiment to study the neural basis of how Dartmouth undergraduates process viewing memes. Twelve Dartmouth undergraduates were scanned using functional magnetic resonance imaging (fMRI) while viewing 76 memes. After viewing each meme, participants made a decision about whether they would likely share this meme with a friend or not. After the scanning session, participants rated each meme on a number of dimensions using a Qualtrics survey (e.g., relatability, enjoyment, type of meme, etc.).

Unfortunately, we discovered that was a between run scanner artifact, which negatively impacted the results of all of the projects. Nevertheless, the questions and analytic approaches are very interesting and highlight the talent of Dartmouth undergraduates.


\subsection{Self\sphinxhyphen{}Referencing With Memes}
\label{\detokenize{content/2019_Spring:self-referencing-with-memes}}
Sarah Eger, Taylor Walsh, Serena Zhu

Memes have become a major phenomenon in recent years, due to individuals of all ages sharingthem across social media platforms. However, no study to date has evaluated the self\sphinxhyphen{}referential processing that occurs in response to relatable memes. Our research undertakes this topic through the analysis of functional MRI (fMRI) scans and behavioral data gathered from ten undergraduate students at Dartmouth College. The behavioral data facilitate the conclusion that 9 of the 76 memes presented in the study directly reference Dartmouth. These data also confirm that there is a statistically significant difference in relatability between these two categories of memes. The differences in activation to stimuli belonging to each of these classes were then evaluated using a univariate analysis of the fMRI scans. This analysis showed that some activation in the dmPFC and hypothalamus survived after thresholding the contrast between Dartmouth and non\sphinxhyphen{}Dartmouth memes. A Multivariate Pattern Analysis (MVPA) was also implemented to determine if a Support Vector Machine (SVM) could predict which stimulus class a participant was processing, based on activation patterns in the entire brain or individual regions of interest (ROI). Both the whole\sphinxhyphen{}brain and 50 ROI MVPAs suggest that no particular voxel or region of the brain accounts for the classification of the two types of memes more than others. However, this pilot study was limited by a small number of subjects and some flaws in the data collection. Therefore, the findings presented in this paper should be considered only as exploratory results that merit additional inquiry through future studies of self\sphinxhyphen{}referential processing in relation to relatable memes, Dartmouth or otherwise.

See their \sphinxcode{\sphinxupquote{presentation}}.


\subsection{FFA Activation While Viewing Memes}
\label{\detokenize{content/2019_Spring:ffa-activation-while-viewing-memes}}
Shivesh Shah, Hana Nazir, Huy Dang, Matthew Yuen

Human face perception is an evolutionary adaptation in humans that has become specialized because of the social nature of human life. The ability to quickly and accurately identify faces facilitates social bonding and the creation of social networks by allowing humans to gauge each other’s emotions and engage in social behavior. Contemporarily, socialization has taken the form of sharing memes, or pieces of social media that convey messages that are passed on from one person to another. Understanding how viewing memes affects the brain will allow us to understand how memes provide us an avenue to socialize with each other and share commonalities. The fusiform face area (FFA) is one area that has been found to highly activate in response to faces. How this area is activated during meme\sphinxhyphen{}viewing has been virtually unstudied. Thus, we endeavor to uncover how the brain, specifically the FFA, reacts when viewing memes with faces and memes without faces. Our study used fMRI data when participants were exposed to memes with and without faces. We ran a univariate contrast to identify whether there was an increase in activation of the FFA in response to Face vs NoFace memes. Then, we performed a prediction analysis to see whether activation in the FFA of our participants could predict whether they perceived faces in the memes. Finally, we performed a univariate contrast of brain activation when participants viewed memes for which they perceived faces and for which they were unsure. We found that there was non\sphinxhyphen{}significantly more activation of the FFA when participants perceived faces than when they did not. Additionally, our model for prediction was unable to reliably predict whether participants perceived faces or not based on their brain activation. And interestingly, we found that activation was significantly greater when participants were sure they saw a face than when they were unsure. Our results provide first steps toward using memes as a way to answer broader questions on what types of materials we like, how we decide what to share, and how memes can serve as a method of strengthening social networks.

See their \sphinxcode{\sphinxupquote{presentation}}.


\section{Spring 2020}
\label{\detokenize{content/2020_Spring:spring-2020}}\label{\detokenize{content/2020_Spring::doc}}
In Spring 2020, students were forced to take a remote version of the course due to the Covi19 pandemic. Unfortunately, this meant that students were unable to design and run their own original study. Instead, students developed original research questions and conducted secondary data analysis projects using data that was publicly available on \sphinxhref{https://openneuro.org/}{OpenNeuro}. One group of students worked with the naturalistic \sphinxhref{https://openneuro.org/search/sherlock}{Sherlock} dataset \sphinxhref{https://www.nature.com/articles/nn.4450}{Chen et al., 2017}. The other group worked with a \sphinxhref{https://openneuro.org/datasets/ds000171/versions/00001}{dataset} in which patient with depression and healthy controls listened to emotional music clips \sphinxhref{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0156859}{Lepping et al., 2016}. Students were able to communicate with the authors of the study to get additional information that was not shared in the data repository to complete their analyses.


\subsection{Self\sphinxhyphen{}Referencing With Memes}
\label{\detokenize{content/2020_Spring:self-referencing-with-memes}}
Lyrra Isanberg, Nina Kosowsky, Mia Newkirk, Kristen Soh, Sabrina Strauss

Memory is often regarded as an individualistic experience, with every person perceiving his or her world differently. To better understand the differences and similarities of human memory among individuals, we used the data collected by Chen et al. and performed several different analyses. We used contrast analyses, a representational similarity analysis (RSA), and an IS\sphinxhyphen{}RSA to answer several questions regarding brain activity during encoding and how that can be correlated with scene recall or semantic similarity. In our analyses, we provide answers to the following questions:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Do participants who successfully recall a scene have different activations during the encoding period than participants who do not recall that particular scene?

\item {} 
Are there common regions activated during encoding across subjects that are correlated with successful recall?

\item {} 
How does the temporal pattern of activity in the brain throughout encoding correlate with subject similarity, based on their scene recall?

\item {} 
How does the semantic similarity of scenes, based on their text similarity, correlate with the spatial representation of scenes during encoding?

\end{enumerate}

See their \sphinxcode{\sphinxupquote{ presentation}}.


\subsection{Music and Depression: A multivariate prediction/classification analysis and representational similarity analysis}
\label{\detokenize{content/2020_Spring:music-and-depression-a-multivariate-prediction-classification-analysis-and-representational-similarity-analysis}}
Jada Brown, Kera Carey, Amanda Chen, Emily Chen, Mia Iqbal, Ephthalia Michael\sphinxhyphen{}Scwarzinger, Nathan Skinner, Bryce West

For years, music has been known to provoke a strong emotional response. Due to peoples’ tendencies to self\sphinxhyphen{}medicate emotions with music, it has been looked at as a treatment for individuals suffering from Major Depressive Disorder (MDD) or Post\sphinxhyphen{}Traumatic Stress Disorder (PTSD). Emotion\sphinxhyphen{}provoking music has been shown to act on the reward circuitry, as well as reactivate the Anterior Hippocampus. Individuals suffering from depression and PTSD have shown damage and decreased activity in the reward system, as well as in the Hippocampus. Newer results have proved that music could be used to reactivate the Anterior Cingulate Cortex, an area with decreased activity in depressed patients. These studies have been difficult to navigate, as the stimuli, as well as cognitive reactions experienced during musical therapy, have been poorly defined. In this project, our group came up with two research questions to test by reanalzing data from (Lepping et al, 2015). (1) Can we create a model to discriminate between MDD and ND participants using the contrast between positive and negative stimuli? And (2) Can we see the differences between MDD and ND participants when processing the same audio clip? While the main study was designed to investigate neural circuitry of emotion and reward in depression, we wanted to do more with that data. Instead of just doing contrasts, and ROIs, we performed a ​Multivoxel Pattern Analysis (MVPA) and a ​Representational Similarity Analysis​ (RSA) using the audio files.

See their \sphinxcode{\sphinxupquote{ presentation}}.


\section{Contributing}
\label{\detokenize{content/Contributing:contributing}}\label{\detokenize{content/Contributing::doc}}
One of the wonderful aspects of both the neuroimaging and Python scientific computing communities is the strong commitment to developing and sharing knowledge and tools within the broader community. The goal of the dartbrains project is to build on this work and provide a resource for people to learn about how to analyze neuroimaging data. We try to incorporate as much open content as we can find that contributes to this goal. Please let us know if we have inadvertently omitted credit for any content generated by others. Though this project is based on a neuroimaging analysis course taught at Dartmouth College, we welcome contributions from anyone in the broader imaging community.


\subsection{Getting Started}
\label{\detokenize{content/Contributing:getting-started}}
The DartBrains project is hosted on \sphinxhref{https://github.com/ljchang/dartbrains}{github}. If you have any questions, comments, or suggestions, please open an issue.

If you notice any mistakes or have idea for new content, please either open an issue or submit a pull request for us to review.

The website is built using \sphinxhref{https://jupyter.org/jupyter-book/intro.html}{jupyter book}, which creates a jekyll website from markdown and jupyter notebooks. Please read their \sphinxhref{https://jupyter.org/jupyter-book/intro.html}{materials} to learn more about this neat resource.


\subsection{License for this book}
\label{\detokenize{content/Contributing:license-for-this-book}}
All content in this book (ie, any files and content in the \sphinxcode{\sphinxupquote{content/}} folder)
is licensed under the \sphinxhref{https://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons Attribution\sphinxhyphen{}ShareAlike 4.0 International}
(CC BY\sphinxhyphen{}SA 4.0) license.







\renewcommand{\indexname}{Index}
\printindex
\end{document}